{
  "generation_timestamp": "2025-11-14T14:53:35.071974",
  "total_experiments": 282,
  "total_setups": 17,
  "results": {
    "pubmedqa_reasoning_required": {
      "pubmedqa_reasoning_required_min": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.59,
            "precision": 0.601,
            "recall": 0.62,
            "f1_score": 0.6052,
            "jaccard": 0.601,
            "bleu": 0.0056,
            "rouge1_f": 0.6367,
            "rouge2_f": 0.0,
            "rougeL_f": 0.6367,
            "semantic_similarity": 0.8452,
            "answer_correctness": 0.68,
            "maybe_overuse_penalty": 1.0,
            "confidence_calibration": 0.84
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0111,
            "recall": 0.28,
            "f1_score": 0.0198,
            "jaccard": 0.0111,
            "bleu": 0.0033,
            "rouge1_f": 0.0815,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0815,
            "semantic_similarity": 0.2086,
            "answer_correctness": 0.8,
            "maybe_overuse_penalty": 0.965,
            "confidence_calibration": 0.9
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.02,
            "precision": 0.0202,
            "recall": 0.03,
            "f1_score": 0.0203,
            "jaccard": 0.0202,
            "bleu": 0.0,
            "rouge1_f": 0.801,
            "rouge2_f": 0.0,
            "rougeL_f": 0.801,
            "semantic_similarity": 0.8325,
            "answer_correctness": 0.84,
            "maybe_overuse_penalty": 0.993,
            "confidence_calibration": 0.92
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0013,
            "recall": 0.1,
            "f1_score": 0.0026,
            "jaccard": 0.0013,
            "bleu": 0.0003,
            "rouge1_f": 0.0035,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0035,
            "semantic_similarity": 0.0763,
            "answer_correctness": 0.08,
            "maybe_overuse_penalty": 0.384,
            "confidence_calibration": 0.534
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0009,
            "recall": 0.07,
            "f1_score": 0.0018,
            "jaccard": 0.0009,
            "bleu": 0.0004,
            "rouge1_f": 0.0183,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0183,
            "semantic_similarity": 0.0425,
            "answer_correctness": 0.33,
            "maybe_overuse_penalty": 0.594,
            "confidence_calibration": 0.653
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0008,
            "recall": 0.04,
            "f1_score": 0.0016,
            "jaccard": 0.0008,
            "bleu": 0.0002,
            "rouge1_f": 0.0172,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0172,
            "semantic_similarity": 0.0426,
            "answer_correctness": 0.58,
            "maybe_overuse_penalty": 0.79,
            "confidence_calibration": 0.774
          }
        }
      },
      "pubmedqa_expert_biostatistician_reasoning_required": {
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.14,
            "precision": 0.1403,
            "recall": 0.17,
            "f1_score": 0.1407,
            "jaccard": 0.1403,
            "bleu": 0.0,
            "rouge1_f": 0.1608,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1608,
            "semantic_similarity": 0.2116,
            "answer_correctness": 0.2,
            "maybe_overuse_penalty": 0.454,
            "confidence_calibration": 0.588
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0052,
            "recall": 0.18,
            "f1_score": 0.0096,
            "jaccard": 0.0052,
            "bleu": 0.0359,
            "rouge1_f": 0.1794,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1794,
            "semantic_similarity": 0.389,
            "answer_correctness": 0.68,
            "maybe_overuse_penalty": 0.881,
            "confidence_calibration": 0.815
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.02,
            "precision": 0.02,
            "recall": 0.02,
            "f1_score": 0.02,
            "jaccard": 0.02,
            "bleu": 0.0,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5519,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.17,
            "precision": 0.17,
            "recall": 0.17,
            "f1_score": 0.17,
            "jaccard": 0.17,
            "bleu": 0.08,
            "rouge1_f": 0.17,
            "rouge2_f": 0.0,
            "rougeL_f": 0.17,
            "semantic_similarity": 0.6346,
            "answer_correctness": 0.17,
            "maybe_overuse_penalty": 0.454,
            "confidence_calibration": 0.576
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.03,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        }
      },
      "pubmedqa_reasoning_required": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0075,
            "recall": 0.05,
            "f1_score": 0.0131,
            "jaccard": 0.0075,
            "bleu": 0.3289,
            "rouge1_f": 0.7225,
            "rouge2_f": 0.0,
            "rougeL_f": 0.7225,
            "semantic_similarity": 0.7456,
            "answer_correctness": 0.8,
            "maybe_overuse_penalty": 0.986,
            "confidence_calibration": 0.9
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.65,
            "precision": 0.65,
            "recall": 0.65,
            "f1_score": 0.65,
            "jaccard": 0.65,
            "bleu": 0.04,
            "rouge1_f": 0.65,
            "rouge2_f": 0.0,
            "rougeL_f": 0.65,
            "semantic_similarity": 0.8575,
            "answer_correctness": 0.65,
            "maybe_overuse_penalty": 0.825,
            "confidence_calibration": 0.819
          }
        },
        "u4b-mistral-7b_ft_pubmedqa_reasoning_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.77,
            "precision": 0.77,
            "recall": 0.77,
            "f1_score": 0.77,
            "jaccard": 0.77,
            "bleu": 0.77,
            "rouge1_f": 0.77,
            "rouge2_f": 0.0,
            "rougeL_f": 0.77,
            "semantic_similarity": 0.9231,
            "answer_correctness": 0.77,
            "maybe_overuse_penalty": 0.993,
            "confidence_calibration": 0.885
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.36,
            "precision": 0.36,
            "recall": 0.36,
            "f1_score": 0.36,
            "jaccard": 0.36,
            "bleu": 0.0,
            "rouge1_f": 0.36,
            "rouge2_f": 0.0,
            "rougeL_f": 0.36,
            "semantic_similarity": 0.7236,
            "answer_correctness": 0.36,
            "maybe_overuse_penalty": 0.608,
            "confidence_calibration": 0.671
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.36,
            "precision": 0.3606,
            "recall": 0.39,
            "f1_score": 0.3611,
            "jaccard": 0.3606,
            "bleu": 0.0002,
            "rouge1_f": 0.3715,
            "rouge2_f": 0.0,
            "rougeL_f": 0.3715,
            "semantic_similarity": 0.5371,
            "answer_correctness": 0.41,
            "maybe_overuse_penalty": 0.622,
            "confidence_calibration": 0.696
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.05,
            "precision": 0.05,
            "recall": 0.05,
            "f1_score": 0.05,
            "jaccard": 0.05,
            "bleu": 0.05,
            "rouge1_f": 0.05,
            "rouge2_f": 0.0,
            "rougeL_f": 0.05,
            "semantic_similarity": 0.5685,
            "answer_correctness": 0.05,
            "maybe_overuse_penalty": 0.342,
            "confidence_calibration": 0.513
          }
        }
      },
      "pubmedqa_expert_primary_care_reasoning_required": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0115,
            "recall": 0.18,
            "f1_score": 0.0208,
            "jaccard": 0.0115,
            "bleu": 0.0594,
            "rouge1_f": 0.3181,
            "rouge2_f": 0.0,
            "rougeL_f": 0.3181,
            "semantic_similarity": 0.4576,
            "answer_correctness": 0.83,
            "maybe_overuse_penalty": 0.979,
            "confidence_calibration": 0.915
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.06,
            "precision": 0.06,
            "recall": 0.06,
            "f1_score": 0.06,
            "jaccard": 0.06,
            "bleu": 0.0,
            "rouge1_f": 0.06,
            "rouge2_f": 0.0,
            "rougeL_f": 0.06,
            "semantic_similarity": 0.5801,
            "answer_correctness": 0.06,
            "maybe_overuse_penalty": 0.349,
            "confidence_calibration": 0.518
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.45,
            "precision": 0.45,
            "recall": 0.45,
            "f1_score": 0.45,
            "jaccard": 0.45,
            "bleu": 0.0,
            "rouge1_f": 0.45,
            "rouge2_f": 0.0,
            "rougeL_f": 0.45,
            "semantic_similarity": 0.76,
            "answer_correctness": 0.45,
            "maybe_overuse_penalty": 0.643,
            "confidence_calibration": 0.716
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.12,
            "precision": 0.1204,
            "recall": 0.15,
            "f1_score": 0.1209,
            "jaccard": 0.1204,
            "bleu": 0.0001,
            "rouge1_f": 0.1613,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1613,
            "semantic_similarity": 0.3236,
            "answer_correctness": 0.19,
            "maybe_overuse_penalty": 0.433,
            "confidence_calibration": 0.583
          }
        }
      },
      "pubmedqa_expert_epidemiologist_reasoning_required": {
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.12,
            "precision": 0.1204,
            "recall": 0.15,
            "f1_score": 0.1208,
            "jaccard": 0.1204,
            "bleu": 0.0001,
            "rouge1_f": 0.1618,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1618,
            "semantic_similarity": 0.2614,
            "answer_correctness": 0.21,
            "maybe_overuse_penalty": 0.447,
            "confidence_calibration": 0.593
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0087,
            "recall": 0.12,
            "f1_score": 0.0157,
            "jaccard": 0.0087,
            "bleu": 0.0464,
            "rouge1_f": 0.2294,
            "rouge2_f": 0.0,
            "rougeL_f": 0.2294,
            "semantic_similarity": 0.3847,
            "answer_correctness": 0.75,
            "maybe_overuse_penalty": 0.923,
            "confidence_calibration": 0.815
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.03,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.03,
            "precision": 0.03,
            "recall": 0.03,
            "f1_score": 0.03,
            "jaccard": 0.03,
            "bleu": 0.0,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5634,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.28,
            "precision": 0.28,
            "recall": 0.28,
            "f1_score": 0.28,
            "jaccard": 0.28,
            "bleu": 0.0,
            "rouge1_f": 0.28,
            "rouge2_f": 0.0,
            "rougeL_f": 0.28,
            "semantic_similarity": 0.679,
            "answer_correctness": 0.28,
            "maybe_overuse_penalty": 0.496,
            "confidence_calibration": 0.628
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "maybe_overuse_penalty": 0.328,
            "confidence_calibration": 0.508
          }
        }
      }
    },
    "ecqa_choice": {
      "ecqa_demographic_middle_aged_asian": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0045,
            "recall": 0.335,
            "f1_score": 0.0088,
            "jaccard": 0.0044,
            "bleu": 0.0012,
            "rouge1_f": 0.0089,
            "rouge2_f": 0.0019,
            "rougeL_f": 0.0089,
            "semantic_similarity": 0.3314,
            "answer_correctness": 0.24,
            "follows_format_instruction": 0.098,
            "answer_extractability": 0.7006
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0075,
            "recall": 0.435,
            "f1_score": 0.0147,
            "jaccard": 0.0075,
            "bleu": 0.0028,
            "rouge1_f": 0.0213,
            "rouge2_f": 0.0063,
            "rougeL_f": 0.0213,
            "semantic_similarity": 0.379,
            "answer_correctness": 0.34,
            "follows_format_instruction": 0.1,
            "answer_extractability": 0.6062
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.05,
            "precision": 0.0588,
            "recall": 0.45,
            "f1_score": 0.0672,
            "jaccard": 0.0588,
            "bleu": 0.0385,
            "rouge1_f": 0.082,
            "rouge2_f": 0.027,
            "rougeL_f": 0.082,
            "semantic_similarity": 0.4076,
            "answer_correctness": 0.37,
            "follows_format_instruction": 0.153,
            "answer_extractability": 0.616
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0085,
            "recall": 0.54,
            "f1_score": 0.0167,
            "jaccard": 0.0085,
            "bleu": 0.0028,
            "rouge1_f": 0.0186,
            "rouge2_f": 0.0056,
            "rougeL_f": 0.0186,
            "semantic_similarity": 0.4164,
            "answer_correctness": 0.47,
            "follows_format_instruction": 0.1,
            "answer_extractability": 0.6436
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0068,
            "recall": 0.485,
            "f1_score": 0.0133,
            "jaccard": 0.0068,
            "bleu": 0.0021,
            "rouge1_f": 0.0138,
            "rouge2_f": 0.0036,
            "rougeL_f": 0.0137,
            "semantic_similarity": 0.3688,
            "answer_correctness": 0.38,
            "follows_format_instruction": 0.099,
            "answer_extractability": 0.6575
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0109,
            "recall": 0.615,
            "f1_score": 0.0206,
            "jaccard": 0.0106,
            "bleu": 0.004,
            "rouge1_f": 0.0224,
            "rouge2_f": 0.0081,
            "rougeL_f": 0.0224,
            "semantic_similarity": 0.4308,
            "answer_correctness": 0.64,
            "follows_format_instruction": 0.12,
            "answer_extractability": 0.7589
          }
        }
      },
      "ecqa_choice_selection": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0258,
            "recall": 0.54,
            "f1_score": 0.0478,
            "jaccard": 0.0257,
            "bleu": 0.0087,
            "rouge1_f": 0.0493,
            "rouge2_f": 0.0172,
            "rougeL_f": 0.0493,
            "semantic_similarity": 0.4357,
            "answer_correctness": 0.63,
            "follows_format_instruction": 0.213,
            "answer_extractability": 0.8299
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.0455,
            "recall": 0.54,
            "f1_score": 0.0686,
            "jaccard": 0.0434,
            "bleu": 0.0362,
            "rouge1_f": 0.1281,
            "rouge2_f": 0.0374,
            "rougeL_f": 0.1281,
            "semantic_similarity": 0.5133,
            "answer_correctness": 0.64,
            "follows_format_instruction": 0.366,
            "answer_extractability": 0.6979
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0106,
            "recall": 0.2833,
            "f1_score": 0.0203,
            "jaccard": 0.0105,
            "bleu": 0.0038,
            "rouge1_f": 0.0274,
            "rouge2_f": 0.0067,
            "rougeL_f": 0.0271,
            "semantic_similarity": 0.3546,
            "answer_correctness": 0.28,
            "follows_format_instruction": 0.186,
            "answer_extractability": 0.8846
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0139,
            "recall": 0.33,
            "f1_score": 0.0261,
            "jaccard": 0.0137,
            "bleu": 0.0078,
            "rouge1_f": 0.0503,
            "rouge2_f": 0.0168,
            "rougeL_f": 0.0503,
            "semantic_similarity": 0.4548,
            "answer_correctness": 0.7,
            "follows_format_instruction": 0.161,
            "answer_extractability": 0.7632
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.71,
            "precision": 0.72,
            "recall": 0.81,
            "f1_score": 0.7257,
            "jaccard": 0.7198,
            "bleu": 0.715,
            "rouge1_f": 0.728,
            "rouge2_f": 0.3139,
            "rougeL_f": 0.728,
            "semantic_similarity": 0.8337,
            "answer_correctness": 0.8,
            "follows_format_instruction": 0.889,
            "answer_extractability": 0.9538
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.24,
            "precision": 0.2598,
            "recall": 0.5,
            "f1_score": 0.2708,
            "jaccard": 0.2577,
            "bleu": 0.2491,
            "rouge1_f": 0.2997,
            "rouge2_f": 0.1264,
            "rougeL_f": 0.2993,
            "semantic_similarity": 0.5654,
            "answer_correctness": 0.49,
            "follows_format_instruction": 0.531,
            "answer_extractability": 0.811
          }
        }
      },
      "ecqa_demographic_senior_european": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0051,
            "recall": 0.34,
            "f1_score": 0.01,
            "jaccard": 0.0051,
            "bleu": 0.0013,
            "rouge1_f": 0.0101,
            "rouge2_f": 0.0026,
            "rougeL_f": 0.0101,
            "semantic_similarity": 0.2872,
            "answer_correctness": 0.18,
            "follows_format_instruction": 0.098,
            "answer_extractability": 0.5879
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0081,
            "recall": 0.425,
            "f1_score": 0.0159,
            "jaccard": 0.0081,
            "bleu": 0.0028,
            "rouge1_f": 0.0185,
            "rouge2_f": 0.0049,
            "rougeL_f": 0.0185,
            "semantic_similarity": 0.3479,
            "answer_correctness": 0.48,
            "follows_format_instruction": 0.098,
            "answer_extractability": 0.7571
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.05,
            "precision": 0.0624,
            "recall": 0.58,
            "f1_score": 0.0741,
            "jaccard": 0.0623,
            "bleu": 0.0393,
            "rouge1_f": 0.0867,
            "rouge2_f": 0.0277,
            "rougeL_f": 0.0867,
            "semantic_similarity": 0.3979,
            "answer_correctness": 0.42,
            "follows_format_instruction": 0.158,
            "answer_extractability": 0.6086
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0073,
            "recall": 0.42,
            "f1_score": 0.0144,
            "jaccard": 0.0073,
            "bleu": 0.0028,
            "rouge1_f": 0.0219,
            "rouge2_f": 0.0062,
            "rougeL_f": 0.0219,
            "semantic_similarity": 0.3834,
            "answer_correctness": 0.34,
            "follows_format_instruction": 0.1,
            "answer_extractability": 0.6078
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0088,
            "recall": 0.53,
            "f1_score": 0.0172,
            "jaccard": 0.0088,
            "bleu": 0.0032,
            "rouge1_f": 0.0215,
            "rouge2_f": 0.0061,
            "rougeL_f": 0.0215,
            "semantic_similarity": 0.4014,
            "answer_correctness": 0.43,
            "follows_format_instruction": 0.1,
            "answer_extractability": 0.6484
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0159,
            "recall": 0.595,
            "f1_score": 0.0287,
            "jaccard": 0.0151,
            "bleu": 0.0111,
            "rouge1_f": 0.0522,
            "rouge2_f": 0.015,
            "rougeL_f": 0.0522,
            "semantic_similarity": 0.4605,
            "answer_correctness": 0.65,
            "follows_format_instruction": 0.203,
            "answer_extractability": 0.7271
          }
        }
      },
      "ecqa_demographic_young_american": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0134,
            "recall": 0.675,
            "f1_score": 0.0247,
            "jaccard": 0.0129,
            "bleu": 0.006,
            "rouge1_f": 0.0302,
            "rouge2_f": 0.011,
            "rougeL_f": 0.0302,
            "semantic_similarity": 0.4488,
            "answer_correctness": 0.59,
            "follows_format_instruction": 0.143,
            "answer_extractability": 0.7261
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0083,
            "recall": 0.585,
            "f1_score": 0.0163,
            "jaccard": 0.0083,
            "bleu": 0.0027,
            "rouge1_f": 0.0183,
            "rouge2_f": 0.0052,
            "rougeL_f": 0.0183,
            "semantic_similarity": 0.4062,
            "answer_correctness": 0.38,
            "follows_format_instruction": 0.1,
            "answer_extractability": 0.5956
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.0508,
            "recall": 0.4667,
            "f1_score": 0.061,
            "jaccard": 0.0507,
            "bleu": 0.0341,
            "rouge1_f": 0.0761,
            "rouge2_f": 0.0175,
            "rougeL_f": 0.0761,
            "semantic_similarity": 0.3956,
            "answer_correctness": 0.33,
            "follows_format_instruction": 0.158,
            "answer_extractability": 0.6477
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0103,
            "recall": 0.47,
            "f1_score": 0.0202,
            "jaccard": 0.0103,
            "bleu": 0.0038,
            "rouge1_f": 0.0249,
            "rouge2_f": 0.0071,
            "rougeL_f": 0.0249,
            "semantic_similarity": 0.4353,
            "answer_correctness": 0.58,
            "follows_format_instruction": 0.101,
            "answer_extractability": 0.8152
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0062,
            "recall": 0.405,
            "f1_score": 0.0123,
            "jaccard": 0.0062,
            "bleu": 0.0014,
            "rouge1_f": 0.0098,
            "rouge2_f": 0.0023,
            "rougeL_f": 0.0098,
            "semantic_similarity": 0.3147,
            "answer_correctness": 0.22,
            "follows_format_instruction": 0.101,
            "answer_extractability": 0.7683
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0089,
            "recall": 0.465,
            "f1_score": 0.0173,
            "jaccard": 0.0088,
            "bleu": 0.0033,
            "rouge1_f": 0.025,
            "rouge2_f": 0.0072,
            "rougeL_f": 0.025,
            "semantic_similarity": 0.4008,
            "answer_correctness": 0.37,
            "follows_format_instruction": 0.104,
            "answer_extractability": 0.5799
          }
        }
      }
    },
    "ecare_explanation_generation": {
      "ecare_explanation_generation_demographic_young_american": {
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0347,
            "recall": 0.5147,
            "f1_score": 0.0643,
            "jaccard": 0.0337,
            "bleu": 0.007,
            "rouge1_f": 0.0567,
            "rouge2_f": 0.0173,
            "rougeL_f": 0.0503,
            "semantic_similarity": 0.5752,
            "response_brevity": 0.292,
            "conceptual_abstraction_level": 0.903,
            "single_sentence_format": 0.118
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0254,
            "recall": 0.5345,
            "f1_score": 0.048,
            "jaccard": 0.0249,
            "bleu": 0.0041,
            "rouge1_f": 0.039,
            "rouge2_f": 0.0108,
            "rougeL_f": 0.0341,
            "semantic_similarity": 0.5281,
            "response_brevity": 0.143,
            "conceptual_abstraction_level": 0.601,
            "single_sentence_format": 0.014
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0284,
            "recall": 0.4461,
            "f1_score": 0.0525,
            "jaccard": 0.0274,
            "bleu": 0.0052,
            "rouge1_f": 0.0464,
            "rouge2_f": 0.0124,
            "rougeL_f": 0.0404,
            "semantic_similarity": 0.5359,
            "response_brevity": 0.198,
            "conceptual_abstraction_level": 0.9095,
            "single_sentence_format": 0.12
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0293,
            "recall": 0.2474,
            "f1_score": 0.0486,
            "jaccard": 0.0256,
            "bleu": 0.0107,
            "rouge1_f": 0.0536,
            "rouge2_f": 0.0078,
            "rougeL_f": 0.0455,
            "semantic_similarity": 0.3878,
            "response_brevity": 0.554,
            "conceptual_abstraction_level": 0.9445,
            "single_sentence_format": 0.426
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0267,
            "recall": 0.4561,
            "f1_score": 0.0499,
            "jaccard": 0.026,
            "bleu": 0.0043,
            "rouge1_f": 0.0409,
            "rouge2_f": 0.0104,
            "rougeL_f": 0.035,
            "semantic_similarity": 0.5352,
            "response_brevity": 0.226,
            "conceptual_abstraction_level": 0.902,
            "single_sentence_format": 0.008
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0246,
            "recall": 0.5406,
            "f1_score": 0.0466,
            "jaccard": 0.0241,
            "bleu": 0.0042,
            "rouge1_f": 0.0336,
            "rouge2_f": 0.012,
            "rougeL_f": 0.0314,
            "semantic_similarity": 0.5588,
            "response_brevity": 0.106,
            "conceptual_abstraction_level": 0.813,
            "single_sentence_format": 0.002
          }
        }
      },
      "ecare_explanation_generation_stakeholder_parent": {
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0202,
            "recall": 0.4909,
            "f1_score": 0.0385,
            "jaccard": 0.0199,
            "bleu": 0.003,
            "rouge1_f": 0.0304,
            "rouge2_f": 0.0074,
            "rougeL_f": 0.0265,
            "semantic_similarity": 0.4903,
            "response_brevity": 0.109,
            "conceptual_abstraction_level": 0.7285,
            "single_sentence_format": 0.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.025,
            "recall": 0.4257,
            "f1_score": 0.0468,
            "jaccard": 0.0243,
            "bleu": 0.0044,
            "rouge1_f": 0.043,
            "rouge2_f": 0.0103,
            "rougeL_f": 0.0359,
            "semantic_similarity": 0.4964,
            "response_brevity": 0.276,
            "conceptual_abstraction_level": 0.8435,
            "single_sentence_format": 0.014
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0259,
            "recall": 0.2817,
            "f1_score": 0.0441,
            "jaccard": 0.0233,
            "bleu": 0.0114,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0061,
            "rougeL_f": 0.033,
            "semantic_similarity": 0.3425,
            "response_brevity": 0.448,
            "conceptual_abstraction_level": 0.9135,
            "single_sentence_format": 0.326
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.025,
            "recall": 0.4093,
            "f1_score": 0.0463,
            "jaccard": 0.024,
            "bleu": 0.0048,
            "rouge1_f": 0.0375,
            "rouge2_f": 0.0082,
            "rougeL_f": 0.0316,
            "semantic_similarity": 0.4678,
            "response_brevity": 0.138,
            "conceptual_abstraction_level": 0.8555,
            "single_sentence_format": 0.036
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.028,
            "recall": 0.4213,
            "f1_score": 0.0519,
            "jaccard": 0.0271,
            "bleu": 0.0054,
            "rouge1_f": 0.049,
            "rouge2_f": 0.01,
            "rougeL_f": 0.0399,
            "semantic_similarity": 0.4574,
            "response_brevity": 0.242,
            "conceptual_abstraction_level": 0.936,
            "single_sentence_format": 0.168
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0192,
            "recall": 0.4955,
            "f1_score": 0.0366,
            "jaccard": 0.0188,
            "bleu": 0.0025,
            "rouge1_f": 0.0256,
            "rouge2_f": 0.0073,
            "rougeL_f": 0.0234,
            "semantic_similarity": 0.4934,
            "response_brevity": 0.074,
            "conceptual_abstraction_level": 0.757,
            "single_sentence_format": 0.0
          }
        }
      },
      "ecare_explanation_generation_expert_psychologist": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0189,
            "recall": 0.5359,
            "f1_score": 0.0363,
            "jaccard": 0.0186,
            "bleu": 0.0024,
            "rouge1_f": 0.0229,
            "rouge2_f": 0.0073,
            "rougeL_f": 0.0218,
            "semantic_similarity": 0.5304,
            "response_brevity": 0.049,
            "conceptual_abstraction_level": 0.725,
            "single_sentence_format": 0.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0261,
            "recall": 0.4449,
            "f1_score": 0.0487,
            "jaccard": 0.0253,
            "bleu": 0.0041,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0088,
            "rougeL_f": 0.0353,
            "semantic_similarity": 0.4558,
            "response_brevity": 0.132,
            "conceptual_abstraction_level": 0.8845,
            "single_sentence_format": 0.048
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0293,
            "recall": 0.4674,
            "f1_score": 0.0545,
            "jaccard": 0.0284,
            "bleu": 0.0049,
            "rouge1_f": 0.0434,
            "rouge2_f": 0.012,
            "rougeL_f": 0.0389,
            "semantic_similarity": 0.502,
            "response_brevity": 0.284,
            "conceptual_abstraction_level": 0.783,
            "single_sentence_format": 0.012
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0273,
            "recall": 0.3883,
            "f1_score": 0.0494,
            "jaccard": 0.0258,
            "bleu": 0.0057,
            "rouge1_f": 0.0367,
            "rouge2_f": 0.0092,
            "rougeL_f": 0.0339,
            "semantic_similarity": 0.449,
            "response_brevity": 0.175,
            "conceptual_abstraction_level": 0.8195,
            "single_sentence_format": 0.06
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0263,
            "recall": 0.4551,
            "f1_score": 0.0492,
            "jaccard": 0.0256,
            "bleu": 0.0039,
            "rouge1_f": 0.0384,
            "rouge2_f": 0.0092,
            "rougeL_f": 0.0339,
            "semantic_similarity": 0.498,
            "response_brevity": 0.16,
            "conceptual_abstraction_level": 0.7545,
            "single_sentence_format": 0.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0194,
            "recall": 0.5218,
            "f1_score": 0.0371,
            "jaccard": 0.019,
            "bleu": 0.0026,
            "rouge1_f": 0.0276,
            "rouge2_f": 0.0065,
            "rougeL_f": 0.0245,
            "semantic_similarity": 0.4713,
            "response_brevity": 0.098,
            "conceptual_abstraction_level": 0.5135,
            "single_sentence_format": 0.0
          }
        }
      },
      "ecare_explanation_generation": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.03,
            "recall": 0.2093,
            "f1_score": 0.0463,
            "jaccard": 0.025,
            "bleu": 0.0309,
            "rouge1_f": 0.0411,
            "rouge2_f": 0.0095,
            "rougeL_f": 0.038,
            "semantic_similarity": 0.2562,
            "response_brevity": 0.681,
            "conceptual_abstraction_level": 0.951,
            "single_sentence_format": 0.602
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0456,
            "recall": 0.5242,
            "f1_score": 0.0823,
            "jaccard": 0.0439,
            "bleu": 0.0093,
            "rouge1_f": 0.07,
            "rouge2_f": 0.0222,
            "rougeL_f": 0.0639,
            "semantic_similarity": 0.5833,
            "response_brevity": 0.324,
            "conceptual_abstraction_level": 0.9155,
            "single_sentence_format": 0.284
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0324,
            "recall": 0.5605,
            "f1_score": 0.0605,
            "jaccard": 0.0317,
            "bleu": 0.0053,
            "rouge1_f": 0.0466,
            "rouge2_f": 0.0143,
            "rougeL_f": 0.0433,
            "semantic_similarity": 0.5461,
            "response_brevity": 0.204,
            "conceptual_abstraction_level": 0.6795,
            "single_sentence_format": 0.052
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0483,
            "recall": 0.4914,
            "f1_score": 0.0862,
            "jaccard": 0.0461,
            "bleu": 0.0099,
            "rouge1_f": 0.0796,
            "rouge2_f": 0.0215,
            "rougeL_f": 0.0707,
            "semantic_similarity": 0.5523,
            "response_brevity": 0.425,
            "conceptual_abstraction_level": 0.912,
            "single_sentence_format": 0.372
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0379,
            "recall": 0.506,
            "f1_score": 0.0694,
            "jaccard": 0.0366,
            "bleu": 0.0068,
            "rouge1_f": 0.0578,
            "rouge2_f": 0.0185,
            "rougeL_f": 0.0521,
            "semantic_similarity": 0.5193,
            "response_brevity": 0.274,
            "conceptual_abstraction_level": 0.8,
            "single_sentence_format": 0.06
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.037,
            "recall": 0.4921,
            "f1_score": 0.0669,
            "jaccard": 0.0356,
            "bleu": 0.0063,
            "rouge1_f": 0.0572,
            "rouge2_f": 0.0145,
            "rougeL_f": 0.0476,
            "semantic_similarity": 0.4388,
            "response_brevity": 0.208,
            "conceptual_abstraction_level": 0.898,
            "single_sentence_format": 0.132
          }
        }
      },
      "ecare_explanation_generation_expert_medical": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0219,
            "recall": 0.5363,
            "f1_score": 0.0418,
            "jaccard": 0.0215,
            "bleu": 0.0031,
            "rouge1_f": 0.0283,
            "rouge2_f": 0.0091,
            "rougeL_f": 0.0265,
            "semantic_similarity": 0.5653,
            "response_brevity": 0.093,
            "conceptual_abstraction_level": 0.8025,
            "single_sentence_format": 0.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0318,
            "recall": 0.4881,
            "f1_score": 0.0589,
            "jaccard": 0.0309,
            "bleu": 0.0055,
            "rouge1_f": 0.0484,
            "rouge2_f": 0.0138,
            "rougeL_f": 0.0433,
            "semantic_similarity": 0.5242,
            "response_brevity": 0.168,
            "conceptual_abstraction_level": 0.923,
            "single_sentence_format": 0.066
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0276,
            "recall": 0.479,
            "f1_score": 0.0516,
            "jaccard": 0.0269,
            "bleu": 0.0044,
            "rouge1_f": 0.0419,
            "rouge2_f": 0.0114,
            "rougeL_f": 0.0372,
            "semantic_similarity": 0.5557,
            "response_brevity": 0.172,
            "conceptual_abstraction_level": 0.7535,
            "single_sentence_format": 0.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0318,
            "recall": 0.3686,
            "f1_score": 0.0555,
            "jaccard": 0.0293,
            "bleu": 0.0077,
            "rouge1_f": 0.0517,
            "rouge2_f": 0.0093,
            "rougeL_f": 0.0444,
            "semantic_similarity": 0.4799,
            "response_brevity": 0.349,
            "conceptual_abstraction_level": 0.88,
            "single_sentence_format": 0.206
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0227,
            "recall": 0.5441,
            "f1_score": 0.0432,
            "jaccard": 0.0223,
            "bleu": 0.0034,
            "rouge1_f": 0.0319,
            "rouge2_f": 0.0092,
            "rougeL_f": 0.029,
            "semantic_similarity": 0.5488,
            "response_brevity": 0.102,
            "conceptual_abstraction_level": 0.644,
            "single_sentence_format": 0.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0325,
            "recall": 0.4878,
            "f1_score": 0.0602,
            "jaccard": 0.0315,
            "bleu": 0.0058,
            "rouge1_f": 0.0513,
            "rouge2_f": 0.015,
            "rougeL_f": 0.0464,
            "semantic_similarity": 0.5681,
            "response_brevity": 0.29,
            "conceptual_abstraction_level": 0.906,
            "single_sentence_format": 0.03
          }
        }
      }
    },
    "ecqa_freeflow": {
      "ecqa_stakeholder_parent": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1189,
            "recall": 0.3578,
            "f1_score": 0.1749,
            "jaccard": 0.0965,
            "bleu": 0.0186,
            "rouge1_f": 0.2041,
            "rouge2_f": 0.0452,
            "rougeL_f": 0.1272,
            "semantic_similarity": 0.5212,
            "all_choices_addressed": 0.264,
            "single_paragraph_format": 0.997,
            "explanation_depth": 0.2344
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0941,
            "recall": 0.3312,
            "f1_score": 0.1437,
            "jaccard": 0.0779,
            "bleu": 0.0127,
            "rouge1_f": 0.1895,
            "rouge2_f": 0.0337,
            "rougeL_f": 0.1148,
            "semantic_similarity": 0.5527,
            "all_choices_addressed": 0.388,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.3159
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0961,
            "recall": 0.3615,
            "f1_score": 0.1492,
            "jaccard": 0.0812,
            "bleu": 0.0141,
            "rouge1_f": 0.1812,
            "rouge2_f": 0.035,
            "rougeL_f": 0.1138,
            "semantic_similarity": 0.5147,
            "all_choices_addressed": 0.246,
            "single_paragraph_format": 0.224,
            "explanation_depth": 0.2631
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0931,
            "recall": 0.3614,
            "f1_score": 0.1456,
            "jaccard": 0.079,
            "bleu": 0.0141,
            "rouge1_f": 0.1731,
            "rouge2_f": 0.0364,
            "rougeL_f": 0.1112,
            "semantic_similarity": 0.5304,
            "all_choices_addressed": 0.236,
            "single_paragraph_format": 0.925,
            "explanation_depth": 0.2186
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1135,
            "recall": 0.3458,
            "f1_score": 0.1645,
            "jaccard": 0.0903,
            "bleu": 0.0154,
            "rouge1_f": 0.1872,
            "rouge2_f": 0.0399,
            "rougeL_f": 0.1181,
            "semantic_similarity": 0.5113,
            "all_choices_addressed": 0.232,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.1986
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0952,
            "recall": 0.4217,
            "f1_score": 0.1525,
            "jaccard": 0.0831,
            "bleu": 0.0145,
            "rouge1_f": 0.1716,
            "rouge2_f": 0.0415,
            "rougeL_f": 0.1084,
            "semantic_similarity": 0.527,
            "all_choices_addressed": 0.328,
            "single_paragraph_format": 0.88,
            "explanation_depth": 0.3156
          }
        }
      },
      "ecqa_stakeholder_disability_advocate": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.105,
            "recall": 0.3613,
            "f1_score": 0.1593,
            "jaccard": 0.0871,
            "bleu": 0.015,
            "rouge1_f": 0.1869,
            "rouge2_f": 0.0389,
            "rougeL_f": 0.1196,
            "semantic_similarity": 0.4381,
            "all_choices_addressed": 0.264,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.227
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1017,
            "recall": 0.3558,
            "f1_score": 0.1553,
            "jaccard": 0.0847,
            "bleu": 0.0133,
            "rouge1_f": 0.1924,
            "rouge2_f": 0.0353,
            "rougeL_f": 0.1193,
            "semantic_similarity": 0.4699,
            "all_choices_addressed": 0.31,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.3006
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0966,
            "recall": 0.3606,
            "f1_score": 0.1497,
            "jaccard": 0.0814,
            "bleu": 0.0136,
            "rouge1_f": 0.1799,
            "rouge2_f": 0.0329,
            "rougeL_f": 0.1109,
            "semantic_similarity": 0.4213,
            "all_choices_addressed": 0.208,
            "single_paragraph_format": 0.216,
            "explanation_depth": 0.2672
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0938,
            "recall": 0.3647,
            "f1_score": 0.1469,
            "jaccard": 0.0797,
            "bleu": 0.0136,
            "rouge1_f": 0.1686,
            "rouge2_f": 0.0347,
            "rougeL_f": 0.1069,
            "semantic_similarity": 0.4441,
            "all_choices_addressed": 0.244,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.2198
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1019,
            "recall": 0.4093,
            "f1_score": 0.1606,
            "jaccard": 0.0879,
            "bleu": 0.0159,
            "rouge1_f": 0.1821,
            "rouge2_f": 0.0435,
            "rougeL_f": 0.1146,
            "semantic_similarity": 0.4261,
            "all_choices_addressed": 0.262,
            "single_paragraph_format": 0.984,
            "explanation_depth": 0.2606
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1173,
            "recall": 0.3421,
            "f1_score": 0.1681,
            "jaccard": 0.0927,
            "bleu": 0.0172,
            "rouge1_f": 0.2019,
            "rouge2_f": 0.0455,
            "rougeL_f": 0.126,
            "semantic_similarity": 0.4278,
            "all_choices_addressed": 0.256,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.2288
          }
        }
      },
      "ecqa_expert_behavioral_economist": {
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0961,
            "recall": 0.3705,
            "f1_score": 0.1494,
            "jaccard": 0.0813,
            "bleu": 0.0145,
            "rouge1_f": 0.1947,
            "rouge2_f": 0.0433,
            "rougeL_f": 0.1199,
            "semantic_similarity": 0.5397,
            "all_choices_addressed": 0.416,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.4467
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0986,
            "recall": 0.3753,
            "f1_score": 0.1534,
            "jaccard": 0.0836,
            "bleu": 0.0154,
            "rouge1_f": 0.1813,
            "rouge2_f": 0.0434,
            "rougeL_f": 0.1187,
            "semantic_similarity": 0.4459,
            "all_choices_addressed": 0.316,
            "single_paragraph_format": 0.982,
            "explanation_depth": 0.3376
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.106,
            "recall": 0.3832,
            "f1_score": 0.163,
            "jaccard": 0.0893,
            "bleu": 0.0156,
            "rouge1_f": 0.2003,
            "rouge2_f": 0.0448,
            "rougeL_f": 0.1229,
            "semantic_similarity": 0.4914,
            "all_choices_addressed": 0.288,
            "single_paragraph_format": 0.205,
            "explanation_depth": 0.3783
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1126,
            "recall": 0.3708,
            "f1_score": 0.1693,
            "jaccard": 0.0931,
            "bleu": 0.0169,
            "rouge1_f": 0.192,
            "rouge2_f": 0.0485,
            "rougeL_f": 0.1247,
            "semantic_similarity": 0.4908,
            "all_choices_addressed": 0.28,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.338
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1089,
            "recall": 0.3868,
            "f1_score": 0.1668,
            "jaccard": 0.0918,
            "bleu": 0.0176,
            "rouge1_f": 0.1965,
            "rouge2_f": 0.0516,
            "rougeL_f": 0.1279,
            "semantic_similarity": 0.5048,
            "all_choices_addressed": 0.39,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.3018
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1003,
            "recall": 0.4257,
            "f1_score": 0.1594,
            "jaccard": 0.0872,
            "bleu": 0.0164,
            "rouge1_f": 0.178,
            "rouge2_f": 0.0491,
            "rougeL_f": 0.1156,
            "semantic_similarity": 0.4992,
            "all_choices_addressed": 0.412,
            "single_paragraph_format": 0.957,
            "explanation_depth": 0.4709
          }
        }
      },
      "ecqa_expert_anthropologist": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1024,
            "recall": 0.426,
            "f1_score": 0.1623,
            "jaccard": 0.0889,
            "bleu": 0.0151,
            "rouge1_f": 0.1797,
            "rouge2_f": 0.0491,
            "rougeL_f": 0.1169,
            "semantic_similarity": 0.5942,
            "all_choices_addressed": 0.388,
            "single_paragraph_format": 0.96,
            "explanation_depth": 0.4834
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1019,
            "recall": 0.3925,
            "f1_score": 0.159,
            "jaccard": 0.0869,
            "bleu": 0.0165,
            "rouge1_f": 0.1919,
            "rouge2_f": 0.0471,
            "rougeL_f": 0.1234,
            "semantic_similarity": 0.6232,
            "all_choices_addressed": 0.268,
            "single_paragraph_format": 0.994,
            "explanation_depth": 0.3849
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1227,
            "recall": 0.3481,
            "f1_score": 0.1764,
            "jaccard": 0.0976,
            "bleu": 0.0186,
            "rouge1_f": 0.1994,
            "rouge2_f": 0.0518,
            "rougeL_f": 0.1359,
            "semantic_similarity": 0.5743,
            "all_choices_addressed": 0.256,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.4108
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.108,
            "recall": 0.3747,
            "f1_score": 0.1646,
            "jaccard": 0.0903,
            "bleu": 0.0136,
            "rouge1_f": 0.1948,
            "rouge2_f": 0.0395,
            "rougeL_f": 0.1237,
            "semantic_similarity": 0.5852,
            "all_choices_addressed": 0.212,
            "single_paragraph_format": 0.374,
            "explanation_depth": 0.431
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0965,
            "recall": 0.3808,
            "f1_score": 0.151,
            "jaccard": 0.0823,
            "bleu": 0.0122,
            "rouge1_f": 0.1865,
            "rouge2_f": 0.0392,
            "rougeL_f": 0.115,
            "semantic_similarity": 0.6302,
            "all_choices_addressed": 0.302,
            "single_paragraph_format": 0.972,
            "explanation_depth": 0.543
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1192,
            "recall": 0.4055,
            "f1_score": 0.1807,
            "jaccard": 0.1001,
            "bleu": 0.0187,
            "rouge1_f": 0.2025,
            "rouge2_f": 0.0531,
            "rougeL_f": 0.1318,
            "semantic_similarity": 0.6503,
            "all_choices_addressed": 0.31,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.4103
          }
        }
      },
      "ecqa_freeflow_explanation": {
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1836,
            "recall": 0.3951,
            "f1_score": 0.2442,
            "jaccard": 0.1411,
            "bleu": 0.0338,
            "rouge1_f": 0.3498,
            "rouge2_f": 0.1024,
            "rougeL_f": 0.2084,
            "semantic_similarity": 0.7117,
            "all_choices_addressed": 0.904,
            "single_paragraph_format": 0.99,
            "explanation_depth": 0.4403
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1774,
            "recall": 0.4797,
            "f1_score": 0.2529,
            "jaccard": 0.1461,
            "bleu": 0.03,
            "rouge1_f": 0.303,
            "rouge2_f": 0.1003,
            "rougeL_f": 0.1887,
            "semantic_similarity": 0.7191,
            "all_choices_addressed": 0.866,
            "single_paragraph_format": 0.561,
            "explanation_depth": 0.4977
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1921,
            "recall": 0.4347,
            "f1_score": 0.2599,
            "jaccard": 0.1513,
            "bleu": 0.0368,
            "rouge1_f": 0.3424,
            "rouge2_f": 0.1074,
            "rougeL_f": 0.2073,
            "semantic_similarity": 0.7224,
            "all_choices_addressed": 0.848,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.4426
          }
        },
        "u4b-mistral-7b_ft_ecqa_freeflow_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.4899,
            "recall": 0.4171,
            "f1_score": 0.4355,
            "jaccard": 0.2849,
            "bleu": 0.0532,
            "rouge1_f": 0.2774,
            "rouge2_f": 0.1136,
            "rougeL_f": 0.2229,
            "semantic_similarity": 0.7053,
            "all_choices_addressed": 0.818,
            "single_paragraph_format": 0.231,
            "explanation_depth": 0.0859
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1842,
            "recall": 0.4678,
            "f1_score": 0.2585,
            "jaccard": 0.1502,
            "bleu": 0.0337,
            "rouge1_f": 0.3148,
            "rouge2_f": 0.1092,
            "rougeL_f": 0.2026,
            "semantic_similarity": 0.7253,
            "all_choices_addressed": 0.94,
            "single_paragraph_format": 0.994,
            "explanation_depth": 0.4152
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1897,
            "recall": 0.4388,
            "f1_score": 0.2572,
            "jaccard": 0.1495,
            "bleu": 0.0371,
            "rouge1_f": 0.3093,
            "rouge2_f": 0.1036,
            "rougeL_f": 0.1949,
            "semantic_similarity": 0.7104,
            "all_choices_addressed": 0.674,
            "single_paragraph_format": 0.915,
            "explanation_depth": 0.3952
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1628,
            "recall": 0.5026,
            "f1_score": 0.2414,
            "jaccard": 0.1386,
            "bleu": 0.0304,
            "rouge1_f": 0.285,
            "rouge2_f": 0.098,
            "rougeL_f": 0.1794,
            "semantic_similarity": 0.7215,
            "all_choices_addressed": 0.98,
            "single_paragraph_format": 0.861,
            "explanation_depth": 0.5681
          }
        }
      },
      "ecqa_expert_psychologist": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0983,
            "recall": 0.4238,
            "f1_score": 0.1568,
            "jaccard": 0.0856,
            "bleu": 0.0146,
            "rouge1_f": 0.1716,
            "rouge2_f": 0.0451,
            "rougeL_f": 0.1115,
            "semantic_similarity": 0.5488,
            "all_choices_addressed": 0.37,
            "single_paragraph_format": 0.904,
            "explanation_depth": 0.4129
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.106,
            "recall": 0.3728,
            "f1_score": 0.1619,
            "jaccard": 0.0887,
            "bleu": 0.0145,
            "rouge1_f": 0.1974,
            "rouge2_f": 0.0411,
            "rougeL_f": 0.1237,
            "semantic_similarity": 0.5423,
            "all_choices_addressed": 0.27,
            "single_paragraph_format": 0.267,
            "explanation_depth": 0.3234
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0945,
            "recall": 0.3819,
            "f1_score": 0.1489,
            "jaccard": 0.081,
            "bleu": 0.0133,
            "rouge1_f": 0.1915,
            "rouge2_f": 0.0426,
            "rougeL_f": 0.1174,
            "semantic_similarity": 0.5998,
            "all_choices_addressed": 0.414,
            "single_paragraph_format": 0.986,
            "explanation_depth": 0.4244
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1147,
            "recall": 0.3515,
            "f1_score": 0.169,
            "jaccard": 0.0931,
            "bleu": 0.0171,
            "rouge1_f": 0.1893,
            "rouge2_f": 0.0457,
            "rougeL_f": 0.1279,
            "semantic_similarity": 0.5241,
            "all_choices_addressed": 0.2,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.2537
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1,
            "recall": 0.376,
            "f1_score": 0.1552,
            "jaccard": 0.0846,
            "bleu": 0.0174,
            "rouge1_f": 0.1851,
            "rouge2_f": 0.0493,
            "rougeL_f": 0.1233,
            "semantic_similarity": 0.5808,
            "all_choices_addressed": 0.296,
            "single_paragraph_format": 0.994,
            "explanation_depth": 0.317
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1086,
            "recall": 0.3777,
            "f1_score": 0.1656,
            "jaccard": 0.0909,
            "bleu": 0.0171,
            "rouge1_f": 0.1866,
            "rouge2_f": 0.0476,
            "rougeL_f": 0.1234,
            "semantic_similarity": 0.5752,
            "all_choices_addressed": 0.252,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.278
          }
        }
      },
      "ecqa_stakeholder_perspective": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.116,
            "recall": 0.4161,
            "f1_score": 0.1781,
            "jaccard": 0.0984,
            "bleu": 0.0181,
            "rouge1_f": 0.2116,
            "rouge2_f": 0.0504,
            "rougeL_f": 0.1299,
            "semantic_similarity": 0.5653,
            "all_choices_addressed": 0.43,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.3581
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1149,
            "recall": 0.3817,
            "f1_score": 0.1734,
            "jaccard": 0.0955,
            "bleu": 0.0162,
            "rouge1_f": 0.2102,
            "rouge2_f": 0.0469,
            "rougeL_f": 0.129,
            "semantic_similarity": 0.5412,
            "all_choices_addressed": 0.296,
            "single_paragraph_format": 0.725,
            "explanation_depth": 0.3428
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1068,
            "recall": 0.3763,
            "f1_score": 0.1634,
            "jaccard": 0.0897,
            "bleu": 0.0169,
            "rouge1_f": 0.1928,
            "rouge2_f": 0.0428,
            "rougeL_f": 0.1198,
            "semantic_similarity": 0.573,
            "all_choices_addressed": 0.262,
            "single_paragraph_format": 0.96,
            "explanation_depth": 0.324
          }
        }
      },
      "ecqa_stakeholder_small_business": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0967,
            "recall": 0.3683,
            "f1_score": 0.1505,
            "jaccard": 0.0819,
            "bleu": 0.0162,
            "rouge1_f": 0.1798,
            "rouge2_f": 0.0423,
            "rougeL_f": 0.1151,
            "semantic_similarity": 0.5193,
            "all_choices_addressed": 0.262,
            "single_paragraph_format": 0.949,
            "explanation_depth": 0.2898
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1144,
            "recall": 0.3388,
            "f1_score": 0.1676,
            "jaccard": 0.0921,
            "bleu": 0.0165,
            "rouge1_f": 0.1998,
            "rouge2_f": 0.0413,
            "rougeL_f": 0.124,
            "semantic_similarity": 0.497,
            "all_choices_addressed": 0.27,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.2636
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0974,
            "recall": 0.3392,
            "f1_score": 0.1481,
            "jaccard": 0.0805,
            "bleu": 0.0137,
            "rouge1_f": 0.1902,
            "rouge2_f": 0.0361,
            "rougeL_f": 0.1137,
            "semantic_similarity": 0.5446,
            "all_choices_addressed": 0.442,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.3622
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0927,
            "recall": 0.3935,
            "f1_score": 0.1474,
            "jaccard": 0.0801,
            "bleu": 0.0133,
            "rouge1_f": 0.1659,
            "rouge2_f": 0.0383,
            "rougeL_f": 0.1053,
            "semantic_similarity": 0.4668,
            "all_choices_addressed": 0.276,
            "single_paragraph_format": 0.944,
            "explanation_depth": 0.3303
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0951,
            "recall": 0.3508,
            "f1_score": 0.1468,
            "jaccard": 0.0797,
            "bleu": 0.0121,
            "rouge1_f": 0.1765,
            "rouge2_f": 0.0347,
            "rougeL_f": 0.112,
            "semantic_similarity": 0.4701,
            "all_choices_addressed": 0.266,
            "single_paragraph_format": 0.296,
            "explanation_depth": 0.3524
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1123,
            "recall": 0.3573,
            "f1_score": 0.165,
            "jaccard": 0.0905,
            "bleu": 0.0157,
            "rouge1_f": 0.1939,
            "rouge2_f": 0.044,
            "rougeL_f": 0.1257,
            "semantic_similarity": 0.5277,
            "all_choices_addressed": 0.25,
            "single_paragraph_format": 1.0,
            "explanation_depth": 0.2998
          }
        }
      }
    },
    "ecqa_choice_selection_3choices": {
      "ecqa_choice_selection_3choices": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0349,
            "recall": 0.585,
            "f1_score": 0.0587,
            "jaccard": 0.0322,
            "bleu": 0.0329,
            "rouge1_f": 0.1437,
            "rouge2_f": 0.0464,
            "rougeL_f": 0.1437,
            "semantic_similarity": 0.5449,
            "answer_correctness": 0.77,
            "follows_format_instruction": 0.344,
            "answer_extractability": 0.7244
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0151,
            "recall": 0.325,
            "f1_score": 0.028,
            "jaccard": 0.0148,
            "bleu": 0.0117,
            "rouge1_f": 0.0776,
            "rouge2_f": 0.0268,
            "rougeL_f": 0.0776,
            "semantic_similarity": 0.4717,
            "answer_correctness": 0.72,
            "follows_format_instruction": 0.275,
            "answer_extractability": 0.742
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.43,
            "precision": 0.4433,
            "recall": 0.655,
            "f1_score": 0.4535,
            "jaccard": 0.443,
            "bleu": 0.4247,
            "rouge1_f": 0.4813,
            "rouge2_f": 0.1754,
            "rougeL_f": 0.4813,
            "semantic_similarity": 0.6942,
            "answer_correctness": 0.72,
            "follows_format_instruction": 0.639,
            "answer_extractability": 0.8579
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0544,
            "recall": 0.72,
            "f1_score": 0.0938,
            "jaccard": 0.0544,
            "bleu": 0.0195,
            "rouge1_f": 0.0947,
            "rouge2_f": 0.0375,
            "rougeL_f": 0.0947,
            "semantic_similarity": 0.4854,
            "answer_correctness": 0.71,
            "follows_format_instruction": 0.293,
            "answer_extractability": 0.7715
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.018,
            "recall": 0.475,
            "f1_score": 0.0344,
            "jaccard": 0.0179,
            "bleu": 0.0066,
            "rouge1_f": 0.0407,
            "rouge2_f": 0.0123,
            "rougeL_f": 0.0407,
            "semantic_similarity": 0.4314,
            "answer_correctness": 0.51,
            "follows_format_instruction": 0.196,
            "answer_extractability": 0.8757
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.88,
            "precision": 0.8861,
            "recall": 0.915,
            "f1_score": 0.8871,
            "jaccard": 0.8844,
            "bleu": 0.8653,
            "rouge1_f": 0.8868,
            "rouge2_f": 0.3503,
            "rougeL_f": 0.8868,
            "semantic_similarity": 0.9308,
            "answer_correctness": 0.9,
            "follows_format_instruction": 0.971,
            "answer_extractability": 0.995
          }
        }
      }
    },
    "gmeg": {
      "gmeg_explaination": {
        "hf-qwen3-30b-thinking": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0662,
            "recall": 0.4387,
            "f1_score": 0.1116,
            "jaccard": 0.0601,
            "bleu": 0.0099,
            "rouge1_f": 0.1045,
            "rouge2_f": 0.0282,
            "rougeL_f": 0.0734,
            "semantic_similarity": 0.4586,
            "bullet_point_ratio": 0.1677,
            "correction_terminology_recall": 0.8667,
            "structural_format_match": 0.5,
            "original_text_mention": 0.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0959,
            "recall": 0.3802,
            "f1_score": 0.1462,
            "jaccard": 0.0808,
            "bleu": 0.0174,
            "rouge1_f": 0.1556,
            "rouge2_f": 0.0374,
            "rougeL_f": 0.108,
            "semantic_similarity": 0.4956,
            "bullet_point_ratio": 0.1062,
            "correction_terminology_recall": 0.7067,
            "structural_format_match": 0.68,
            "original_text_mention": 0.0
          }
        },
        "u4b-mistral-7b_ft_gmeg_explanation_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.3252,
            "recall": 0.3299,
            "f1_score": 0.3135,
            "jaccard": 0.1966,
            "bleu": 0.1122,
            "rouge1_f": 0.3518,
            "rouge2_f": 0.1356,
            "rougeL_f": 0.2764,
            "semantic_similarity": 0.6538,
            "bullet_point_ratio": 0.6252,
            "correction_terminology_recall": 0.7775,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0842,
            "recall": 0.2587,
            "f1_score": 0.1203,
            "jaccard": 0.0653,
            "bleu": 0.0096,
            "rouge1_f": 0.1222,
            "rouge2_f": 0.0204,
            "rougeL_f": 0.0908,
            "semantic_similarity": 0.4611,
            "bullet_point_ratio": 0.0941,
            "correction_terminology_recall": 0.5225,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1086,
            "recall": 0.3494,
            "f1_score": 0.1593,
            "jaccard": 0.0888,
            "bleu": 0.0168,
            "rouge1_f": 0.1647,
            "rouge2_f": 0.0419,
            "rougeL_f": 0.1243,
            "semantic_similarity": 0.547,
            "bullet_point_ratio": 0.2559,
            "correction_terminology_recall": 0.7117,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1165,
            "recall": 0.3805,
            "f1_score": 0.1696,
            "jaccard": 0.0953,
            "bleu": 0.0205,
            "rouge1_f": 0.1998,
            "rouge2_f": 0.0444,
            "rougeL_f": 0.1405,
            "semantic_similarity": 0.5152,
            "bullet_point_ratio": 0.2497,
            "correction_terminology_recall": 0.8008,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1048,
            "recall": 0.3391,
            "f1_score": 0.1545,
            "jaccard": 0.0858,
            "bleu": 0.0197,
            "rouge1_f": 0.195,
            "rouge2_f": 0.0438,
            "rougeL_f": 0.1395,
            "semantic_similarity": 0.5267,
            "bullet_point_ratio": 0.455,
            "correction_terminology_recall": 0.7217,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1205,
            "recall": 0.3467,
            "f1_score": 0.1708,
            "jaccard": 0.096,
            "bleu": 0.0182,
            "rouge1_f": 0.1794,
            "rouge2_f": 0.0398,
            "rougeL_f": 0.1252,
            "semantic_similarity": 0.5254,
            "bullet_point_ratio": 0.0752,
            "correction_terminology_recall": 0.65,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        }
      },
      "gmeg_few_shot": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1054,
            "recall": 0.3321,
            "f1_score": 0.1531,
            "jaccard": 0.0848,
            "bleu": 0.0237,
            "rouge1_f": 0.204,
            "rouge2_f": 0.0517,
            "rougeL_f": 0.1422,
            "semantic_similarity": 0.5343,
            "bullet_point_ratio": 0.551,
            "correction_terminology_recall": 0.73,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.12,
            "recall": 0.3779,
            "f1_score": 0.1732,
            "jaccard": 0.0972,
            "bleu": 0.0268,
            "rouge1_f": 0.2119,
            "rouge2_f": 0.0499,
            "rougeL_f": 0.1461,
            "semantic_similarity": 0.5276,
            "bullet_point_ratio": 0.2878,
            "correction_terminology_recall": 0.7558,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1373,
            "recall": 0.35,
            "f1_score": 0.1855,
            "jaccard": 0.1052,
            "bleu": 0.0269,
            "rouge1_f": 0.2,
            "rouge2_f": 0.051,
            "rougeL_f": 0.1486,
            "semantic_similarity": 0.5092,
            "bullet_point_ratio": 0.1351,
            "correction_terminology_recall": 0.67,
            "structural_format_match": 0.9,
            "original_text_mention": 0.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.09,
            "recall": 0.2707,
            "f1_score": 0.1272,
            "jaccard": 0.0696,
            "bleu": 0.0106,
            "rouge1_f": 0.1275,
            "rouge2_f": 0.0188,
            "rougeL_f": 0.0951,
            "semantic_similarity": 0.4276,
            "bullet_point_ratio": 0.0357,
            "correction_terminology_recall": 0.6317,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1158,
            "recall": 0.3969,
            "f1_score": 0.1708,
            "jaccard": 0.0962,
            "bleu": 0.0266,
            "rouge1_f": 0.173,
            "rouge2_f": 0.045,
            "rougeL_f": 0.1253,
            "semantic_similarity": 0.4864,
            "bullet_point_ratio": 0.0512,
            "correction_terminology_recall": 0.8342,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1285,
            "recall": 0.3712,
            "f1_score": 0.1827,
            "jaccard": 0.1037,
            "bleu": 0.0262,
            "rouge1_f": 0.1933,
            "rouge2_f": 0.0558,
            "rougeL_f": 0.1434,
            "semantic_similarity": 0.5686,
            "bullet_point_ratio": 0.277,
            "correction_terminology_recall": 0.7425,
            "structural_format_match": 0.91,
            "original_text_mention": 0.0
          }
        }
      }
    },
    "ecqa_positive": {
      "ecqa_positive_explanation": {
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2475,
            "recall": 0.3542,
            "f1_score": 0.28,
            "jaccard": 0.171,
            "bleu": 0.059,
            "rouge1_f": 0.3548,
            "rouge2_f": 0.1321,
            "rougeL_f": 0.2596,
            "semantic_similarity": 0.6851,
            "simple_atomic_sentences": 0.6031,
            "correct_answer_focus": 0.714,
            "concise_justification": 0.8469
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2644,
            "recall": 0.3578,
            "f1_score": 0.2929,
            "jaccard": 0.1777,
            "bleu": 0.0715,
            "rouge1_f": 0.3622,
            "rouge2_f": 0.1363,
            "rougeL_f": 0.2661,
            "semantic_similarity": 0.6797,
            "simple_atomic_sentences": 0.574,
            "correct_answer_focus": 0.9045,
            "concise_justification": 0.8418
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2828,
            "recall": 0.3362,
            "f1_score": 0.2968,
            "jaccard": 0.1813,
            "bleu": 0.0694,
            "rouge1_f": 0.3601,
            "rouge2_f": 0.138,
            "rougeL_f": 0.259,
            "semantic_similarity": 0.7156,
            "simple_atomic_sentences": 0.6341,
            "correct_answer_focus": 0.8645,
            "concise_justification": 0.8899
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2886,
            "recall": 0.3723,
            "f1_score": 0.2804,
            "jaccard": 0.1731,
            "bleu": 0.0549,
            "rouge1_f": 0.2747,
            "rouge2_f": 0.1121,
            "rougeL_f": 0.2184,
            "semantic_similarity": 0.6046,
            "simple_atomic_sentences": 0.8997,
            "correct_answer_focus": 0.6885,
            "concise_justification": 0.7845
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2299,
            "recall": 0.3857,
            "f1_score": 0.2703,
            "jaccard": 0.1631,
            "bleu": 0.0558,
            "rouge1_f": 0.3197,
            "rouge2_f": 0.1231,
            "rougeL_f": 0.2393,
            "semantic_similarity": 0.6743,
            "simple_atomic_sentences": 0.5588,
            "correct_answer_focus": 0.922,
            "concise_justification": 0.8477
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.3747,
            "recall": 0.2673,
            "f1_score": 0.2917,
            "jaccard": 0.1802,
            "bleu": 0.0719,
            "rouge1_f": 0.3659,
            "rouge2_f": 0.15,
            "rougeL_f": 0.303,
            "semantic_similarity": 0.673,
            "simple_atomic_sentences": 0.8377,
            "correct_answer_focus": 0.685,
            "concise_justification": 0.9619
          }
        },
        "u4b-mistral-7b_ft_ecqa_positive_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.289,
            "recall": 0.5113,
            "f1_score": 0.3469,
            "jaccard": 0.2182,
            "bleu": 0.0207,
            "rouge1_f": 0.1233,
            "rouge2_f": 0.0566,
            "rougeL_f": 0.1126,
            "semantic_similarity": 0.6373,
            "simple_atomic_sentences": 0.946,
            "correct_answer_focus": 0.7935,
            "concise_justification": 0.5112
          }
        }
      },
      "ecqa_positive_explanation_formatted": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1822,
            "recall": 0.4071,
            "f1_score": 0.2381,
            "jaccard": 0.1394,
            "bleu": 0.0429,
            "rouge1_f": 0.2578,
            "rouge2_f": 0.0901,
            "rougeL_f": 0.1919,
            "semantic_similarity": 0.6837,
            "simple_atomic_sentences": 0.7059,
            "correct_answer_focus": 0.646,
            "concise_justification": 0.8412
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.243,
            "recall": 0.2886,
            "f1_score": 0.2487,
            "jaccard": 0.1476,
            "bleu": 0.0473,
            "rouge1_f": 0.3295,
            "rouge2_f": 0.1006,
            "rougeL_f": 0.2401,
            "semantic_similarity": 0.6961,
            "simple_atomic_sentences": 0.9613,
            "correct_answer_focus": 0.4605,
            "concise_justification": 0.9515
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2178,
            "recall": 0.4059,
            "f1_score": 0.2735,
            "jaccard": 0.1631,
            "bleu": 0.0513,
            "rouge1_f": 0.3096,
            "rouge2_f": 0.1035,
            "rougeL_f": 0.2228,
            "semantic_similarity": 0.696,
            "simple_atomic_sentences": 0.9325,
            "correct_answer_focus": 0.6465,
            "concise_justification": 0.894
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1593,
            "recall": 0.4398,
            "f1_score": 0.2267,
            "jaccard": 0.1305,
            "bleu": 0.0372,
            "rouge1_f": 0.2714,
            "rouge2_f": 0.095,
            "rougeL_f": 0.2013,
            "semantic_similarity": 0.6959,
            "simple_atomic_sentences": 0.8659,
            "correct_answer_focus": 0.4335,
            "concise_justification": 0.6778
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2271,
            "recall": 0.3391,
            "f1_score": 0.26,
            "jaccard": 0.1534,
            "bleu": 0.0499,
            "rouge1_f": 0.3248,
            "rouge2_f": 0.1069,
            "rougeL_f": 0.2445,
            "semantic_similarity": 0.7357,
            "simple_atomic_sentences": 0.9551,
            "correct_answer_focus": 0.668,
            "concise_justification": 0.9486
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1775,
            "recall": 0.4398,
            "f1_score": 0.2426,
            "jaccard": 0.1412,
            "bleu": 0.0391,
            "rouge1_f": 0.2722,
            "rouge2_f": 0.0914,
            "rougeL_f": 0.1936,
            "semantic_similarity": 0.701,
            "simple_atomic_sentences": 0.718,
            "correct_answer_focus": 0.744,
            "concise_justification": 0.8521
          }
        }
      }
    },
    "gmeg_ours": {
      "gmeg_few_shot_original": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1156,
            "recall": 0.4122,
            "f1_score": 0.1667,
            "jaccard": 0.0939,
            "bleu": 0.0247,
            "rouge1_f": 0.1696,
            "rouge2_f": 0.0502,
            "rougeL_f": 0.1202,
            "semantic_similarity": 0.5409,
            "bullet_point_ratio": 0.2767,
            "correction_terminology_recall": 0.7617,
            "structural_format_match": 0.65,
            "original_text_mention": 0.7313
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1153,
            "recall": 0.3602,
            "f1_score": 0.1685,
            "jaccard": 0.0946,
            "bleu": 0.0262,
            "rouge1_f": 0.1895,
            "rouge2_f": 0.0448,
            "rougeL_f": 0.126,
            "semantic_similarity": 0.4967,
            "bullet_point_ratio": 0.2826,
            "correction_terminology_recall": 0.7058,
            "structural_format_match": 0.91,
            "original_text_mention": 0.994
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0983,
            "recall": 0.3533,
            "f1_score": 0.1479,
            "jaccard": 0.0813,
            "bleu": 0.0207,
            "rouge1_f": 0.1549,
            "rouge2_f": 0.0355,
            "rougeL_f": 0.1079,
            "semantic_similarity": 0.4878,
            "bullet_point_ratio": 0.4494,
            "correction_terminology_recall": 0.6217,
            "structural_format_match": 0.91,
            "original_text_mention": 0.9859
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0978,
            "recall": 0.2784,
            "f1_score": 0.1383,
            "jaccard": 0.0759,
            "bleu": 0.0175,
            "rouge1_f": 0.1443,
            "rouge2_f": 0.0258,
            "rougeL_f": 0.1023,
            "semantic_similarity": 0.3812,
            "bullet_point_ratio": 0.2765,
            "correction_terminology_recall": 0.565,
            "structural_format_match": 0.91,
            "original_text_mention": 0.9498
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1102,
            "recall": 0.3484,
            "f1_score": 0.1595,
            "jaccard": 0.089,
            "bleu": 0.0236,
            "rouge1_f": 0.1698,
            "rouge2_f": 0.0345,
            "rougeL_f": 0.1167,
            "semantic_similarity": 0.4569,
            "bullet_point_ratio": 0.1315,
            "correction_terminology_recall": 0.6808,
            "structural_format_match": 0.9,
            "original_text_mention": 0.6454
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1163,
            "recall": 0.3287,
            "f1_score": 0.1647,
            "jaccard": 0.0918,
            "bleu": 0.0248,
            "rouge1_f": 0.1909,
            "rouge2_f": 0.0459,
            "rougeL_f": 0.1282,
            "semantic_similarity": 0.5034,
            "bullet_point_ratio": 0.5137,
            "correction_terminology_recall": 0.6458,
            "structural_format_match": 0.91,
            "original_text_mention": 0.9841
          }
        }
      },
      "gmeg_basic": {
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1665,
            "recall": 0.2933,
            "f1_score": 0.1945,
            "jaccard": 0.1112,
            "bleu": 0.0356,
            "rouge1_f": 0.2425,
            "rouge2_f": 0.0596,
            "rougeL_f": 0.1708,
            "semantic_similarity": 0.5141,
            "bullet_point_ratio": 0.263,
            "correction_terminology_recall": 0.8275,
            "structural_format_match": 0.91,
            "original_text_mention": 0.4102
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1832,
            "recall": 0.2443,
            "f1_score": 0.1948,
            "jaccard": 0.1123,
            "bleu": 0.0365,
            "rouge1_f": 0.2331,
            "rouge2_f": 0.0515,
            "rougeL_f": 0.1671,
            "semantic_similarity": 0.482,
            "bullet_point_ratio": 0.2109,
            "correction_terminology_recall": 0.7442,
            "structural_format_match": 0.91,
            "original_text_mention": 0.4236
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1341,
            "recall": 0.268,
            "f1_score": 0.1694,
            "jaccard": 0.0951,
            "bleu": 0.0312,
            "rouge1_f": 0.2429,
            "rouge2_f": 0.0525,
            "rougeL_f": 0.1719,
            "semantic_similarity": 0.5069,
            "bullet_point_ratio": 0.5931,
            "correction_terminology_recall": 0.7317,
            "structural_format_match": 0.91,
            "original_text_mention": 0.2661
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1129,
            "recall": 0.2978,
            "f1_score": 0.1549,
            "jaccard": 0.0861,
            "bleu": 0.023,
            "rouge1_f": 0.1774,
            "rouge2_f": 0.041,
            "rougeL_f": 0.1231,
            "semantic_similarity": 0.4819,
            "bullet_point_ratio": 0.0905,
            "correction_terminology_recall": 0.7858,
            "structural_format_match": 0.3,
            "original_text_mention": 0.4917
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1241,
            "recall": 0.3173,
            "f1_score": 0.1596,
            "jaccard": 0.0903,
            "bleu": 0.0236,
            "rouge1_f": 0.1683,
            "rouge2_f": 0.0359,
            "rougeL_f": 0.1193,
            "semantic_similarity": 0.4719,
            "bullet_point_ratio": 0.2238,
            "correction_terminology_recall": 0.8383,
            "structural_format_match": 0.89,
            "original_text_mention": 0.447
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0909,
            "recall": 0.1946,
            "f1_score": 0.1147,
            "jaccard": 0.0623,
            "bleu": 0.0143,
            "rouge1_f": 0.1464,
            "rouge2_f": 0.0238,
            "rougeL_f": 0.1068,
            "semantic_similarity": 0.4206,
            "bullet_point_ratio": 0.0798,
            "correction_terminology_recall": 0.7875,
            "structural_format_match": 0.91,
            "original_text_mention": 0.7313
          }
        }
      }
    },
    "pubmedqa_reasoning_free": {
      "pubmedqa_expert_primary_care": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.05,
            "precision": 0.05,
            "recall": 0.05,
            "f1_score": 0.05,
            "jaccard": 0.05,
            "bleu": 0.0,
            "rouge1_f": 0.0505,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0505,
            "semantic_similarity": 0.5664,
            "answer_correctness": 0.06,
            "strict_format_compliance": 0.78,
            "no_explanation_penalty": 0.98
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.08,
            "precision": 0.0804,
            "recall": 0.11,
            "f1_score": 0.0808,
            "jaccard": 0.0804,
            "bleu": 0.0,
            "rouge1_f": 0.272,
            "rouge2_f": 0.0,
            "rougeL_f": 0.272,
            "semantic_similarity": 0.3489,
            "answer_correctness": 0.35,
            "strict_format_compliance": 0.216,
            "no_explanation_penalty": 0.38
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0059,
            "recall": 0.06,
            "f1_score": 0.0103,
            "jaccard": 0.0059,
            "bleu": 0.0885,
            "rouge1_f": 0.3042,
            "rouge2_f": 0.0,
            "rougeL_f": 0.3042,
            "semantic_similarity": 0.4178,
            "answer_correctness": 0.6,
            "strict_format_compliance": 0.169,
            "no_explanation_penalty": 0.462
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.63,
            "precision": 0.63,
            "recall": 0.63,
            "f1_score": 0.63,
            "jaccard": 0.63,
            "bleu": 0.0,
            "rouge1_f": 0.63,
            "rouge2_f": 0.0,
            "rougeL_f": 0.63,
            "semantic_similarity": 0.8587,
            "answer_correctness": 0.63,
            "strict_format_compliance": 0.8,
            "no_explanation_penalty": 1.0
          }
        }
      },
      "pubmedqa_expert_biostatistician": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.01,
            "recall": 0.01,
            "f1_score": 0.01,
            "jaccard": 0.01,
            "bleu": 0.0,
            "rouge1_f": 0.0304,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0304,
            "semantic_similarity": 0.4751,
            "answer_correctness": 0.05,
            "strict_format_compliance": 0.372,
            "no_explanation_penalty": 0.88
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.32,
            "precision": 0.32,
            "recall": 0.32,
            "f1_score": 0.32,
            "jaccard": 0.32,
            "bleu": 0.09,
            "rouge1_f": 0.32,
            "rouge2_f": 0.0,
            "rougeL_f": 0.32,
            "semantic_similarity": 0.7084,
            "answer_correctness": 0.32,
            "strict_format_compliance": 0.822,
            "no_explanation_penalty": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.12,
            "precision": 0.1204,
            "recall": 0.17,
            "f1_score": 0.1208,
            "jaccard": 0.1204,
            "bleu": 0.0,
            "rouge1_f": 0.1406,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1406,
            "semantic_similarity": 0.1548,
            "answer_correctness": 0.18,
            "strict_format_compliance": 0.132,
            "no_explanation_penalty": 0.18
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0001,
            "recall": 0.01,
            "f1_score": 0.0002,
            "jaccard": 0.0001,
            "bleu": 0.0436,
            "rouge1_f": 0.1586,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1586,
            "semantic_similarity": 0.3501,
            "answer_correctness": 0.5,
            "strict_format_compliance": 0.031,
            "no_explanation_penalty": 0.302
          }
        }
      },
      "pubmedqa_reasoning_free_min": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.62,
            "precision": 0.645,
            "recall": 0.67,
            "f1_score": 0.6533,
            "jaccard": 0.645,
            "bleu": 0.0138,
            "rouge1_f": 0.6864,
            "rouge2_f": 0.0,
            "rougeL_f": 0.6864,
            "semantic_similarity": 0.8583,
            "answer_correctness": 0.73,
            "strict_format_compliance": 0.708,
            "no_explanation_penalty": 0.932
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.13,
            "precision": 0.13,
            "recall": 0.13,
            "f1_score": 0.13,
            "jaccard": 0.13,
            "bleu": 0.0,
            "rouge1_f": 0.8343,
            "rouge2_f": 0.0,
            "rougeL_f": 0.8343,
            "semantic_similarity": 0.8686,
            "answer_correctness": 0.85,
            "strict_format_compliance": 0.45,
            "no_explanation_penalty": 0.984
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0005,
            "recall": 0.03,
            "f1_score": 0.001,
            "jaccard": 0.0005,
            "bleu": 0.0006,
            "rouge1_f": 0.0902,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0902,
            "semantic_similarity": 0.1168,
            "answer_correctness": 0.52,
            "strict_format_compliance": 0.032,
            "no_explanation_penalty": 0.08
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0014,
            "recall": 0.05,
            "f1_score": 0.0028,
            "jaccard": 0.0014,
            "bleu": 0.0005,
            "rouge1_f": 0.0253,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0253,
            "semantic_similarity": 0.0454,
            "answer_correctness": 0.66,
            "strict_format_compliance": 0.0,
            "no_explanation_penalty": 0.002
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0008,
            "recall": 0.06,
            "f1_score": 0.0016,
            "jaccard": 0.0008,
            "bleu": 0.0003,
            "rouge1_f": 0.0047,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0047,
            "semantic_similarity": 0.0828,
            "answer_correctness": 0.12,
            "strict_format_compliance": 0.0,
            "no_explanation_penalty": 0.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0441,
            "recall": 0.32,
            "f1_score": 0.0758,
            "jaccard": 0.0441,
            "bleu": 0.0771,
            "rouge1_f": 0.3512,
            "rouge2_f": 0.0,
            "rougeL_f": 0.3512,
            "semantic_similarity": 0.5152,
            "answer_correctness": 0.87,
            "strict_format_compliance": 0.114,
            "no_explanation_penalty": 0.452
          }
        }
      },
      "pubmedqa_reasoning_free": {
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.73,
            "precision": 0.73,
            "recall": 0.73,
            "f1_score": 0.73,
            "jaccard": 0.73,
            "bleu": 0.0,
            "rouge1_f": 0.73,
            "rouge2_f": 0.0,
            "rougeL_f": 0.73,
            "semantic_similarity": 0.8938,
            "answer_correctness": 0.73,
            "strict_format_compliance": 0.8,
            "no_explanation_penalty": 1.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.15,
            "precision": 0.15,
            "recall": 0.15,
            "f1_score": 0.15,
            "jaccard": 0.15,
            "bleu": 0.15,
            "rouge1_f": 0.15,
            "rouge2_f": 0.0,
            "rougeL_f": 0.15,
            "semantic_similarity": 0.6201,
            "answer_correctness": 0.15,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.68,
            "precision": 0.6802,
            "recall": 0.69,
            "f1_score": 0.6805,
            "jaccard": 0.6802,
            "bleu": 0.0,
            "rouge1_f": 0.7008,
            "rouge2_f": 0.0,
            "rougeL_f": 0.7008,
            "semantic_similarity": 0.8674,
            "answer_correctness": 0.72,
            "strict_format_compliance": 0.74,
            "no_explanation_penalty": 0.952
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0091,
            "recall": 0.06,
            "f1_score": 0.0158,
            "jaccard": 0.0091,
            "bleu": 0.2765,
            "rouge1_f": 0.741,
            "rouge2_f": 0.0,
            "rougeL_f": 0.741,
            "semantic_similarity": 0.7479,
            "answer_correctness": 0.9,
            "strict_format_compliance": 0.346,
            "no_explanation_penalty": 0.818
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.08,
            "precision": 0.0813,
            "recall": 0.09,
            "f1_score": 0.0822,
            "jaccard": 0.0813,
            "bleu": 0.0804,
            "rouge1_f": 0.082,
            "rouge2_f": 0.0,
            "rougeL_f": 0.082,
            "semantic_similarity": 0.5798,
            "answer_correctness": 0.09,
            "strict_format_compliance": 0.936,
            "no_explanation_penalty": 0.96
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.8,
            "precision": 0.8,
            "recall": 0.8,
            "f1_score": 0.8,
            "jaccard": 0.8,
            "bleu": 0.02,
            "rouge1_f": 0.8,
            "rouge2_f": 0.0,
            "rougeL_f": 0.8,
            "semantic_similarity": 0.9331,
            "answer_correctness": 0.8,
            "strict_format_compliance": 0.804,
            "no_explanation_penalty": 1.0
          }
        }
      },
      "pubmedqa_expert_epidemiologist": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.004,
            "recall": 0.03,
            "f1_score": 0.007,
            "jaccard": 0.004,
            "bleu": 0.0867,
            "rouge1_f": 0.2857,
            "rouge2_f": 0.0,
            "rougeL_f": 0.2857,
            "semantic_similarity": 0.4006,
            "answer_correctness": 0.64,
            "strict_format_compliance": 0.102,
            "no_explanation_penalty": 0.382
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.0405,
            "recall": 0.09,
            "f1_score": 0.0411,
            "jaccard": 0.0405,
            "bleu": 0.0001,
            "rouge1_f": 0.1913,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1913,
            "semantic_similarity": 0.2157,
            "answer_correctness": 0.25,
            "strict_format_compliance": 0.112,
            "no_explanation_penalty": 0.22
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.44,
            "precision": 0.44,
            "recall": 0.44,
            "f1_score": 0.44,
            "jaccard": 0.44,
            "bleu": 0.01,
            "rouge1_f": 0.44,
            "rouge2_f": 0.0,
            "rougeL_f": 0.44,
            "semantic_similarity": 0.7667,
            "answer_correctness": 0.44,
            "strict_format_compliance": 0.802,
            "no_explanation_penalty": 1.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.01,
            "recall": 0.01,
            "f1_score": 0.01,
            "jaccard": 0.01,
            "bleu": 0.0,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5361,
            "answer_correctness": 0.04,
            "strict_format_compliance": 0.548,
            "no_explanation_penalty": 0.99
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.04,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5697,
            "answer_correctness": 0.04,
            "strict_format_compliance": 1.0,
            "no_explanation_penalty": 1.0
          }
        }
      },
      "pubmedqa_expert_impersonation": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.62,
            "precision": 0.62,
            "recall": 0.62,
            "f1_score": 0.62,
            "jaccard": 0.62,
            "bleu": 0.0,
            "rouge1_f": 0.68,
            "rouge2_f": 0.0,
            "rougeL_f": 0.68,
            "semantic_similarity": 0.8742,
            "answer_correctness": 0.68,
            "strict_format_compliance": 0.764,
            "no_explanation_penalty": 0.99
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.07,
            "precision": 0.0701,
            "recall": 0.08,
            "f1_score": 0.0702,
            "jaccard": 0.0701,
            "bleu": 0.0,
            "rouge1_f": 0.166,
            "rouge2_f": 0.0,
            "rougeL_f": 0.166,
            "semantic_similarity": 0.2304,
            "answer_correctness": 0.37,
            "strict_format_compliance": 0.1,
            "no_explanation_penalty": 0.17
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.04,
            "recall": 0.04,
            "f1_score": 0.04,
            "jaccard": 0.04,
            "bleu": 0.02,
            "rouge1_f": 0.04,
            "rouge2_f": 0.0,
            "rougeL_f": 0.04,
            "semantic_similarity": 0.5712,
            "answer_correctness": 0.04,
            "strict_format_compliance": 0.882,
            "no_explanation_penalty": 1.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0002,
            "recall": 0.02,
            "f1_score": 0.0003,
            "jaccard": 0.0002,
            "bleu": 0.0004,
            "rouge1_f": 0.0032,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0032,
            "semantic_similarity": 0.2578,
            "answer_correctness": 0.05,
            "strict_format_compliance": 0.16,
            "no_explanation_penalty": 0.414
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "jaccard": 0.0,
            "bleu": 0.0488,
            "rouge1_f": 0.3515,
            "rouge2_f": 0.0,
            "rougeL_f": 0.3515,
            "semantic_similarity": 0.4455,
            "answer_correctness": 0.79,
            "strict_format_compliance": 0.165,
            "no_explanation_penalty": 0.404
          }
        }
      }
    },
    "ecare_choice_selection": {
      "ecare_choice_selection_expert_medical": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.02,
            "precision": 0.0247,
            "recall": 0.49,
            "f1_score": 0.0293,
            "jaccard": 0.0247,
            "bleu": 0.0214,
            "rouge1_f": 0.0294,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0294,
            "semantic_similarity": 0.11,
            "answer_correctness": 0.47,
            "follows_format_instruction": 0.216,
            "answer_extractability": 0.826
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0039,
            "recall": 0.22,
            "f1_score": 0.0076,
            "jaccard": 0.0039,
            "bleu": 0.0015,
            "rouge1_f": 0.0095,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0095,
            "semantic_similarity": 0.0602,
            "answer_correctness": 0.36,
            "follows_format_instruction": 0.166,
            "answer_extractability": 0.758
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0089,
            "recall": 0.57,
            "f1_score": 0.0174,
            "jaccard": 0.0089,
            "bleu": 0.0026,
            "rouge1_f": 0.0169,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0169,
            "semantic_similarity": 0.0806,
            "answer_correctness": 0.48,
            "follows_format_instruction": 0.192,
            "answer_extractability": 0.734
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0016,
            "recall": 0.14,
            "f1_score": 0.0032,
            "jaccard": 0.0016,
            "bleu": 0.001,
            "rouge1_f": 0.0063,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0063,
            "semantic_similarity": 0.0463,
            "answer_correctness": 0.46,
            "follows_format_instruction": 0.18,
            "answer_extractability": 0.7468
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.024,
            "recall": 0.72,
            "f1_score": 0.0425,
            "jaccard": 0.024,
            "bleu": 0.007,
            "rouge1_f": 0.0385,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0385,
            "semantic_similarity": 0.0787,
            "answer_correctness": 0.48,
            "follows_format_instruction": 0.206,
            "answer_extractability": 0.936
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.008,
            "recall": 0.69,
            "f1_score": 0.0158,
            "jaccard": 0.008,
            "bleu": 0.002,
            "rouge1_f": 0.0126,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0126,
            "semantic_similarity": 0.0892,
            "answer_correctness": 0.46,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.808
          }
        }
      },
      "ecare_choice_selection": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0049,
            "recall": 0.21,
            "f1_score": 0.0095,
            "jaccard": 0.0049,
            "bleu": 0.0034,
            "rouge1_f": 0.0208,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0208,
            "semantic_similarity": 0.1205,
            "answer_correctness": 0.51,
            "follows_format_instruction": 0.18,
            "answer_extractability": 0.7364
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0214,
            "recall": 0.92,
            "f1_score": 0.0395,
            "jaccard": 0.0214,
            "bleu": 0.0063,
            "rouge1_f": 0.0307,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0307,
            "semantic_similarity": 0.1849,
            "answer_correctness": 0.58,
            "follows_format_instruction": 0.221,
            "answer_extractability": 0.872
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.04,
            "precision": 0.055,
            "recall": 0.87,
            "f1_score": 0.0695,
            "jaccard": 0.055,
            "bleu": 0.0435,
            "rouge1_f": 0.0619,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0619,
            "semantic_similarity": 0.2019,
            "answer_correctness": 0.5,
            "follows_format_instruction": 0.256,
            "answer_extractability": 0.8044
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0097,
            "recall": 0.51,
            "f1_score": 0.0191,
            "jaccard": 0.0097,
            "bleu": 0.0026,
            "rouge1_f": 0.0165,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0165,
            "semantic_similarity": 0.0954,
            "answer_correctness": 0.36,
            "follows_format_instruction": 0.17,
            "answer_extractability": 0.836
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0205,
            "recall": 0.82,
            "f1_score": 0.0398,
            "jaccard": 0.0205,
            "bleu": 0.0052,
            "rouge1_f": 0.032,
            "rouge2_f": 0.0,
            "rougeL_f": 0.032,
            "semantic_similarity": 0.1002,
            "answer_correctness": 0.59,
            "follows_format_instruction": 0.198,
            "answer_extractability": 0.804
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.11,
            "precision": 0.1338,
            "recall": 0.75,
            "f1_score": 0.153,
            "jaccard": 0.1338,
            "bleu": 0.1177,
            "rouge1_f": 0.1513,
            "rouge2_f": 0.0,
            "rougeL_f": 0.1513,
            "semantic_similarity": 0.2506,
            "answer_correctness": 0.58,
            "follows_format_instruction": 0.365,
            "answer_extractability": 0.962
          }
        }
      },
      "ecare_choice_selection_demographic_young_american": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0006,
            "recall": 0.06,
            "f1_score": 0.0013,
            "jaccard": 0.0006,
            "bleu": 0.001,
            "rouge1_f": 0.0063,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0063,
            "semantic_similarity": 0.0637,
            "answer_correctness": 0.27,
            "follows_format_instruction": 0.136,
            "answer_extractability": 0.526
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.06,
            "precision": 0.0828,
            "recall": 0.76,
            "f1_score": 0.1011,
            "jaccard": 0.0828,
            "bleu": 0.0666,
            "rouge1_f": 0.0958,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0958,
            "semantic_similarity": 0.1547,
            "answer_correctness": 0.5,
            "follows_format_instruction": 0.274,
            "answer_extractability": 0.946
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.006,
            "recall": 0.41,
            "f1_score": 0.0118,
            "jaccard": 0.006,
            "bleu": 0.0024,
            "rouge1_f": 0.0152,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0152,
            "semantic_similarity": 0.0803,
            "answer_correctness": 0.51,
            "follows_format_instruction": 0.196,
            "answer_extractability": 0.744
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0041,
            "recall": 0.37,
            "f1_score": 0.0081,
            "jaccard": 0.0041,
            "bleu": 0.0015,
            "rouge1_f": 0.0099,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0099,
            "semantic_similarity": 0.0852,
            "answer_correctness": 0.45,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.8272
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0081,
            "recall": 0.81,
            "f1_score": 0.016,
            "jaccard": 0.0081,
            "bleu": 0.0019,
            "rouge1_f": 0.0118,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0118,
            "semantic_similarity": 0.1191,
            "answer_correctness": 0.53,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.854
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0031,
            "recall": 0.23,
            "f1_score": 0.0062,
            "jaccard": 0.0031,
            "bleu": 0.0013,
            "rouge1_f": 0.0081,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0081,
            "semantic_similarity": 0.0708,
            "answer_correctness": 0.29,
            "follows_format_instruction": 0.113,
            "answer_extractability": 0.488
          }
        }
      },
      "ecare_choice_selection_expert_psychologist": {
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0004,
            "recall": 0.05,
            "f1_score": 0.0008,
            "jaccard": 0.0004,
            "bleu": 0.0008,
            "rouge1_f": 0.0054,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0054,
            "semantic_similarity": 0.0554,
            "answer_correctness": 0.47,
            "follows_format_instruction": 0.196,
            "answer_extractability": 0.79
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.0214,
            "recall": 0.72,
            "f1_score": 0.0324,
            "jaccard": 0.0214,
            "bleu": 0.0128,
            "rouge1_f": 0.0273,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0273,
            "semantic_similarity": 0.0808,
            "answer_correctness": 0.49,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.88
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.004,
            "recall": 0.34,
            "f1_score": 0.0079,
            "jaccard": 0.004,
            "bleu": 0.0016,
            "rouge1_f": 0.0105,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0105,
            "semantic_similarity": 0.0798,
            "answer_correctness": 0.48,
            "follows_format_instruction": 0.196,
            "answer_extractability": 0.816
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0035,
            "recall": 0.34,
            "f1_score": 0.007,
            "jaccard": 0.0035,
            "bleu": 0.0014,
            "rouge1_f": 0.0094,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0094,
            "semantic_similarity": 0.0866,
            "answer_correctness": 0.47,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.9076
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0057,
            "recall": 0.34,
            "f1_score": 0.0111,
            "jaccard": 0.0057,
            "bleu": 0.0023,
            "rouge1_f": 0.0143,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0143,
            "semantic_similarity": 0.1095,
            "answer_correctness": 0.44,
            "follows_format_instruction": 0.187,
            "answer_extractability": 0.7448
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.0161,
            "recall": 0.65,
            "f1_score": 0.022,
            "jaccard": 0.0161,
            "bleu": 0.0113,
            "rouge1_f": 0.0183,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0183,
            "semantic_similarity": 0.103,
            "answer_correctness": 0.46,
            "follows_format_instruction": 0.208,
            "answer_extractability": 0.79
          }
        }
      },
      "ecare_choice_selection_stakeholder_parent": {
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.03,
            "precision": 0.0421,
            "recall": 0.69,
            "f1_score": 0.0536,
            "jaccard": 0.0421,
            "bleu": 0.0336,
            "rouge1_f": 0.0515,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0515,
            "semantic_similarity": 0.1095,
            "answer_correctness": 0.47,
            "follows_format_instruction": 0.227,
            "answer_extractability": 0.964
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0056,
            "recall": 0.37,
            "f1_score": 0.0109,
            "jaccard": 0.0056,
            "bleu": 0.002,
            "rouge1_f": 0.0119,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0119,
            "semantic_similarity": 0.1036,
            "answer_correctness": 0.3,
            "follows_format_instruction": 0.138,
            "answer_extractability": 0.637
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0036,
            "recall": 0.38,
            "f1_score": 0.0072,
            "jaccard": 0.0036,
            "bleu": 0.0014,
            "rouge1_f": 0.009,
            "rouge2_f": 0.0,
            "rougeL_f": 0.009,
            "semantic_similarity": 0.071,
            "answer_correctness": 0.46,
            "follows_format_instruction": 0.2,
            "answer_extractability": 0.862
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.02,
            "precision": 0.0297,
            "recall": 0.51,
            "f1_score": 0.0376,
            "jaccard": 0.0297,
            "bleu": 0.023,
            "rouge1_f": 0.0345,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0345,
            "semantic_similarity": 0.1157,
            "answer_correctness": 0.48,
            "follows_format_instruction": 0.233,
            "answer_extractability": 0.844
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0005,
            "recall": 0.05,
            "f1_score": 0.001,
            "jaccard": 0.0005,
            "bleu": 0.0009,
            "rouge1_f": 0.0057,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0057,
            "semantic_similarity": 0.0593,
            "answer_correctness": 0.41,
            "follows_format_instruction": 0.174,
            "answer_extractability": 0.5954
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0042,
            "recall": 0.4,
            "f1_score": 0.0083,
            "jaccard": 0.0042,
            "bleu": 0.0014,
            "rouge1_f": 0.0094,
            "rouge2_f": 0.0,
            "rougeL_f": 0.0094,
            "semantic_similarity": 0.0669,
            "answer_correctness": 0.49,
            "follows_format_instruction": 0.196,
            "answer_extractability": 0.806
          }
        }
      }
    },
    "RHAI_cose_explanation": {
      "cose_explanation": {
        "u4b-mistral-7b_ft_cose_explanation_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2556,
            "recall": 0.1683,
            "f1_score": 0.1777,
            "jaccard": 0.1027,
            "bleu": 0.0311,
            "rouge1_f": 0.259,
            "rouge2_f": 0.0726,
            "rougeL_f": 0.1934,
            "semantic_similarity": 0.543,
            "answer_identification_correctness": 0.625,
            "explanation_quality": 0.506,
            "response_cleanliness": 1.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1569,
            "recall": 0.1314,
            "f1_score": 0.1211,
            "jaccard": 0.0691,
            "bleu": 0.0229,
            "rouge1_f": 0.2155,
            "rouge2_f": 0.049,
            "rougeL_f": 0.1618,
            "semantic_similarity": 0.4523,
            "answer_identification_correctness": 0.695,
            "explanation_quality": 0.41,
            "response_cleanliness": 1.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2469,
            "recall": 0.2007,
            "f1_score": 0.2091,
            "jaccard": 0.1212,
            "bleu": 0.0377,
            "rouge1_f": 0.2956,
            "rouge2_f": 0.0763,
            "rougeL_f": 0.2086,
            "semantic_similarity": 0.5535,
            "answer_identification_correctness": 0.62,
            "explanation_quality": 0.64,
            "response_cleanliness": 1.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1723,
            "recall": 0.3034,
            "f1_score": 0.2033,
            "jaccard": 0.1157,
            "bleu": 0.0266,
            "rouge1_f": 0.2655,
            "rouge2_f": 0.0635,
            "rougeL_f": 0.183,
            "semantic_similarity": 0.5586,
            "answer_identification_correctness": 0.705,
            "explanation_quality": 0.828,
            "response_cleanliness": 0.943
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1576,
            "recall": 0.1601,
            "f1_score": 0.1395,
            "jaccard": 0.0784,
            "bleu": 0.0255,
            "rouge1_f": 0.2095,
            "rouge2_f": 0.0464,
            "rougeL_f": 0.1604,
            "semantic_similarity": 0.4501,
            "answer_identification_correctness": 0.615,
            "explanation_quality": 0.506,
            "response_cleanliness": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1978,
            "recall": 0.2222,
            "f1_score": 0.1931,
            "jaccard": 0.1111,
            "bleu": 0.0352,
            "rouge1_f": 0.2714,
            "rouge2_f": 0.0683,
            "rougeL_f": 0.1908,
            "semantic_similarity": 0.5622,
            "answer_identification_correctness": 0.65,
            "explanation_quality": 0.756,
            "response_cleanliness": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2043,
            "recall": 0.0764,
            "f1_score": 0.104,
            "jaccard": 0.0583,
            "bleu": 0.018,
            "rouge1_f": 0.1869,
            "rouge2_f": 0.0448,
            "rougeL_f": 0.1541,
            "semantic_similarity": 0.4241,
            "answer_identification_correctness": 0.655,
            "explanation_quality": 0.09,
            "response_cleanliness": 1.0
          }
        }
      }
    },
    "ecqa_negative": {
      "ecqa_negative_explanation": {
        "u4b-mistral-7b_ft_ecqa_negative_ft": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.349,
            "recall": 0.4086,
            "f1_score": 0.343,
            "jaccard": 0.2126,
            "bleu": 0.0588,
            "rouge1_f": 0.3069,
            "rouge2_f": 0.1254,
            "rougeL_f": 0.2556,
            "semantic_similarity": 0.6522,
            "incorrect_choices_coverage": 0.9097,
            "atomic_sentence_format": 0.8009,
            "external_knowledge_usage": 1.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1641,
            "recall": 0.4663,
            "f1_score": 0.2285,
            "jaccard": 0.1311,
            "bleu": 0.0293,
            "rouge1_f": 0.2787,
            "rouge2_f": 0.0952,
            "rougeL_f": 0.2034,
            "semantic_similarity": 0.6311,
            "incorrect_choices_coverage": 0.9577,
            "atomic_sentence_format": 0.5096,
            "external_knowledge_usage": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1957,
            "recall": 0.4721,
            "f1_score": 0.2603,
            "jaccard": 0.1518,
            "bleu": 0.0336,
            "rouge1_f": 0.3251,
            "rouge2_f": 0.1057,
            "rougeL_f": 0.2342,
            "semantic_similarity": 0.7107,
            "incorrect_choices_coverage": 0.9632,
            "atomic_sentence_format": 0.6698,
            "external_knowledge_usage": 1.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.19,
            "recall": 0.4786,
            "f1_score": 0.2541,
            "jaccard": 0.1491,
            "bleu": 0.0232,
            "rouge1_f": 0.2266,
            "rouge2_f": 0.0842,
            "rougeL_f": 0.1701,
            "semantic_similarity": 0.5595,
            "incorrect_choices_coverage": 0.9475,
            "atomic_sentence_format": 0.9057,
            "external_knowledge_usage": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1815,
            "recall": 0.3444,
            "f1_score": 0.2246,
            "jaccard": 0.1284,
            "bleu": 0.0297,
            "rouge1_f": 0.3542,
            "rouge2_f": 0.1247,
            "rougeL_f": 0.2679,
            "semantic_similarity": 0.6749,
            "incorrect_choices_coverage": 0.9632,
            "atomic_sentence_format": 0.7616,
            "external_knowledge_usage": 1.0
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2361,
            "recall": 0.4403,
            "f1_score": 0.288,
            "jaccard": 0.1708,
            "bleu": 0.0406,
            "rouge1_f": 0.3386,
            "rouge2_f": 0.1066,
            "rougeL_f": 0.2446,
            "semantic_similarity": 0.728,
            "incorrect_choices_coverage": 0.6975,
            "atomic_sentence_format": 0.7049,
            "external_knowledge_usage": 1.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1473,
            "recall": 0.3875,
            "f1_score": 0.1974,
            "jaccard": 0.1128,
            "bleu": 0.0196,
            "rouge1_f": 0.1937,
            "rouge2_f": 0.0611,
            "rougeL_f": 0.1479,
            "semantic_similarity": 0.5665,
            "incorrect_choices_coverage": 0.7107,
            "atomic_sentence_format": 0.7466,
            "external_knowledge_usage": 1.0
          }
        }
      }
    },
    "RHAI_cose_choice_selection": {
      "cose_choice_selection": {
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.68,
            "precision": 0.69,
            "recall": 0.69,
            "f1_score": 0.69,
            "jaccard": 0.6867,
            "bleu": 0.69,
            "rouge1_f": 0.695,
            "rouge2_f": 0.31,
            "rougeL_f": 0.695,
            "semantic_similarity": 0.8125,
            "answer_correctness": 0.68,
            "response_conciseness": 0.992,
            "instruction_following_penalty": 1.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.13,
            "precision": 0.2009,
            "recall": 0.5925,
            "f1_score": 0.2335,
            "jaccard": 0.1951,
            "bleu": 0.1677,
            "rouge1_f": 0.3159,
            "rouge2_f": 0.1608,
            "rougeL_f": 0.3159,
            "semantic_similarity": 0.5626,
            "answer_correctness": 0.81,
            "response_conciseness": 0.523,
            "instruction_following_penalty": 0.884
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.31,
            "precision": 0.34,
            "recall": 0.3417,
            "f1_score": 0.3407,
            "jaccard": 0.3292,
            "bleu": 0.3006,
            "rouge1_f": 0.703,
            "rouge2_f": 0.3267,
            "rougeL_f": 0.703,
            "semantic_similarity": 0.756,
            "answer_correctness": 0.7,
            "response_conciseness": 0.99,
            "instruction_following_penalty": 1.0
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0179,
            "recall": 0.0742,
            "f1_score": 0.0272,
            "jaccard": 0.0156,
            "bleu": 0.0081,
            "rouge1_f": 0.1715,
            "rouge2_f": 0.0737,
            "rougeL_f": 0.1715,
            "semantic_similarity": 0.4892,
            "answer_correctness": 0.46,
            "response_conciseness": 0.665,
            "instruction_following_penalty": 0.994
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.18,
            "precision": 0.1958,
            "recall": 0.2392,
            "f1_score": 0.1986,
            "jaccard": 0.191,
            "bleu": 0.1849,
            "rouge1_f": 0.605,
            "rouge2_f": 0.2866,
            "rougeL_f": 0.605,
            "semantic_similarity": 0.6935,
            "answer_correctness": 0.69,
            "response_conciseness": 0.911,
            "instruction_following_penalty": 1.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.13,
            "precision": 0.1707,
            "recall": 0.1833,
            "f1_score": 0.1692,
            "jaccard": 0.1541,
            "bleu": 0.1565,
            "rouge1_f": 0.6508,
            "rouge2_f": 0.3,
            "rougeL_f": 0.6508,
            "semantic_similarity": 0.7021,
            "answer_correctness": 0.65,
            "response_conciseness": 0.974,
            "instruction_following_penalty": 1.0
          }
        }
      }
    },
    "ecqa_choice_selection_no_concept": {
      "ecqa_choice_selection_no_concept": {
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0344,
            "recall": 0.57,
            "f1_score": 0.0571,
            "jaccard": 0.0317,
            "bleu": 0.0258,
            "rouge1_f": 0.1191,
            "rouge2_f": 0.0468,
            "rougeL_f": 0.1191,
            "semantic_similarity": 0.5219,
            "answer_correctness": 0.72,
            "follows_format_instruction": 0.333,
            "answer_extractability": 0.7215
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.0255,
            "recall": 0.3517,
            "f1_score": 0.0385,
            "jaccard": 0.0254,
            "bleu": 0.0194,
            "rouge1_f": 0.0715,
            "rouge2_f": 0.0322,
            "rougeL_f": 0.0715,
            "semantic_similarity": 0.4729,
            "answer_correctness": 0.74,
            "follows_format_instruction": 0.225,
            "answer_extractability": 0.8022
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.38,
            "precision": 0.3943,
            "recall": 0.56,
            "f1_score": 0.4046,
            "jaccard": 0.3938,
            "bleu": 0.3774,
            "rouge1_f": 0.4286,
            "rouge2_f": 0.1302,
            "rougeL_f": 0.4281,
            "semantic_similarity": 0.6547,
            "answer_correctness": 0.65,
            "follows_format_instruction": 0.653,
            "answer_extractability": 0.857
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0344,
            "recall": 0.6,
            "f1_score": 0.0615,
            "jaccard": 0.0344,
            "bleu": 0.0123,
            "rouge1_f": 0.0647,
            "rouge2_f": 0.0258,
            "rougeL_f": 0.0647,
            "semantic_similarity": 0.4459,
            "answer_correctness": 0.61,
            "follows_format_instruction": 0.262,
            "answer_extractability": 0.8004
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.71,
            "precision": 0.714,
            "recall": 0.775,
            "f1_score": 0.717,
            "jaccard": 0.7138,
            "bleu": 0.7021,
            "rouge1_f": 0.7216,
            "rouge2_f": 0.3138,
            "rougeL_f": 0.7216,
            "semantic_similarity": 0.8369,
            "answer_correctness": 0.81,
            "follows_format_instruction": 0.886,
            "answer_extractability": 0.9483
          }
        },
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0121,
            "recall": 0.3117,
            "f1_score": 0.0232,
            "jaccard": 0.012,
            "bleu": 0.0043,
            "rouge1_f": 0.0287,
            "rouge2_f": 0.0084,
            "rougeL_f": 0.0287,
            "semantic_similarity": 0.3724,
            "answer_correctness": 0.26,
            "follows_format_instruction": 0.213,
            "answer_extractability": 0.8685
          }
        }
      }
    },
    "RHAI_cose_explanation_with_answer": {
      "cose_explanation_with_answer": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2126,
            "recall": 0.2583,
            "f1_score": 0.224,
            "jaccard": 0.1299,
            "bleu": 0.0408,
            "rouge1_f": 0.289,
            "rouge2_f": 0.0716,
            "rougeL_f": 0.2021,
            "semantic_similarity": 0.5673,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.944,
            "format_cleanliness": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.01,
            "precision": 0.2711,
            "recall": 0.1742,
            "f1_score": 0.1989,
            "jaccard": 0.1199,
            "bleu": 0.0498,
            "rouge1_f": 0.3036,
            "rouge2_f": 0.0922,
            "rougeL_f": 0.2271,
            "semantic_similarity": 0.5997,
            "explanation_correctness": 0.66,
            "brevity_compliance": 0.956,
            "format_cleanliness": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.253,
            "recall": 0.2814,
            "f1_score": 0.2521,
            "jaccard": 0.1498,
            "bleu": 0.0483,
            "rouge1_f": 0.338,
            "rouge2_f": 0.0975,
            "rougeL_f": 0.2361,
            "semantic_similarity": 0.6856,
            "explanation_correctness": 0.91,
            "brevity_compliance": 0.972,
            "format_cleanliness": 1.0
          }
        },
        "hf-qwen3-30b-thinking": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0775,
            "recall": 0.5299,
            "f1_score": 0.1328,
            "jaccard": 0.0718,
            "bleu": 0.0088,
            "rouge1_f": 0.1175,
            "rouge2_f": 0.0311,
            "rougeL_f": 0.0861,
            "semantic_similarity": 0.4513,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.0,
            "format_cleanliness": 0.985
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2172,
            "recall": 0.2642,
            "f1_score": 0.2261,
            "jaccard": 0.1305,
            "bleu": 0.0386,
            "rouge1_f": 0.3187,
            "rouge2_f": 0.0782,
            "rougeL_f": 0.2164,
            "semantic_similarity": 0.6366,
            "explanation_correctness": 0.99,
            "brevity_compliance": 0.968,
            "format_cleanliness": 1.0
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2112,
            "recall": 0.2997,
            "f1_score": 0.2378,
            "jaccard": 0.1381,
            "bleu": 0.0392,
            "rouge1_f": 0.3144,
            "rouge2_f": 0.0746,
            "rougeL_f": 0.2125,
            "semantic_similarity": 0.6308,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.898,
            "format_cleanliness": 1.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.2057,
            "recall": 0.2919,
            "f1_score": 0.2178,
            "jaccard": 0.1257,
            "bleu": 0.0343,
            "rouge1_f": 0.2667,
            "rouge2_f": 0.0724,
            "rougeL_f": 0.1944,
            "semantic_similarity": 0.5475,
            "explanation_correctness": 0.977,
            "brevity_compliance": 0.757,
            "format_cleanliness": 0.941
          }
        }
      }
    },
    "RHAI_cose_explanation_with_answer_and_questions": {
      "cose_self_rationalization": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1401,
            "recall": 0.3776,
            "f1_score": 0.1951,
            "jaccard": 0.1101,
            "bleu": 0.0229,
            "rouge1_f": 0.2202,
            "rouge2_f": 0.058,
            "rougeL_f": 0.1566,
            "semantic_similarity": 0.6151,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.433,
            "format_cleanliness": 1.0
          }
        },
        "u4b-llama3.3-70b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1142,
            "recall": 0.4326,
            "f1_score": 0.1725,
            "jaccard": 0.0961,
            "bleu": 0.0145,
            "rouge1_f": 0.1664,
            "rouge2_f": 0.0409,
            "rougeL_f": 0.118,
            "semantic_similarity": 0.5573,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.195,
            "format_cleanliness": 0.797
          }
        },
        "u4b-llama3-8b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.141,
            "recall": 0.419,
            "f1_score": 0.2025,
            "jaccard": 0.1149,
            "bleu": 0.0224,
            "rouge1_f": 0.2274,
            "rouge2_f": 0.0629,
            "rougeL_f": 0.1566,
            "semantic_similarity": 0.6239,
            "explanation_correctness": 0.97,
            "brevity_compliance": 0.44,
            "format_cleanliness": 1.0
          }
        },
        "hf-qwen3-30b-thinking": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0716,
            "recall": 0.5242,
            "f1_score": 0.1238,
            "jaccard": 0.0666,
            "bleu": 0.0077,
            "rouge1_f": 0.1133,
            "rouge2_f": 0.0279,
            "rougeL_f": 0.0825,
            "semantic_similarity": 0.3429,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.0,
            "format_cleanliness": 0.98
          }
        },
        "hf-qwen2.5-3b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.13,
            "recall": 0.4347,
            "f1_score": 0.193,
            "jaccard": 0.1085,
            "bleu": 0.0209,
            "rouge1_f": 0.2089,
            "rouge2_f": 0.0583,
            "rougeL_f": 0.1487,
            "semantic_similarity": 0.6269,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.372,
            "format_cleanliness": 1.0
          }
        },
        "u4b-mistral-7b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1445,
            "recall": 0.4268,
            "f1_score": 0.208,
            "jaccard": 0.1179,
            "bleu": 0.0225,
            "rouge1_f": 0.2354,
            "rouge2_f": 0.0616,
            "rougeL_f": 0.1626,
            "semantic_similarity": 0.6388,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.509,
            "format_cleanliness": 1.0
          }
        },
        "hf-mistral-nemo-12b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.1402,
            "recall": 0.3746,
            "f1_score": 0.1954,
            "jaccard": 0.1101,
            "bleu": 0.0222,
            "rouge1_f": 0.238,
            "rouge2_f": 0.0536,
            "rougeL_f": 0.1589,
            "semantic_similarity": 0.6158,
            "explanation_correctness": 1.0,
            "brevity_compliance": 0.581,
            "format_cleanliness": 1.0
          }
        }
      }
    },
    "gmeg_paper": {
      "gmeg_explaination": {
        "u4b-llama3.2-1b": {
          "num_experiments": 1,
          "metrics": {
            "exact_match": 0.0,
            "precision": 0.0881,
            "recall": 0.2739,
            "f1_score": 0.1258,
            "jaccard": 0.0685,
            "bleu": 0.0102,
            "rouge1_f": 0.1252,
            "rouge2_f": 0.0218,
            "rougeL_f": 0.0943,
            "semantic_similarity": 0.4594
          }
        }
      }
    }
  },
  "best_performers": {
    "by_recall": {
      "experiment_name": "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection__100__0p100",
      "setup": "ecare_choice_selection",
      "prompt": "ecare_choice_selection",
      "model": "u4b-llama3.3-70b",
      "score": 0.92
    },
    "by_rouge2_f": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.3503
    },
    "by_correct_answer_focus": {
      "experiment_name": "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation__100__0p100",
      "setup": "ecqa_positive",
      "prompt": "ecqa_positive_explanation",
      "model": "u4b-llama3.2-1b",
      "score": 0.922
    },
    "by_precision": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8861
    },
    "by_rougeL_f": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8868
    },
    "by_original_text_mention": {
      "experiment_name": "gmeg_ours__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot_original__100__0p100",
      "setup": "gmeg_ours",
      "prompt": "gmeg_few_shot_original",
      "model": "hf-mistral-nemo-12b",
      "score": 0.994
    },
    "by_incorrect_choices_coverage": {
      "experiment_name": "ecqa_negative__u4b-mistral-7b__zero-shot__ecqa_negative_explanation__100__0p100",
      "setup": "ecqa_negative",
      "prompt": "ecqa_negative_explanation",
      "model": "u4b-mistral-7b",
      "score": 0.9632
    },
    "by_bullet_point_ratio": {
      "experiment_name": "gmeg__u4b-mistral-7b_ft_gmeg_explanation_ft__zero-shot__gmeg_explaination__100__0p100",
      "setup": "gmeg",
      "prompt": "gmeg_explaination",
      "model": "u4b-mistral-7b_ft_gmeg_explanation_ft",
      "score": 0.6252
    },
    "by_simple_atomic_sentences": {
      "experiment_name": "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
      "setup": "ecqa_positive",
      "prompt": "ecqa_positive_explanation_formatted",
      "model": "hf-mistral-nemo-12b",
      "score": 0.9613
    },
    "by_explanation_quality": {
      "experiment_name": "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
      "setup": "RHAI_cose_explanation",
      "prompt": "cose_explanation",
      "model": "u4b-llama3.3-70b",
      "score": 0.828
    },
    "by_conceptual_abstraction_level": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
      "setup": "ecare_explanation_generation",
      "prompt": "ecare_explanation_generation",
      "model": "u4b-llama3.2-1b",
      "score": 0.951
    },
    "by_correction_terminology_recall": {
      "experiment_name": "gmeg__hf-qwen3-30b-thinking__zero-shot__gmeg_explaination__100__0p100",
      "setup": "gmeg",
      "prompt": "gmeg_explaination",
      "model": "hf-qwen3-30b-thinking",
      "score": 0.8667
    },
    "by_brevity_compliance": {
      "experiment_name": "RHAI_cose_explanation_with_answer__u4b-mistral-7b__zero-shot__cose_explanation_with_answer__100__0p100",
      "setup": "RHAI_cose_explanation_with_answer",
      "prompt": "cose_explanation_with_answer",
      "model": "u4b-mistral-7b",
      "score": 0.972
    },
    "by_single_paragraph_format": {
      "experiment_name": "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
      "setup": "ecqa_freeflow",
      "prompt": "ecqa_stakeholder_disability_advocate",
      "model": "u4b-llama3-8b",
      "score": 1.0
    },
    "by_response_conciseness": {
      "experiment_name": "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100",
      "setup": "RHAI_cose_choice_selection",
      "prompt": "cose_choice_selection",
      "model": "hf-qwen2.5-3b",
      "score": 0.992
    },
    "by_semantic_similarity": {
      "experiment_name": "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free__100__0p100",
      "setup": "pubmedqa_reasoning_free",
      "prompt": "pubmedqa_reasoning_free",
      "model": "u4b-llama3-8b",
      "score": 0.9331
    },
    "by_response_brevity": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
      "setup": "ecare_explanation_generation",
      "prompt": "ecare_explanation_generation",
      "model": "u4b-llama3.2-1b",
      "score": 0.681
    },
    "by_answer_identification_correctness": {
      "experiment_name": "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
      "setup": "RHAI_cose_explanation",
      "prompt": "cose_explanation",
      "model": "u4b-llama3.3-70b",
      "score": 0.705
    },
    "by_atomic_sentence_format": {
      "experiment_name": "ecqa_negative__u4b-llama3.3-70b__zero-shot__ecqa_negative_explanation__100__0p100",
      "setup": "ecqa_negative",
      "prompt": "ecqa_negative_explanation",
      "model": "u4b-llama3.3-70b",
      "score": 0.9057
    },
    "by_answer_extractability": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.995
    },
    "by_external_knowledge_usage": {
      "experiment_name": "ecqa_negative__u4b-mistral-7b_ft_ecqa_negative_ft__zero-shot__ecqa_negative_explanation__100__0p100",
      "setup": "ecqa_negative",
      "prompt": "ecqa_negative_explanation",
      "model": "u4b-mistral-7b_ft_ecqa_negative_ft",
      "score": 1.0
    },
    "by_confidence_calibration": {
      "experiment_name": "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
      "setup": "pubmedqa_reasoning_required",
      "prompt": "pubmedqa_reasoning_required_min",
      "model": "u4b-llama3-8b",
      "score": 0.92
    },
    "by_single_sentence_format": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
      "setup": "ecare_explanation_generation",
      "prompt": "ecare_explanation_generation",
      "model": "u4b-llama3.2-1b",
      "score": 0.602
    },
    "by_exact_match": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.88
    },
    "by_rouge1_f": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8868
    },
    "by_structural_format_match": {
      "experiment_name": "gmeg_ours__hf-mistral-nemo-12b__zero-shot__gmeg_basic__100__0p100",
      "setup": "gmeg_ours",
      "prompt": "gmeg_basic",
      "model": "hf-mistral-nemo-12b",
      "score": 0.91
    },
    "by_bleu": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8653
    },
    "by_follows_format_instruction": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.971
    },
    "by_instruction_following_penalty": {
      "experiment_name": "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100",
      "setup": "RHAI_cose_choice_selection",
      "prompt": "cose_choice_selection",
      "model": "hf-qwen2.5-3b",
      "score": 1.0
    },
    "by_format_cleanliness": {
      "experiment_name": "RHAI_cose_explanation_with_answer__u4b-llama3.2-1b__zero-shot__cose_explanation_with_answer__100__0p100",
      "setup": "RHAI_cose_explanation_with_answer",
      "prompt": "cose_explanation_with_answer",
      "model": "u4b-llama3.2-1b",
      "score": 1.0
    },
    "by_jaccard": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8844
    },
    "by_strict_format_compliance": {
      "experiment_name": "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
      "setup": "pubmedqa_reasoning_free",
      "prompt": "pubmedqa_expert_primary_care",
      "model": "hf-qwen2.5-3b",
      "score": 1.0
    },
    "by_maybe_overuse_penalty": {
      "experiment_name": "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
      "setup": "pubmedqa_reasoning_required",
      "prompt": "pubmedqa_reasoning_required_min",
      "model": "u4b-llama3.2-1b",
      "score": 1.0
    },
    "by_concise_justification": {
      "experiment_name": "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation__100__0p100",
      "setup": "ecqa_positive",
      "prompt": "ecqa_positive_explanation",
      "model": "hf-mistral-nemo-12b",
      "score": 0.9619
    },
    "by_f1_score": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "setup": "ecqa_choice_selection_3choices",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "score": 0.8871
    },
    "by_response_cleanliness": {
      "experiment_name": "RHAI_cose_explanation__u4b-mistral-7b_ft_cose_explanation_ft__zero-shot__cose_explanation__100__0p100",
      "setup": "RHAI_cose_explanation",
      "prompt": "cose_explanation",
      "model": "u4b-mistral-7b_ft_cose_explanation_ft",
      "score": 1.0
    },
    "by_all_choices_addressed": {
      "experiment_name": "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
      "setup": "ecqa_freeflow",
      "prompt": "ecqa_freeflow_explanation",
      "model": "u4b-llama3.3-70b",
      "score": 0.98
    },
    "by_explanation_depth": {
      "experiment_name": "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
      "setup": "ecqa_freeflow",
      "prompt": "ecqa_freeflow_explanation",
      "model": "u4b-llama3.3-70b",
      "score": 0.5681
    },
    "by_no_explanation_penalty": {
      "experiment_name": "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
      "setup": "pubmedqa_reasoning_free",
      "prompt": "pubmedqa_expert_primary_care",
      "model": "hf-qwen2.5-3b",
      "score": 1.0
    },
    "by_answer_correctness": {
      "experiment_name": "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free__100__0p100",
      "setup": "pubmedqa_reasoning_free",
      "prompt": "pubmedqa_reasoning_free",
      "model": "u4b-llama3.3-70b",
      "score": 0.9
    },
    "by_explanation_correctness": {
      "experiment_name": "RHAI_cose_explanation_with_answer__u4b-llama3.2-1b__zero-shot__cose_explanation_with_answer__100__0p100",
      "setup": "RHAI_cose_explanation_with_answer",
      "prompt": "cose_explanation_with_answer",
      "model": "u4b-llama3.2-1b",
      "score": 1.0
    }
  },
  "summary_statistics": {
    "total_setups": 17,
    "total_prompts": 47,
    "total_models": 13,
    "setups": [
      "RHAI_cose_choice_selection",
      "RHAI_cose_explanation",
      "RHAI_cose_explanation_with_answer",
      "RHAI_cose_explanation_with_answer_and_questions",
      "ecare_choice_selection",
      "ecare_explanation_generation",
      "ecqa_choice",
      "ecqa_choice_selection_3choices",
      "ecqa_choice_selection_no_concept",
      "ecqa_freeflow",
      "ecqa_negative",
      "ecqa_positive",
      "gmeg",
      "gmeg_ours",
      "gmeg_paper",
      "pubmedqa_reasoning_free",
      "pubmedqa_reasoning_required"
    ],
    "models": [
      "hf-mistral-nemo-12b",
      "hf-qwen2.5-3b",
      "hf-qwen3-30b-thinking",
      "u4b-llama3-8b",
      "u4b-llama3.2-1b",
      "u4b-llama3.3-70b",
      "u4b-mistral-7b",
      "u4b-mistral-7b_ft_cose_explanation_ft",
      "u4b-mistral-7b_ft_ecqa_freeflow_ft",
      "u4b-mistral-7b_ft_ecqa_negative_ft",
      "u4b-mistral-7b_ft_ecqa_positive_ft",
      "u4b-mistral-7b_ft_gmeg_explanation_ft",
      "u4b-mistral-7b_ft_pubmedqa_reasoning_ft"
    ],
    "prompts_per_setup": {
      "pubmedqa_reasoning_required": [
        "pubmedqa_expert_biostatistician_reasoning_required",
        "pubmedqa_expert_epidemiologist_reasoning_required",
        "pubmedqa_expert_primary_care_reasoning_required",
        "pubmedqa_reasoning_required",
        "pubmedqa_reasoning_required_min"
      ],
      "ecqa_choice": [
        "ecqa_choice_selection",
        "ecqa_demographic_middle_aged_asian",
        "ecqa_demographic_senior_european",
        "ecqa_demographic_young_american"
      ],
      "ecare_explanation_generation": [
        "ecare_explanation_generation",
        "ecare_explanation_generation_demographic_young_american",
        "ecare_explanation_generation_expert_medical",
        "ecare_explanation_generation_expert_psychologist",
        "ecare_explanation_generation_stakeholder_parent"
      ],
      "ecqa_freeflow": [
        "ecqa_expert_anthropologist",
        "ecqa_expert_behavioral_economist",
        "ecqa_expert_psychologist",
        "ecqa_freeflow_explanation",
        "ecqa_stakeholder_disability_advocate",
        "ecqa_stakeholder_parent",
        "ecqa_stakeholder_perspective",
        "ecqa_stakeholder_small_business"
      ],
      "ecqa_choice_selection_3choices": [
        "ecqa_choice_selection_3choices"
      ],
      "gmeg": [
        "gmeg_explaination",
        "gmeg_few_shot"
      ],
      "ecqa_positive": [
        "ecqa_positive_explanation",
        "ecqa_positive_explanation_formatted"
      ],
      "gmeg_ours": [
        "gmeg_basic",
        "gmeg_few_shot_original"
      ],
      "pubmedqa_reasoning_free": [
        "pubmedqa_expert_biostatistician",
        "pubmedqa_expert_epidemiologist",
        "pubmedqa_expert_impersonation",
        "pubmedqa_expert_primary_care",
        "pubmedqa_reasoning_free",
        "pubmedqa_reasoning_free_min"
      ],
      "ecare_choice_selection": [
        "ecare_choice_selection",
        "ecare_choice_selection_demographic_young_american",
        "ecare_choice_selection_expert_medical",
        "ecare_choice_selection_expert_psychologist",
        "ecare_choice_selection_stakeholder_parent"
      ],
      "RHAI_cose_explanation": [
        "cose_explanation"
      ],
      "ecqa_negative": [
        "ecqa_negative_explanation"
      ],
      "RHAI_cose_choice_selection": [
        "cose_choice_selection"
      ],
      "ecqa_choice_selection_no_concept": [
        "ecqa_choice_selection_no_concept"
      ],
      "RHAI_cose_explanation_with_answer": [
        "cose_explanation_with_answer"
      ],
      "RHAI_cose_explanation_with_answer_and_questions": [
        "cose_self_rationalization"
      ],
      "gmeg_paper": [
        "gmeg_explaination"
      ]
    }
  }
}