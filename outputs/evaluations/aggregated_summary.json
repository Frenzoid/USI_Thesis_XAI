{
  "generation_timestamp": "2025-10-10T16:52:02.401045",
  "total_experiments": 282,
  "unique_prompts": 46,
  "unique_models": 13,
  "by_prompt": {
    "cose_choice_selection": {
      "num_experiments": 6,
      "experiments": [
        "RHAI_cose_choice_selection__hf-mistral-nemo-12b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_choice_selection__u4b-llama3-8b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_choice_selection__u4b-llama3.2-1b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_choice_selection__u4b-llama3.3-70b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_choice_selection__u4b-mistral-7b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.2383333333333333,
          "std": 0.21751755995525718,
          "min": 0.0,
          "max": 0.68,
          "median": 0.155,
          "values": [
            0.31,
            0.68,
            0.13,
            0.0,
            0.13,
            0.18
          ]
        },
        "precision": {
          "mean": 0.26922040042647505,
          "std": 0.2101564009595022,
          "min": 0.017889339826839826,
          "max": 0.69,
          "median": 0.19838319803267168,
          "values": [
            0.34,
            0.69,
            0.17066666666666663,
            0.017889339826839826,
            0.200942016740964,
            0.19582437932437935
          ]
        },
        "recall": {
          "mean": 0.3534722222222222,
          "std": 0.2200565040373399,
          "min": 0.07416666666666666,
          "max": 0.69,
          "median": 0.29041666666666666,
          "values": [
            0.3416666666666666,
            0.69,
            0.18333333333333332,
            0.07416666666666666,
            0.5925,
            0.23916666666666664
          ]
        },
        "f1_score": {
          "mean": 0.2765340002132734,
          "std": 0.20682630621630066,
          "min": 0.027241593047475404,
          "max": 0.69,
          "median": 0.21606453744941573,
          "values": [
            0.3406666666666666,
            0.69,
            0.16916666666666663,
            0.027241593047475404,
            0.233500611191643,
            0.19862846370718848
          ]
        },
        "jaccard": {
          "mean": 0.2619239482183742,
          "std": 0.21085129790969148,
          "min": 0.015567133520074695,
          "max": 0.6866666666666665,
          "median": 0.1930239921807998,
          "values": [
            0.3291666666666667,
            0.6866666666666665,
            0.1540952380952381,
            0.015567133520074695,
            0.19509040973011188,
            0.1909575746314877
          ]
        },
        "semantic_similarity": {
          "mean": 0.6692889881196121,
          "std": 0.1106939632123332,
          "min": 0.48915969584137203,
          "max": 0.8124932911247015,
          "median": 0.6977574129402637,
          "values": [
            0.7559962159395218,
            0.8124932911247015,
            0.7020514233410359,
            0.48915969584137203,
            0.56256989993155,
            0.6934634025394917
          ]
        },
        "answer_correctness": {
          "mean": 0.6649999999999999,
          "std": 0.10436314802968846,
          "min": 0.46,
          "max": 0.81,
          "median": 0.685,
          "values": [
            0.7,
            0.68,
            0.65,
            0.46,
            0.81,
            0.69
          ]
        },
        "response_conciseness": {
          "mean": 0.8424999999999999,
          "std": 0.18242692600965826,
          "min": 0.523,
          "max": 0.992,
          "median": 0.9425,
          "values": [
            0.99,
            0.992,
            0.9740000000000001,
            0.665,
            0.523,
            0.9109999999999999
          ]
        },
        "instruction_following_penalty": {
          "mean": 0.9666666666666667,
          "std": 0.07188571176218234,
          "min": 0.8059999999999999,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            0.9940000000000001,
            0.8059999999999999,
            1.0
          ]
        }
      }
    },
    "cose_explanation": {
      "num_experiments": 7,
      "experiments": [
        "RHAI_cose_explanation__hf-mistral-nemo-12b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__hf-qwen2.5-3b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__u4b-llama3-8b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__u4b-llama3.2-1b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__u4b-mistral-7b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation__u4b-mistral-7b_ft_cose_explanation_ft__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.19878140580855908,
          "std": 0.0372825743926139,
          "min": 0.15689040508315139,
          "max": 0.25560764835925204,
          "median": 0.19782797322544143,
          "values": [
            0.2043470206264324,
            0.24690214964218196,
            0.15689040508315139,
            0.1575692080430046,
            0.17232543568044964,
            0.19782797322544143,
            0.25560764835925204
          ]
        },
        "recall": {
          "mean": 0.18037372952831748,
          "std": 0.06662589993198408,
          "min": 0.07638833259103094,
          "max": 0.30343662607300603,
          "median": 0.16833561645792386,
          "values": [
            0.07638833259103094,
            0.20070787972517093,
            0.1314184130551263,
            0.160118435573066,
            0.30343662607300603,
            0.22221080322289843,
            0.16833561645792386
          ]
        },
        "f1_score": {
          "mean": 0.16397652678700145,
          "std": 0.039030422000385145,
          "min": 0.104005606130473,
          "max": 0.20911999062887965,
          "median": 0.17768690410135687,
          "values": [
            0.104005606130473,
            0.20911999062887965,
            0.12107995168359142,
            0.13946256212663946,
            0.20334576845697389,
            0.19313490438109585,
            0.17768690410135687
          ]
        },
        "jaccard": {
          "mean": 0.093783865188291,
          "std": 0.02304438501580759,
          "min": 0.05832851534349138,
          "max": 0.12121886485500809,
          "median": 0.10270769847885078,
          "values": [
            0.05832851534349138,
            0.12121886485500809,
            0.0690678591308944,
            0.07837163580719397,
            0.11565473176818783,
            0.11113775093441064,
            0.10270769847885078
          ]
        },
        "semantic_similarity": {
          "mean": 0.5062495180379066,
          "std": 0.0564091753484744,
          "min": 0.4240999024733901,
          "max": 0.5622329947352409,
          "median": 0.5429833361133933,
          "values": [
            0.4240999024733901,
            0.5534891698509454,
            0.45225615467876196,
            0.4500850836187601,
            0.5585999847948551,
            0.5622329947352409,
            0.5429833361133933
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.652142857142857,
          "std": 0.03336563061179045,
          "min": 0.615,
          "max": 0.705,
          "median": 0.65,
          "values": [
            0.655,
            0.62,
            0.695,
            0.615,
            0.705,
            0.65,
            0.625
          ]
        },
        "explanation_quality": {
          "mean": 0.5337142857142857,
          "std": 0.2271045789849213,
          "min": 0.09,
          "max": 0.828,
          "median": 0.506,
          "values": [
            0.09,
            0.64,
            0.41,
            0.506,
            0.828,
            0.7559999999999999,
            0.506
          ]
        },
        "response_cleanliness": {
          "mean": 0.9918571428571428,
          "std": 0.01994584504837729,
          "min": 0.9430000000000001,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            1.0,
            0.9430000000000001,
            1.0,
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer": {
      "num_experiments": 7,
      "experiments": [
        "RHAI_cose_explanation_with_answer__hf-mistral-nemo-12b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__hf-qwen2.5-3b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__hf-qwen3-30b-thinking__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3-8b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3.2-1b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3.3-70b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-mistral-7b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0014285714285714286,
          "std": 0.0034992710611188257,
          "min": 0.0,
          "max": 0.01,
          "median": 0.0,
          "values": [
            0.01,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.20689374478116726,
          "std": 0.0575155453450688,
          "min": 0.07746335729309779,
          "max": 0.2710679820056553,
          "median": 0.21263816083201761,
          "values": [
            0.2710679820056553,
            0.21715598631224858,
            0.07746335729309779,
            0.21116309727705734,
            0.21263816083201761,
            0.205745266140876,
            0.25302236360721825
          ]
        },
        "recall": {
          "mean": 0.2999466771159867,
          "std": 0.10149051644280882,
          "min": 0.1742063311299156,
          "max": 0.529891475853589,
          "median": 0.28141934071330515,
          "values": [
            0.1742063311299156,
            0.2642219724126587,
            0.529891475853589,
            0.29967265965370204,
            0.258322573271235,
            0.29189238677750157,
            0.28141934071330515
          ]
        },
        "f1_score": {
          "mean": 0.21279151950551914,
          "std": 0.03605550660988611,
          "min": 0.13276991742268632,
          "max": 0.25214153009130524,
          "median": 0.22402495042812845,
          "values": [
            0.19887381666910323,
            0.2260900319435403,
            0.13276991742268632,
            0.23781148459618684,
            0.22402495042812845,
            0.21782890538768335,
            0.25214153009130524
          ]
        },
        "jaccard": {
          "mean": 0.12367050119797325,
          "std": 0.02294642047082219,
          "min": 0.07179172630138222,
          "max": 0.14976971623070223,
          "median": 0.12987108632341812,
          "values": [
            0.1198940158325367,
            0.13051404237270547,
            0.07179172630138222,
            0.1381444301898855,
            0.12987108632341812,
            0.12570849113518245,
            0.14976971623070223
          ]
        },
        "semantic_similarity": {
          "mean": 0.5884255652342524,
          "std": 0.07030601010012774,
          "min": 0.45134885251522067,
          "max": 0.6856044900417327,
          "median": 0.5996882364153862,
          "values": [
            0.5996882364153862,
            0.6366396707296371,
            0.45134885251522067,
            0.6308293950557708,
            0.5673414608836174,
            0.5475268509984016,
            0.6856044900417327
          ]
        },
        "explanation_correctness": {
          "mean": 0.9338571428571429,
          "std": 0.11569964634027756,
          "min": 0.66,
          "max": 1.0,
          "median": 0.99,
          "values": [
            0.66,
            0.99,
            1.0,
            1.0,
            1.0,
            0.977,
            0.91
          ]
        },
        "brevity_compliance": {
          "mean": 0.7849999999999999,
          "std": 0.3279281280141384,
          "min": 0.0,
          "max": 0.972,
          "median": 0.9440000000000001,
          "values": [
            0.9560000000000001,
            0.968,
            0.0,
            0.8980000000000001,
            0.9440000000000001,
            0.757,
            0.972
          ]
        },
        "format_cleanliness": {
          "mean": 0.9877142857142857,
          "std": 0.02452237628729136,
          "min": 0.929,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            0.985,
            1.0,
            1.0,
            0.929,
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization": {
      "num_experiments": 7,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__hf-mistral-nemo-12b__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen2.5-3b__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen3-30b-thinking__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3-8b__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.2-1b__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.3-70b__zero-shot__cose_self_rationalization__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-mistral-7b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.1259350408910257,
          "std": 0.02415123323800944,
          "min": 0.07156681635132248,
          "max": 0.1445024106698146,
          "median": 0.14005320774344646,
          "values": [
            0.14020504346682058,
            0.1300239358349212,
            0.07156681635132248,
            0.14095966309739427,
            0.14005320774344646,
            0.11423420907346031,
            0.1445024106698146
          ]
        },
        "recall": {
          "mean": 0.42705075073034715,
          "std": 0.04594960876769023,
          "min": 0.3745891674788984,
          "max": 0.524159463753342,
          "median": 0.4267943266174317,
          "values": [
            0.3745891674788984,
            0.43467512639630457,
            0.524159463753342,
            0.4189656730716152,
            0.3776197880578145,
            0.4325517097370235,
            0.4267943266174317
          ]
        },
        "f1_score": {
          "mean": 0.18432991657838402,
          "std": 0.02674472640493813,
          "min": 0.12378835508799688,
          "max": 0.2079784619571892,
          "median": 0.19510097900672346,
          "values": [
            0.1953615087241507,
            0.19304835868081727,
            0.12378835508799688,
            0.20248926343199922,
            0.19510097900672346,
            0.17254248915981138,
            0.2079784619571892
          ]
        },
        "jaccard": {
          "mean": 0.10345975482768979,
          "std": 0.016330736761498748,
          "min": 0.0665978537758392,
          "max": 0.11794179670312226,
          "median": 0.11013022879812855,
          "values": [
            0.11013022879812855,
            0.10845905009479939,
            0.0665978537758392,
            0.11485640211464439,
            0.11014193094810602,
            0.09609102135918876,
            0.11794179670312226
          ]
        },
        "semantic_similarity": {
          "mean": 0.5743764095646995,
          "std": 0.09755177806301561,
          "min": 0.34287726983428,
          "max": 0.6387710550427437,
          "median": 0.6157916417717934,
          "values": [
            0.6157916417717934,
            0.6268906626105308,
            0.34287726983428,
            0.6239275026321411,
            0.6150726193189621,
            0.557304115742445,
            0.6387710550427437
          ]
        },
        "explanation_correctness": {
          "mean": 0.9957142857142857,
          "std": 0.010497813183356486,
          "min": 0.97,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            0.97,
            1.0,
            1.0,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.3614285714285714,
          "std": 0.18505818169496016,
          "min": 0.0,
          "max": 0.5809999999999998,
          "median": 0.433,
          "values": [
            0.5809999999999998,
            0.37200000000000005,
            0.0,
            0.44000000000000006,
            0.433,
            0.195,
            0.509
          ]
        },
        "format_cleanliness": {
          "mean": 0.9680000000000001,
          "std": 0.07055696786488007,
          "min": 0.7959999999999999,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            0.98,
            1.0,
            1.0,
            0.7959999999999999,
            1.0
          ]
        }
      }
    },
    "ecare_choice_selection": {
      "num_experiments": 6,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.024999999999999998,
          "std": 0.04072263907623539,
          "min": 0.0,
          "max": 0.11,
          "median": 0.0,
          "values": [
            0.0,
            0.04,
            0.0,
            0.0,
            0.0,
            0.11
          ]
        },
        "precision": {
          "mean": 0.04088563265798876,
          "std": 0.04451861896066464,
          "min": 0.004871921526766584,
          "max": 0.13377257475840934,
          "median": 0.020942512458169506,
          "values": [
            0.020452365268034355,
            0.05504448497041228,
            0.004871921526766584,
            0.009739789776005309,
            0.021432659648304653,
            0.13377257475840934
          ]
        },
        "recall": {
          "mean": 0.68,
          "std": 0.24765567494675617,
          "min": 0.21,
          "max": 0.92,
          "median": 0.7849999999999999,
          "values": [
            0.82,
            0.87,
            0.21,
            0.51,
            0.92,
            0.75
          ]
        },
        "f1_score": {
          "mean": 0.05507073346068503,
          "std": 0.04769687181042847,
          "min": 0.009484041067423092,
          "max": 0.1530097313245453,
          "median": 0.03968278788764105,
          "values": [
            0.03984708344301362,
            0.06951497669012922,
            0.009484041067423092,
            0.019050075906730475,
            0.03951849233226848,
            0.1530097313245453
          ]
        },
        "jaccard": {
          "mean": 0.04088563265798876,
          "std": 0.04451861896066464,
          "min": 0.004871921526766584,
          "max": 0.13377257475840934,
          "median": 0.020942512458169506,
          "values": [
            0.020452365268034355,
            0.05504448497041228,
            0.004871921526766584,
            0.009739789776005309,
            0.021432659648304653,
            0.13377257475840934
          ]
        },
        "semantic_similarity": {
          "mean": 0.1589434656050677,
          "std": 0.05755981575008666,
          "min": 0.09544986858265475,
          "max": 0.25062844025669617,
          "median": 0.15272338140755892,
          "values": [
            0.10024355170316994,
            0.20189217027276754,
            0.12052421312779188,
            0.09544986858265475,
            0.18492254968732597,
            0.25062844025669617
          ]
        },
        "answer_correctness": {
          "mean": 0.52,
          "std": 0.07979139469057216,
          "min": 0.36,
          "max": 0.59,
          "median": 0.5449999999999999,
          "values": [
            0.59,
            0.5,
            0.51,
            0.36,
            0.58,
            0.58
          ]
        },
        "follows_format_instruction": {
          "mean": 0.2316666666666667,
          "std": 0.06594105111553979,
          "min": 0.16999999999999996,
          "max": 0.36500000000000016,
          "median": 0.20949999999999996,
          "values": [
            0.19799999999999998,
            0.256,
            0.17999999999999994,
            0.16999999999999996,
            0.22099999999999997,
            0.36500000000000016
          ]
        },
        "answer_extractability": {
          "mean": 0.8357999999999999,
          "std": 0.06965926595459736,
          "min": 0.7364,
          "max": 0.9619999999999999,
          "median": 0.8202,
          "values": [
            0.8039999999999998,
            0.8044,
            0.7364,
            0.836,
            0.8719999999999999,
            0.9619999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american": {
      "num_experiments": 6,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.022360679774997894,
          "min": 0.0,
          "max": 0.06,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.06
          ]
        },
        "precision": {
          "mean": 0.017455670321567203,
          "std": 0.029308837369029048,
          "min": 0.0006477361291654041,
          "max": 0.08278881113019149,
          "median": 0.005041121576064544,
          "values": [
            0.005985092345049565,
            0.004097150807079523,
            0.0006477361291654041,
            0.0031231891411951083,
            0.008092042376722132,
            0.08278881113019149
          ]
        },
        "recall": {
          "mean": 0.44,
          "std": 0.26882460204874603,
          "min": 0.06,
          "max": 0.81,
          "median": 0.39,
          "values": [
            0.41,
            0.37,
            0.06,
            0.23,
            0.81,
            0.76
          ]
        },
        "f1_score": {
          "mean": 0.024073706142861185,
          "std": 0.03474714602022735,
          "min": 0.0012807210728379142,
          "max": 0.10109790316320959,
          "median": 0.009944400834446126,
          "values": [
            0.011789023708306517,
            0.008099777960585737,
            0.0012807210728379142,
            0.006159422614456698,
            0.01601538833777067,
            0.10109790316320959
          ]
        },
        "jaccard": {
          "mean": 0.017455670321567203,
          "std": 0.029308837369029048,
          "min": 0.0006477361291654041,
          "max": 0.08278881113019149,
          "median": 0.005041121576064544,
          "values": [
            0.005985092345049565,
            0.004097150807079523,
            0.0006477361291654041,
            0.0031231891411951083,
            0.008092042376722132,
            0.08278881113019149
          ]
        },
        "semantic_similarity": {
          "mean": 0.09564111276723757,
          "std": 0.03165728227066127,
          "min": 0.06372932297177612,
          "max": 0.15466225712327286,
          "median": 0.08279187277890743,
          "values": [
            0.08034662605263293,
            0.08523711950518191,
            0.06372932297177612,
            0.07076871836092323,
            0.11910263258963823,
            0.15466225712327286
          ]
        },
        "answer_correctness": {
          "mean": 0.425,
          "std": 0.10547511554864494,
          "min": 0.27,
          "max": 0.53,
          "median": 0.475,
          "values": [
            0.51,
            0.45,
            0.27,
            0.29,
            0.53,
            0.5
          ]
        },
        "follows_format_instruction": {
          "mean": 0.18649999999999997,
          "std": 0.05174214916294066,
          "min": 0.11299999999999998,
          "max": 0.274,
          "median": 0.19799999999999995,
          "values": [
            0.19599999999999995,
            0.19999999999999996,
            0.13599999999999998,
            0.11299999999999998,
            0.19999999999999996,
            0.274
          ]
        },
        "answer_extractability": {
          "mean": 0.7308666666666666,
          "std": 0.16923914703427484,
          "min": 0.48800000000000004,
          "max": 0.9460000000000001,
          "median": 0.7855999999999999,
          "values": [
            0.7439999999999998,
            0.8271999999999999,
            0.5259999999999999,
            0.48800000000000004,
            0.8539999999999999,
            0.9460000000000001
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical": {
      "num_experiments": 6,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0033333333333333335,
          "std": 0.007453559924999299,
          "min": 0.0,
          "max": 0.02,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.02,
            0.0
          ]
        },
        "precision": {
          "mean": 0.011853137155201381,
          "std": 0.00917364716707813,
          "min": 0.0016166789530344982,
          "max": 0.02470466625104554,
          "median": 0.008437157347556852,
          "values": [
            0.008865008079372771,
            0.008009306615740935,
            0.0016166789530344982,
            0.0039019365517186364,
            0.02470466625104554,
            0.02402122648029591
          ]
        },
        "recall": {
          "mean": 0.4716666666666667,
          "std": 0.22086320552675937,
          "min": 0.14,
          "max": 0.72,
          "median": 0.53,
          "values": [
            0.57,
            0.69,
            0.14,
            0.22,
            0.49,
            0.72
          ]
        },
        "f1_score": {
          "mean": 0.019317004928144376,
          "std": 0.01323975016732124,
          "min": 0.0031952622714167545,
          "max": 0.042532834690145994,
          "median": 0.01662699329983796,
          "values": [
            0.017433196194303512,
            0.015820790405372404,
            0.0031952622714167545,
            0.007605957379241352,
            0.029313988628386235,
            0.042532834690145994
          ]
        },
        "jaccard": {
          "mean": 0.011853137155201381,
          "std": 0.00917364716707813,
          "min": 0.0016166789530344982,
          "max": 0.02470466625104554,
          "median": 0.008437157347556852,
          "values": [
            0.008865008079372771,
            0.008009306615740935,
            0.0016166789530344982,
            0.0039019365517186364,
            0.02470466625104554,
            0.02402122648029591
          ]
        },
        "semantic_similarity": {
          "mean": 0.07748576967394911,
          "std": 0.0203204720269931,
          "min": 0.04626693273894489,
          "max": 0.10996505566872657,
          "median": 0.07966012860182672,
          "values": [
            0.08063821282004938,
            0.0891882423334755,
            0.04626693273894489,
            0.06017413009889424,
            0.10996505566872657,
            0.07868204438360409
          ]
        },
        "answer_correctness": {
          "mean": 0.4516666666666666,
          "std": 0.04179978734661484,
          "min": 0.36,
          "max": 0.48,
          "median": 0.46499999999999997,
          "values": [
            0.48,
            0.46,
            0.46,
            0.36,
            0.47,
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19333333333333327,
          "std": 0.016559656464498952,
          "min": 0.16599999999999998,
          "max": 0.21599999999999994,
          "median": 0.19599999999999995,
          "values": [
            0.19199999999999995,
            0.19999999999999996,
            0.17999999999999997,
            0.16599999999999998,
            0.21599999999999994,
            0.206
          ]
        },
        "answer_extractability": {
          "mean": 0.8014666666666667,
          "std": 0.06852412389484126,
          "min": 0.7339999999999999,
          "max": 0.9359999999999999,
          "median": 0.7829999999999999,
          "values": [
            0.7339999999999999,
            0.8079999999999999,
            0.7467999999999999,
            0.758,
            0.8259999999999998,
            0.9359999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist": {
      "num_experiments": 6,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0033333333333333335,
          "std": 0.004714045207910317,
          "min": 0.0,
          "max": 0.01,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.01,
            0.01
          ]
        },
        "precision": {
          "mean": 0.008517838589027944,
          "std": 0.007548715585224828,
          "min": 0.0004219207887564308,
          "max": 0.02139867556280669,
          "median": 0.0048492926199425425,
          "values": [
            0.004004692475962245,
            0.0035154361016346003,
            0.0004219207887564308,
            0.00569389276392284,
            0.016072413841084866,
            0.02139867556280669
          ]
        },
        "recall": {
          "mean": 0.40666666666666673,
          "std": 0.22283526551144356,
          "min": 0.05,
          "max": 0.72,
          "median": 0.34,
          "values": [
            0.34,
            0.34,
            0.05,
            0.34,
            0.65,
            0.72
          ]
        },
        "f1_score": {
          "mean": 0.01354659590278654,
          "std": 0.010571862558929517,
          "min": 0.0008367523270071996,
          "max": 0.0324165497585378,
          "median": 0.009521969014716759,
          "values": [
            0.00790448794836974,
            0.0069559761868289,
            0.0008367523270071996,
            0.011139450081063778,
            0.022026359114911825,
            0.0324165497585378
          ]
        },
        "jaccard": {
          "mean": 0.008517838589027944,
          "std": 0.007548715585224828,
          "min": 0.0004219207887564308,
          "max": 0.02139867556280669,
          "median": 0.0048492926199425425,
          "values": [
            0.004004692475962245,
            0.0035154361016346003,
            0.0004219207887564308,
            0.00569389276392284,
            0.016072413841084866,
            0.02139867556280669
          ]
        },
        "semantic_similarity": {
          "mean": 0.08583742355656189,
          "std": 0.017520150477436252,
          "min": 0.05541439942084253,
          "max": 0.1094636170216836,
          "median": 0.08366668778238817,
          "values": [
            0.07979790750890971,
            0.08658209849148989,
            0.05541439942084253,
            0.1094636170216836,
            0.10301524182315916,
            0.08075127707328647
          ]
        },
        "answer_correctness": {
          "mean": 0.46833333333333327,
          "std": 0.015723301886761,
          "min": 0.44,
          "max": 0.49,
          "median": 0.47,
          "values": [
            0.48,
            0.47,
            0.47,
            0.44,
            0.46,
            0.49
          ]
        },
        "follows_format_instruction": {
          "mean": 0.1978333333333333,
          "std": 0.00628269227499026,
          "min": 0.18699999999999994,
          "max": 0.20799999999999996,
          "median": 0.19799999999999995,
          "values": [
            0.19599999999999995,
            0.19999999999999996,
            0.19599999999999995,
            0.18699999999999994,
            0.20799999999999996,
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.8214,
          "std": 0.05587641124243162,
          "min": 0.7447999999999999,
          "max": 0.9076000000000001,
          "median": 0.8029999999999999,
          "values": [
            0.8159999999999998,
            0.9076000000000001,
            0.7900000000000001,
            0.7447999999999999,
            0.7899999999999998,
            0.88
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent": {
      "num_experiments": 6,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.008333333333333333,
          "std": 0.012133516482134198,
          "min": 0.0,
          "max": 0.03,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.02,
            0.03
          ]
        },
        "precision": {
          "mean": 0.014285713161371269,
          "std": 0.015778793977231656,
          "min": 0.0005192621365700335,
          "max": 0.04214882710953342,
          "median": 0.004881306612434676,
          "values": [
            0.004203346237664929,
            0.003621677298628642,
            0.0005192621365700335,
            0.0055592669872044235,
            0.029661899198626177,
            0.04214882710953342
          ]
        },
        "recall": {
          "mean": 0.4000000000000001,
          "std": 0.1914854215512676,
          "min": 0.05,
          "max": 0.69,
          "median": 0.39,
          "values": [
            0.4,
            0.38,
            0.05,
            0.37,
            0.51,
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.019787373471419626,
          "std": 0.01908416153628213,
          "min": 0.0010277187254434798,
          "max": 0.05364486570022528,
          "median": 0.009629617306662931,
          "values": [
            0.008314979305548883,
            0.007172080855749655,
            0.0010277187254434798,
            0.01094425530777698,
            0.03762034093377349,
            0.05364486570022528
          ]
        },
        "jaccard": {
          "mean": 0.014285713161371269,
          "std": 0.015778793977231656,
          "min": 0.0005192621365700335,
          "max": 0.04214882710953342,
          "median": 0.004881306612434676,
          "values": [
            0.004203346237664929,
            0.003621677298628642,
            0.0005192621365700335,
            0.0055592669872044235,
            0.029661899198626177,
            0.04214882710953342
          ]
        },
        "semantic_similarity": {
          "mean": 0.08765840355268058,
          "std": 0.022458783847568584,
          "min": 0.059290827624499796,
          "max": 0.11566578415920958,
          "median": 0.0872969322395511,
          "values": [
            0.06692904483061284,
            0.0709951462643221,
            0.059290827624499796,
            0.10359871821478009,
            0.11566578415920958,
            0.10947090022265911
          ]
        },
        "answer_correctness": {
          "mean": 0.4349999999999999,
          "std": 0.0655108133567785,
          "min": 0.3,
          "max": 0.49,
          "median": 0.46499999999999997,
          "values": [
            0.49,
            0.46,
            0.41,
            0.3,
            0.48,
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19466666666666663,
          "std": 0.03210226714043037,
          "min": 0.13799999999999998,
          "max": 0.23299999999999998,
          "median": 0.19799999999999995,
          "values": [
            0.19599999999999995,
            0.19999999999999996,
            0.17399999999999996,
            0.13799999999999998,
            0.23299999999999998,
            0.22699999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.7847333333333334,
          "std": 0.12893391933682755,
          "min": 0.5954,
          "max": 0.9640000000000001,
          "median": 0.825,
          "values": [
            0.8059999999999999,
            0.862,
            0.5954,
            0.637,
            0.8439999999999999,
            0.9640000000000001
          ]
        }
      }
    },
    "ecare_explanation_generation": {
      "num_experiments": 6,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.03852584816445039,
          "std": 0.00654964657547687,
          "min": 0.029972077692139765,
          "max": 0.048265080545658645,
          "median": 0.03744900280140678,
          "values": [
            0.032449076719028774,
            0.04557084842706161,
            0.03794357996519728,
            0.029972077692139765,
            0.036954425637616285,
            0.048265080545658645
          ]
        },
        "recall": {
          "mean": 0.46391475898828843,
          "std": 0.11628236865318055,
          "min": 0.20926401049930457,
          "max": 0.5604792103321515,
          "median": 0.49906251265074797,
          "values": [
            0.5604792103321515,
            0.5241906949259891,
            0.5060214556685145,
            0.20926401049930457,
            0.4921035696329814,
            0.49142961287078935
          ]
        },
        "f1_score": {
          "mean": 0.068622419003804,
          "std": 0.013320751825868216,
          "min": 0.04631395111031909,
          "max": 0.0862324113979467,
          "median": 0.06816383287898145,
          "values": [
            0.06052878394901255,
            0.08233170180758277,
            0.06938919762058396,
            0.04631395111031909,
            0.06693846813737894,
            0.0862324113979467
          ]
        },
        "jaccard": {
          "mean": 0.03649013603812255,
          "std": 0.0071136920596223665,
          "min": 0.024981592147678998,
          "max": 0.04608547698920304,
          "median": 0.03611344065332596,
          "values": [
            0.031707883672439735,
            0.04393898211276162,
            0.03659897824736949,
            0.024981592147678998,
            0.03562790305928242,
            0.04608547698920304
          ]
        },
        "semantic_similarity": {
          "mean": 0.482660980966563,
          "std": 0.11072200510418445,
          "min": 0.2561993674002588,
          "max": 0.5832784813642502,
          "median": 0.5326879568770528,
          "values": [
            0.5460975639522075,
            0.5832784813642502,
            0.519278349801898,
            0.2561993674002588,
            0.43876771464943887,
            0.5523444086313247
          ]
        },
        "response_brevity": {
          "mean": 0.3526666666666667,
          "std": 0.1648612210989055,
          "min": 0.20400000000000007,
          "max": 0.6809999999999999,
          "median": 0.29900000000000004,
          "values": [
            0.20400000000000007,
            0.32400000000000007,
            0.27399999999999997,
            0.6809999999999999,
            0.20800000000000007,
            0.4249999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8089166666666667,
          "std": 0.10041556015987875,
          "min": 0.6245,
          "max": 0.927,
          "median": 0.8467500000000001,
          "values": [
            0.6245,
            0.8715000000000002,
            0.7369999999999999,
            0.927,
            0.8295,
            0.8640000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.2503333333333333,
          "std": 0.19543597985586536,
          "min": 0.052000000000000005,
          "max": 0.602,
          "median": 0.208,
          "values": [
            0.052000000000000005,
            0.284,
            0.06000000000000001,
            0.602,
            0.132,
            0.37199999999999994
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american": {
      "num_experiments": 6,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.028169349226112514,
          "std": 0.003336120091836781,
          "min": 0.024600590783897664,
          "max": 0.03471821090973017,
          "median": 0.02751536439719257,
          "values": [
            0.02539934680838183,
            0.024600590783897664,
            0.026672471833243463,
            0.029267218060280285,
            0.028358256961141675,
            0.03471821090973017
          ]
        },
        "recall": {
          "mean": 0.45657449168478575,
          "std": 0.10031395966906814,
          "min": 0.247400302312067,
          "max": 0.5406409015820781,
          "median": 0.4854070243482008,
          "values": [
            0.5344611466964408,
            0.5406409015820781,
            0.4560957343310284,
            0.247400302312067,
            0.44613055082172726,
            0.5147183143653732
          ]
        },
        "f1_score": {
          "mean": 0.051649324838302195,
          "std": 0.005935787480126486,
          "min": 0.04658093408689554,
          "max": 0.0642778376021098,
          "median": 0.049279813628378716,
          "values": [
            0.047977696170655594,
            0.04658093408689554,
            0.049912306747785413,
            0.04864732050897201,
            0.052499853913394796,
            0.0642778376021098
          ]
        },
        "jaccard": {
          "mean": 0.026959499494672107,
          "std": 0.0031973036352764155,
          "min": 0.024130243659895724,
          "max": 0.03374377868076859,
          "median": 0.025790651814945956,
          "values": [
            0.02488708714681504,
            0.024130243659895724,
            0.025979021207328982,
            0.025602282422562926,
            0.027414583850661388,
            0.03374377868076859
          ]
        },
        "semantic_similarity": {
          "mean": 0.5201601640119528,
          "std": 0.06134456510770051,
          "min": 0.38778922430239615,
          "max": 0.5751970493793488,
          "median": 0.5355727617070078,
          "values": [
            0.5280703355371952,
            0.5587588514387608,
            0.5351960927993059,
            0.38778922430239615,
            0.5359494306147099,
            0.5751970493793488
          ]
        },
        "response_brevity": {
          "mean": 0.25316666666666665,
          "std": 0.1469607846407409,
          "min": 0.106,
          "max": 0.5539999999999999,
          "median": 0.21200000000000005,
          "values": [
            0.14300000000000002,
            0.106,
            0.22600000000000006,
            0.5539999999999999,
            0.19800000000000004,
            0.292
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8106666666666666,
          "std": 0.1274711296289826,
          "min": 0.544,
          "max": 0.9294999999999999,
          "median": 0.8695000000000002,
          "values": [
            0.544,
            0.7765,
            0.875,
            0.9294999999999999,
            0.8655000000000002,
            0.8735
          ]
        },
        "single_sentence_format": {
          "mean": 0.11466666666666665,
          "std": 0.14785879149452769,
          "min": 0.002,
          "max": 0.42599999999999993,
          "median": 0.066,
          "values": [
            0.013999999999999999,
            0.002,
            0.008,
            0.42599999999999993,
            0.12,
            0.118
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical": {
      "num_experiments": 6,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.02802880614742631,
          "std": 0.004368287631701154,
          "min": 0.02190535877702346,
          "max": 0.03246709410402508,
          "median": 0.029667082232964128,
          "values": [
            0.02266541758495709,
            0.02190535877702346,
            0.027553456553650088,
            0.031800801952624004,
            0.03178070791227817,
            0.03246709410402508
          ]
        },
        "recall": {
          "mean": 0.4839858673461615,
          "std": 0.057326344074851245,
          "min": 0.3685907785025432,
          "max": 0.5440752074722662,
          "median": 0.48798641309670726,
          "values": [
            0.5440752074722662,
            0.536275461466638,
            0.479000930442107,
            0.3685907785025432,
            0.48813311361840783,
            0.4878397125750067
          ]
        },
        "f1_score": {
          "mean": 0.05185599356038673,
          "std": 0.007185557729905206,
          "min": 0.041750485357798885,
          "max": 0.060162972182120834,
          "median": 0.053574854234683916,
          "values": [
            0.04316994111214483,
            0.041750485357798885,
            0.05161889758064091,
            0.05553081088872693,
            0.05890285424088801,
            0.060162972182120834
          ]
        },
        "jaccard": {
          "mean": 0.027072336001702713,
          "std": 0.003920730157619606,
          "min": 0.021547826665100175,
          "max": 0.031521197770612505,
          "median": 0.028094134400924665,
          "values": [
            0.02232481265604236,
            0.021547826665100175,
            0.02686778350922517,
            0.029320485292624158,
            0.030851910116611946,
            0.031521197770612505
          ]
        },
        "semantic_similarity": {
          "mean": 0.5403357746017476,
          "std": 0.03060256639365527,
          "min": 0.47987603928893807,
          "max": 0.5680893646925688,
          "median": 0.5522693594172596,
          "values": [
            0.5488447332382203,
            0.5653236883878708,
            0.5556939855962991,
            0.47987603928893807,
            0.5241868364065886,
            0.5680893646925688
          ]
        },
        "response_brevity": {
          "mean": 0.19566666666666666,
          "std": 0.09402954618392857,
          "min": 0.09299999999999999,
          "max": 0.349,
          "median": 0.17000000000000007,
          "values": [
            0.10199999999999998,
            0.09299999999999999,
            0.17200000000000007,
            0.349,
            0.16800000000000007,
            0.29
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7912500000000001,
          "std": 0.09850877710471624,
          "min": 0.607,
          "max": 0.884,
          "median": 0.8257500000000001,
          "values": [
            0.607,
            0.7829999999999999,
            0.7340000000000001,
            0.8709999999999998,
            0.8685000000000003,
            0.884
          ]
        },
        "single_sentence_format": {
          "mean": 0.05033333333333334,
          "std": 0.07358819349747772,
          "min": 0.0,
          "max": 0.20600000000000002,
          "median": 0.015,
          "values": [
            0.0,
            0.0,
            0.0,
            0.20600000000000002,
            0.066,
            0.03
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist": {
      "num_experiments": 6,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.024529002329534793,
          "std": 0.003953397147876815,
          "min": 0.0189175577179169,
          "max": 0.02929200460140548,
          "median": 0.026177114359537013,
          "values": [
            0.01935852294089931,
            0.0189175577179169,
            0.02628224892419033,
            0.027251699997913034,
            0.026071979794883696,
            0.02929200460140548
          ]
        },
        "recall": {
          "mean": 0.46889595698419234,
          "std": 0.0492306187479521,
          "min": 0.3883164858017799,
          "max": 0.5358647153500096,
          "median": 0.46123735170058705,
          "values": [
            0.521779503176562,
            0.5358647153500096,
            0.4550645335057101,
            0.3883164858017799,
            0.44494033417562834,
            0.467410169895464
          ]
        },
        "f1_score": {
          "mean": 0.045863485257449495,
          "std": 0.006787076287766477,
          "min": 0.03627560367279813,
          "max": 0.054526152588432734,
          "median": 0.048951905286357913,
          "values": [
            0.037057229303371096,
            0.03627560367279813,
            0.04923023302997629,
            0.049418115407379164,
            0.04867357754273953,
            0.054526152588432734
          ]
        },
        "jaccard": {
          "mean": 0.02380613235302236,
          "std": 0.003664926081859763,
          "min": 0.01862308872016518,
          "max": 0.02844595563828819,
          "median": 0.025436498699451443,
          "values": [
            0.019046013172166105,
            0.01862308872016518,
            0.025596235273989052,
            0.025848739188611792,
            0.025276762124913837,
            0.02844595563828819
          ]
        },
        "semantic_similarity": {
          "mean": 0.484432678008452,
          "std": 0.02842296706272849,
          "min": 0.44903883112594484,
          "max": 0.53038289681077,
          "median": 0.48466149732470515,
          "values": [
            0.4713038282096386,
            0.53038289681077,
            0.4980191664397717,
            0.44903883112594484,
            0.4558450558036566,
            0.5020062896609306
          ]
        },
        "response_brevity": {
          "mean": 0.1496666666666667,
          "std": 0.07292614224146389,
          "min": 0.04899999999999999,
          "max": 0.284,
          "median": 0.14600000000000002,
          "values": [
            0.09799999999999998,
            0.04899999999999999,
            0.16000000000000003,
            0.17500000000000007,
            0.132,
            0.284
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7179166666666666,
          "std": 0.11393909801097934,
          "min": 0.4770000000000001,
          "max": 0.8155000000000002,
          "median": 0.75125,
          "values": [
            0.4770000000000001,
            0.709,
            0.7325,
            0.8035,
            0.8155000000000002,
            0.77
          ]
        },
        "single_sentence_format": {
          "mean": 0.02,
          "std": 0.024657656011875903,
          "min": 0.0,
          "max": 0.06,
          "median": 0.006000000000000001,
          "values": [
            0.0,
            0.0,
            0.0,
            0.06,
            0.04800000000000001,
            0.012000000000000002
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent": {
      "num_experiments": 6,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.023886632037156522,
          "std": 0.0031500880253628716,
          "min": 0.019163296167254287,
          "max": 0.02803827301960644,
          "median": 0.024973581531051676,
          "values": [
            0.020226702046395123,
            0.019163296167254287,
            0.024979105849702088,
            0.025944357927579927,
            0.02803827301960644,
            0.02496805721240127
          ]
        },
        "recall": {
          "mean": 0.42072774148509445,
          "std": 0.07069959315458532,
          "min": 0.28173212898212896,
          "max": 0.4955000603971192,
          "median": 0.4234922267275208,
          "values": [
            0.4908949808361573,
            0.4955000603971192,
            0.40925482524011936,
            0.28173212898212896,
            0.4213058118352236,
            0.425678641619818
          ]
        },
        "f1_score": {
          "mean": 0.044034262492863,
          "std": 0.005153141706548133,
          "min": 0.036631992125233236,
          "max": 0.05191380799356806,
          "median": 0.04518130847368075,
          "values": [
            0.03854518039754913,
            0.036631992125233236,
            0.04625390969398709,
            0.0441087072533744,
            0.05191380799356806,
            0.04675197749346605
          ]
        },
        "jaccard": {
          "mean": 0.022903793048854374,
          "std": 0.002793544006125895,
          "min": 0.018844493331047055,
          "max": 0.02711375677050899,
          "median": 0.023672601773105505,
          "values": [
            0.019853574130229187,
            0.018844493331047055,
            0.024047286391003904,
            0.02329791715520711,
            0.02711375677050899,
            0.024265730515130007
          ]
        },
        "semantic_similarity": {
          "mean": 0.4579445618138804,
          "std": 0.05355076111711671,
          "min": 0.34247431508963927,
          "max": 0.4963519271463156,
          "median": 0.4790399205684662,
          "values": [
            0.4902907335758209,
            0.49335808366537093,
            0.4677891075611115,
            0.34247431508963927,
            0.4574032038450241,
            0.4963519271463156
          ]
        },
        "response_brevity": {
          "mean": 0.2145,
          "std": 0.12634839400113745,
          "min": 0.07399999999999998,
          "max": 0.44799999999999995,
          "median": 0.19,
          "values": [
            0.10899999999999999,
            0.07399999999999998,
            0.138,
            0.44799999999999995,
            0.24200000000000002,
            0.276
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8090833333333333,
          "std": 0.08401161460708217,
          "min": 0.6744999999999999,
          "max": 0.8980000000000001,
          "median": 0.8342499999999999,
          "values": [
            0.6744999999999999,
            0.7199999999999999,
            0.8345,
            0.8980000000000001,
            0.8935000000000001,
            0.8339999999999997
          ]
        },
        "single_sentence_format": {
          "mean": 0.09066666666666667,
          "std": 0.1201868915018975,
          "min": 0.0,
          "max": 0.32599999999999996,
          "median": 0.025,
          "values": [
            0.0,
            0.0,
            0.036000000000000004,
            0.32599999999999996,
            0.168,
            0.014000000000000002
          ]
        }
      }
    },
    "ecqa_choice_selection": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.16,
          "std": 0.26083200212652846,
          "min": 0.0,
          "max": 0.71,
          "median": 0.005,
          "values": [
            0.0,
            0.71,
            0.0,
            0.0,
            0.01,
            0.24
          ]
        },
        "precision": {
          "mean": 0.1792665460812561,
          "std": 0.2569537769431573,
          "min": 0.010596463102426823,
          "max": 0.7200303295285826,
          "median": 0.03563460139401252,
          "values": [
            0.01392248062886549,
            0.7200303295285826,
            0.025762460610937087,
            0.010596463102426823,
            0.04550674217708795,
            0.2597808004396367
          ]
        },
        "recall": {
          "mean": 0.5005555555555555,
          "std": 0.17085207665921257,
          "min": 0.2833333333333333,
          "max": 0.81,
          "median": 0.52,
          "values": [
            0.33,
            0.81,
            0.54,
            0.2833333333333333,
            0.54,
            0.5
          ]
        },
        "f1_score": {
          "mean": 0.19321859687243234,
          "std": 0.2530177232231008,
          "min": 0.020271237832488128,
          "max": 0.7257207795377792,
          "median": 0.058206044817825556,
          "values": [
            0.026068600412325373,
            0.7257207795377792,
            0.04777438834102674,
            0.020271237832488128,
            0.06863770129462438,
            0.2708388738163502
          ]
        },
        "jaccard": {
          "mean": 0.1784708694711776,
          "std": 0.2570120102394336,
          "min": 0.010506532081025269,
          "max": 0.7198487332966234,
          "median": 0.03453791320974348,
          "values": [
            0.013670879765186989,
            0.7198487332966234,
            0.02571569986949813,
            0.010506532081025269,
            0.04336012654998884,
            0.25772324526474283
          ]
        },
        "semantic_similarity": {
          "mean": 0.5262450123361001,
          "std": 0.15221699251447143,
          "min": 0.35456543466076257,
          "max": 0.8337197145819664,
          "median": 0.48403886079788205,
          "values": [
            0.4548057132959366,
            0.8337197145819664,
            0.4356939936056733,
            0.35456543466076257,
            0.5132720082998276,
            0.5654132095724345
          ]
        },
        "answer_correctness": {
          "mean": 0.59,
          "std": 0.16653327995729061,
          "min": 0.28,
          "max": 0.8,
          "median": 0.635,
          "values": [
            0.7,
            0.8,
            0.63,
            0.28,
            0.64,
            0.49
          ]
        },
        "follows_format_instruction": {
          "mean": 0.391,
          "std": 0.2565274514225199,
          "min": 0.161,
          "max": 0.889,
          "median": 0.2895,
          "values": [
            0.161,
            0.889,
            0.21300000000000008,
            0.18600000000000008,
            0.36599999999999994,
            0.5309999999999999
          ]
        },
        "answer_extractability": {
          "mean": 0.8234,
          "std": 0.08197550447135618,
          "min": 0.6979,
          "max": 0.9538,
          "median": 0.8204500000000001,
          "values": [
            0.7632000000000001,
            0.9538,
            0.8299000000000002,
            0.8846000000000003,
            0.6979,
            0.811
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.008333333333333333,
          "std": 0.018633899812498248,
          "min": 0.0,
          "max": 0.05,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.05
          ]
        },
        "precision": {
          "mean": 0.016162576493054682,
          "std": 0.01916627527617422,
          "min": 0.004456594705640506,
          "max": 0.058802392947091234,
          "median": 0.008023386496071361,
          "values": [
            0.0075244364532344185,
            0.008522336538908305,
            0.006771988861443903,
            0.004456594705640506,
            0.010897709452009716,
            0.058802392947091234
          ]
        },
        "recall": {
          "mean": 0.47666666666666674,
          "std": 0.08730533902472529,
          "min": 0.335,
          "max": 0.615,
          "median": 0.4675,
          "values": [
            0.435,
            0.54,
            0.485,
            0.335,
            0.615,
            0.45
          ]
        },
        "f1_score": {
          "mean": 0.023555752931827115,
          "std": 0.019827226673004485,
          "min": 0.008781695072649216,
          "max": 0.06717507399267747,
          "median": 0.015729656176521282,
          "values": [
            0.014743151517810566,
            0.016716160835231995,
            0.013331199194835587,
            0.008781695072649216,
            0.020587236977757863,
            0.06717507399267747
          ]
        },
        "jaccard": {
          "mean": 0.016100172360792312,
          "std": 0.019173934091498136,
          "min": 0.004442679345564411,
          "max": 0.05877274591003262,
          "median": 0.007999814605481773,
          "values": [
            0.007500635352431935,
            0.00849899385853161,
            0.006752608745969655,
            0.004442679345564411,
            0.01063337095222363,
            0.05877274591003262
          ]
        },
        "semantic_similarity": {
          "mean": 0.3890048677039643,
          "std": 0.033340110652731396,
          "min": 0.33140153259038924,
          "max": 0.43076386410743,
          "median": 0.39333084266632795,
          "values": [
            0.3790281207114458,
            0.41642322421073913,
            0.36877889998257163,
            0.33140153259038924,
            0.43076386410743,
            0.4076335646212101
          ]
        },
        "answer_correctness": {
          "mean": 0.4066666666666667,
          "std": 0.1243203746598101,
          "min": 0.24,
          "max": 0.64,
          "median": 0.375,
          "values": [
            0.34,
            0.47,
            0.38,
            0.24,
            0.64,
            0.37
          ]
        },
        "follows_format_instruction": {
          "mean": 0.11166666666666665,
          "std": 0.019988885800753274,
          "min": 0.09799999999999998,
          "max": 0.153,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998,
            0.09999999999999998,
            0.09899999999999999,
            0.09799999999999998,
            0.12,
            0.153
          ]
        },
        "answer_extractability": {
          "mean": 0.6638000000000001,
          "std": 0.052358666904343605,
          "min": 0.6062,
          "max": 0.7589000000000001,
          "median": 0.6505499999999999,
          "values": [
            0.6062,
            0.6435999999999998,
            0.6574999999999999,
            0.7006,
            0.7589000000000001,
            0.616
          ]
        }
      }
    },
    "ecqa_demographic_senior_european": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.008333333333333333,
          "std": 0.018633899812498248,
          "min": 0.0,
          "max": 0.05,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.05
          ]
        },
        "precision": {
          "mean": 0.017925438525932185,
          "std": 0.020148911924021067,
          "min": 0.005083942634255996,
          "max": 0.06235942907808319,
          "median": 0.008443456096641641,
          "values": [
            0.007321897345832062,
            0.008787499082289697,
            0.008099413110993587,
            0.005083942634255996,
            0.015900449904138577,
            0.06235942907808319
          ]
        },
        "recall": {
          "mean": 0.4816666666666667,
          "std": 0.0930352382463524,
          "min": 0.34,
          "max": 0.595,
          "median": 0.47750000000000004,
          "values": [
            0.42,
            0.53,
            0.425,
            0.34,
            0.595,
            0.58
          ]
        },
        "f1_score": {
          "mean": 0.026691543804315005,
          "std": 0.021931539730303855,
          "min": 0.009984408741376986,
          "max": 0.07405976287381924,
          "median": 0.016540512269419445,
          "values": [
            0.014357012832301902,
            0.017228950058844103,
            0.015852074479994788,
            0.009984408741376986,
            0.028667053839553005,
            0.07405976287381924
          ]
        },
        "jaccard": {
          "mean": 0.017772060888491644,
          "std": 0.020153068557655775,
          "min": 0.00507128939950518,
          "max": 0.06230743797721426,
          "median": 0.00841147885900975,
          "values": [
            0.007296665108754196,
            0.00875311524941499,
            0.008069842468604512,
            0.00507128939950518,
            0.01513401512745671,
            0.06230743797721426
          ]
        },
        "semantic_similarity": {
          "mean": 0.37973341152382395,
          "std": 0.05308041839582512,
          "min": 0.28721057476475836,
          "max": 0.4604719139635563,
          "median": 0.39067181980237364,
          "values": [
            0.38342820569872854,
            0.4014344811439514,
            0.3479398596659303,
            0.28721057476475836,
            0.4604719139635563,
            0.39791543390601875
          ]
        },
        "answer_correctness": {
          "mean": 0.4166666666666667,
          "std": 0.14173527750312867,
          "min": 0.18,
          "max": 0.65,
          "median": 0.425,
          "values": [
            0.34,
            0.43,
            0.48,
            0.18,
            0.65,
            0.42
          ]
        },
        "follows_format_instruction": {
          "mean": 0.12616666666666668,
          "std": 0.04056442064447889,
          "min": 0.09799999999999998,
          "max": 0.20300000000000007,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998,
            0.09999999999999998,
            0.09799999999999998,
            0.09799999999999998,
            0.20300000000000007,
            0.158
          ]
        },
        "answer_extractability": {
          "mean": 0.6561500000000001,
          "std": 0.06395526431290338,
          "min": 0.5879000000000001,
          "max": 0.7571000000000001,
          "median": 0.6285,
          "values": [
            0.6078000000000001,
            0.6483999999999999,
            0.7571000000000001,
            0.5879000000000001,
            0.7271,
            0.6086
          ]
        }
      }
    },
    "ecqa_demographic_young_american": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.006666666666666667,
          "std": 0.014907119849998597,
          "min": 0.0,
          "max": 0.04,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.04
          ]
        },
        "precision": {
          "mean": 0.016326723610308846,
          "std": 0.015574492641793915,
          "min": 0.00624003478957561,
          "max": 0.05081169626119044,
          "median": 0.009600599004865547,
          "values": [
            0.008851904336562421,
            0.008311672919133647,
            0.010349293673168674,
            0.00624003478957561,
            0.013395739682222275,
            0.05081169626119044
          ]
        },
        "recall": {
          "mean": 0.5111111111111112,
          "std": 0.09071050138615924,
          "min": 0.405,
          "max": 0.675,
          "median": 0.46833333333333327,
          "values": [
            0.465,
            0.585,
            0.47,
            0.405,
            0.675,
            0.4666666666666666
          ]
        },
        "f1_score": {
          "mean": 0.02530409951249232,
          "std": 0.016406742239907107,
          "min": 0.01226328848181181,
          "max": 0.06100051377676524,
          "median": 0.01874652832705434,
          "values": [
            0.01731503299164184,
            0.01633992298797158,
            0.020178023662466838,
            0.01226328848181181,
            0.024727815174296598,
            0.06100051377676524
          ]
        },
        "jaccard": {
          "mean": 0.016214420464708848,
          "std": 0.015573542491526217,
          "min": 0.0062284589483395745,
          "max": 0.050740899211527586,
          "median": 0.00955895547974973,
          "values": [
            0.008818959330429315,
            0.008289133337773414,
            0.010298951629070146,
            0.0062284589483395745,
            0.012910120331113056,
            0.050740899211527586
          ]
        },
        "semantic_similarity": {
          "mean": 0.4002315909881145,
          "std": 0.04273673103496829,
          "min": 0.31469399413093924,
          "max": 0.4487741879373789,
          "median": 0.40351073347032074,
          "values": [
            0.4007721272110939,
            0.4062493397295475,
            0.4352716864645481,
            0.31469399413093924,
            0.4487741879373789,
            0.3956282104551792
          ]
        },
        "answer_correctness": {
          "mean": 0.4116666666666667,
          "std": 0.13309353436169952,
          "min": 0.22,
          "max": 0.59,
          "median": 0.375,
          "values": [
            0.37,
            0.38,
            0.58,
            0.22,
            0.59,
            0.33
          ]
        },
        "follows_format_instruction": {
          "mean": 0.11783333333333333,
          "std": 0.023533073652958217,
          "min": 0.09999999999999998,
          "max": 0.158,
          "median": 0.10249999999999998,
          "values": [
            0.10399999999999998,
            0.09999999999999998,
            0.10099999999999998,
            0.10099999999999998,
            0.14300000000000002,
            0.158
          ]
        },
        "answer_extractability": {
          "mean": 0.6888,
          "std": 0.08748634179116196,
          "min": 0.5799,
          "max": 0.8152000000000001,
          "median": 0.6868999999999998,
          "values": [
            0.5799,
            0.5955999999999999,
            0.8152000000000001,
            0.7683000000000001,
            0.7260999999999999,
            0.6476999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice_selection_3choices__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3-8b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_3choices__u4b-mistral-7b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.21833333333333335,
          "std": 0.334983415841574,
          "min": 0.0,
          "max": 0.88,
          "median": 0.0,
          "values": [
            0.0,
            0.88,
            0.0,
            0.0,
            0.0,
            0.43
          ]
        },
        "precision": {
          "mean": 0.2419502223475339,
          "std": 0.3253660684439245,
          "min": 0.015070599419300978,
          "max": 0.8861134676564157,
          "median": 0.0446361364072172,
          "values": [
            0.015070599419300978,
            0.8861134676564157,
            0.05439928258493284,
            0.01796684672017075,
            0.03487299022950155,
            0.4432781474748814
          ]
        },
        "recall": {
          "mean": 0.6124999999999999,
          "std": 0.18582585934148133,
          "min": 0.325,
          "max": 0.915,
          "median": 0.62,
          "values": [
            0.325,
            0.915,
            0.72,
            0.475,
            0.585,
            0.655
          ]
        },
        "f1_score": {
          "mean": 0.25924833020708543,
          "std": 0.31717436959610334,
          "min": 0.028028060643947743,
          "max": 0.8871428571428572,
          "median": 0.07624270724109952,
          "values": [
            0.028028060643947743,
            0.8871428571428572,
            0.0937732139126142,
            0.034374142890135184,
            0.05871220056958485,
            0.4534595060833736
          ]
        },
        "jaccard": {
          "mean": 0.24110680050550667,
          "std": 0.32512449456897813,
          "min": 0.014757363766174309,
          "max": 0.8844468009897491,
          "median": 0.04326218051611891,
          "values": [
            0.014757363766174309,
            0.8844468009897491,
            0.05436054496686274,
            0.017903803683989073,
            0.03216381606537508,
            0.4430084735608898
          ]
        },
        "semantic_similarity": {
          "mean": 0.5930621203035116,
          "std": 0.17277088627415166,
          "min": 0.43138746194541455,
          "max": 0.9308395344018936,
          "median": 0.5151594095677137,
          "values": [
            0.47166656263172624,
            0.9308395344018936,
            0.4854020461440086,
            0.43138746194541455,
            0.5449167729914188,
            0.6941603437066078
          ]
        },
        "answer_correctness": {
          "mean": 0.7216666666666667,
          "std": 0.11480660066196349,
          "min": 0.51,
          "max": 0.9,
          "median": 0.72,
          "values": [
            0.72,
            0.9,
            0.71,
            0.51,
            0.77,
            0.72
          ]
        },
        "follows_format_instruction": {
          "mean": 0.453,
          "std": 0.2702264482491181,
          "min": 0.1960000000000001,
          "max": 0.9710000000000001,
          "median": 0.3185,
          "values": [
            0.275,
            0.9710000000000001,
            0.293,
            0.1960000000000001,
            0.3440000000000001,
            0.639
          ]
        },
        "answer_extractability": {
          "mean": 0.82775,
          "std": 0.09343369752574997,
          "min": 0.7243999999999998,
          "max": 0.995,
          "median": 0.8147,
          "values": [
            0.742,
            0.995,
            0.7715000000000001,
            0.8757,
            0.7243999999999998,
            0.8578999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_choice_selection_no_concept__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_choice_selection_no_concept__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3-8b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-mistral-7b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.18333333333333335,
          "std": 0.2729265265394496,
          "min": 0.0,
          "max": 0.71,
          "median": 0.005,
          "values": [
            0.01,
            0.71,
            0.0,
            0.0,
            0.0,
            0.38
          ]
        },
        "precision": {
          "mean": 0.20245396903808702,
          "std": 0.26535900618168173,
          "min": 0.012120390341419882,
          "max": 0.713981449444274,
          "median": 0.03439223723377034,
          "values": [
            0.025522110573418848,
            0.713981449444274,
            0.034395576029195414,
            0.012120390341419882,
            0.03438889843834528,
            0.3943153894018689
          ]
        },
        "recall": {
          "mean": 0.5280555555555555,
          "std": 0.15645794804410815,
          "min": 0.31166666666666665,
          "max": 0.775,
          "median": 0.565,
          "values": [
            0.3516666666666666,
            0.775,
            0.6,
            0.31166666666666665,
            0.57,
            0.56
          ]
        },
        "f1_score": {
          "mean": 0.21697249982090386,
          "std": 0.2596184546861093,
          "min": 0.02315734635492113,
          "max": 0.7170445398747617,
          "median": 0.05927097193541937,
          "values": [
            0.0385387951016717,
            0.7170445398747617,
            0.061479280912402456,
            0.02315734635492113,
            0.05706266295843628,
            0.4045523737232297
          ]
        },
        "jaccard": {
          "mean": 0.20185610209002489,
          "std": 0.26555182534451144,
          "min": 0.012029112390921175,
          "max": 0.7138028780157026,
          "median": 0.033047502208051416,
          "values": [
            0.02539481979440683,
            0.7138028780157026,
            0.034357544259054885,
            0.012029112390921175,
            0.03173746015704794,
            0.39381479792301577
          ]
        },
        "semantic_similarity": {
          "mean": 0.5508094951487147,
          "std": 0.1540400811896906,
          "min": 0.37241387432906775,
          "max": 0.8369075068831444,
          "median": 0.4974453831091523,
          "values": [
            0.47294129610061647,
            0.8369075068831444,
            0.4459284994006157,
            0.37241387432906775,
            0.5219494701176882,
            0.6547163240611553
          ]
        },
        "answer_correctness": {
          "mean": 0.6316666666666666,
          "std": 0.17808393776219375,
          "min": 0.26,
          "max": 0.81,
          "median": 0.685,
          "values": [
            0.74,
            0.81,
            0.61,
            0.26,
            0.72,
            0.65
          ]
        },
        "follows_format_instruction": {
          "mean": 0.4286666666666667,
          "std": 0.25311437906387074,
          "min": 0.21300000000000005,
          "max": 0.8859999999999999,
          "median": 0.2975,
          "values": [
            0.22500000000000003,
            0.8859999999999999,
            0.262,
            0.21300000000000005,
            0.333,
            0.6529999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8329833333333334,
          "std": 0.0702068234259004,
          "min": 0.7214999999999999,
          "max": 0.9483,
          "median": 0.8295999999999999,
          "values": [
            0.8022,
            0.9483,
            0.8004000000000001,
            0.8685,
            0.7214999999999999,
            0.8569999999999999
          ]
        }
      }
    },
    "ecqa_expert_anthropologist": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10845277751663579,
          "std": 0.00948979399174655,
          "min": 0.09650138003581887,
          "max": 0.12267641724595217,
          "median": 0.105203068019411,
          "values": [
            0.09650138003581887,
            0.10244197370875899,
            0.1192233368466715,
            0.12267641724595217,
            0.10190939493255025,
            0.10796416233006301
          ]
        },
        "recall": {
          "mean": 0.3879405648381451,
          "std": 0.024464785943817076,
          "min": 0.34810781854230255,
          "max": 0.4260019471045889,
          "median": 0.3866513159223698,
          "values": [
            0.38084291264397324,
            0.4260019471045889,
            0.4055359033221203,
            0.34810781854230255,
            0.3924597192007664,
            0.37469508821511915
          ]
        },
        "f1_score": {
          "mean": 0.16565248091926066,
          "std": 0.010088188078207765,
          "min": 0.15099882675974377,
          "max": 0.18065707063999145,
          "median": 0.1634355201675323,
          "values": [
            0.15099882675974377,
            0.1623083618713913,
            0.18065707063999145,
            0.17635583715970152,
            0.1590321106210627,
            0.16456267846367328
          ]
        },
        "jaccard": {
          "mean": 0.09101778784815316,
          "std": 0.006104900071657529,
          "min": 0.08225184840743578,
          "max": 0.10006887092421408,
          "median": 0.0896186029267241,
          "values": [
            0.08225184840743578,
            0.0889264163712685,
            0.10006887092421408,
            0.09760465403467201,
            0.08694414786914884,
            0.09031078948217973
          ]
        },
        "semantic_similarity": {
          "mean": 0.6095648536582788,
          "std": 0.026892353338074004,
          "min": 0.5743074625730514,
          "max": 0.6502753323316575,
          "median": 0.608698766976595,
          "values": [
            0.6301788941025734,
            0.5942050370573998,
            0.6502753323316575,
            0.5743074625730514,
            0.6231924968957901,
            0.5852298989892006
          ]
        },
        "all_choices_addressed": {
          "mean": 0.28933333333333333,
          "std": 0.05457309064202087,
          "min": 0.212,
          "max": 0.3880000000000001,
          "median": 0.28500000000000003,
          "values": [
            0.302,
            0.3880000000000001,
            0.30999999999999994,
            0.256,
            0.268,
            0.212
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8833333333333334,
          "std": 0.22826495326459742,
          "min": 0.3740000000000002,
          "max": 1.0,
          "median": 0.9830000000000001,
          "values": [
            0.972,
            0.96,
            1.0,
            1.0,
            0.9940000000000001,
            0.3740000000000002
          ]
        },
        "explanation_depth": {
          "mean": 0.4439,
          "std": 0.053623750956704526,
          "min": 0.3849,
          "max": 0.543,
          "median": 0.4209,
          "values": [
            0.543,
            0.48340000000000005,
            0.4103,
            0.4108,
            0.3849,
            0.431
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10373676147244519,
          "std": 0.005874275387701972,
          "min": 0.09607084698848553,
          "max": 0.11263724615167627,
          "median": 0.1031132158857048,
          "values": [
            0.09607084698848553,
            0.10027596243556176,
            0.10885086322362555,
            0.11263724615167627,
            0.09863518069947419,
            0.10595046933584781
          ]
        },
        "recall": {
          "mean": 0.3853737079349897,
          "std": 0.019021212312410316,
          "min": 0.37047164898773843,
          "max": 0.4257268849721244,
          "median": 0.37921011063836035,
          "values": [
            0.37047164898773843,
            0.4257268849721244,
            0.3867849572008764,
            0.3708385351724781,
            0.3752638393508984,
            0.3831563819258223
          ]
        },
        "f1_score": {
          "mean": 0.16021706365599428,
          "std": 0.007046856928115162,
          "min": 0.14936706195967742,
          "max": 0.1692936614355088,
          "median": 0.16117698074963777,
          "values": [
            0.14936706195967742,
            0.1593944547925764,
            0.16683838327643852,
            0.1692936614355088,
            0.15344931376506538,
            0.16295950670669915
          ]
        },
        "jaccard": {
          "mean": 0.08773468900800997,
          "std": 0.004205165397972572,
          "min": 0.08134974725981296,
          "max": 0.09312802898286413,
          "median": 0.08826133175265526,
          "values": [
            0.08134974725981296,
            0.08722653665749008,
            0.09179937456138797,
            0.09312802898286413,
            0.08360831973868428,
            0.08929612684782043
          ]
        },
        "semantic_similarity": {
          "mean": 0.4952886815865835,
          "std": 0.027556937008124595,
          "min": 0.445855346173048,
          "max": 0.5396952456235886,
          "median": 0.49532421611249444,
          "values": [
            0.5396952456235886,
            0.4992094074189663,
            0.5047750861942768,
            0.4907579793035984,
            0.445855346173048,
            0.49143902480602264
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3503333333333334,
          "std": 0.05729940856782229,
          "min": 0.28,
          "max": 0.4160000000000001,
          "median": 0.3530000000000001,
          "values": [
            0.4160000000000001,
            0.41200000000000014,
            0.39000000000000007,
            0.28,
            0.31600000000000006,
            0.28800000000000003
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8573333333333334,
          "std": 0.29214018704876754,
          "min": 0.205,
          "max": 1.0,
          "median": 0.991,
          "values": [
            1.0,
            0.9570000000000001,
            1.0,
            1.0,
            0.982,
            0.205
          ]
        },
        "explanation_depth": {
          "mean": 0.3788833333333334,
          "std": 0.061079331383228345,
          "min": 0.3018,
          "max": 0.4709,
          "median": 0.35814999999999997,
          "values": [
            0.4467000000000001,
            0.4709,
            0.3018,
            0.338,
            0.33760000000000007,
            0.37829999999999997
          ]
        }
      }
    },
    "ecqa_expert_psychologist": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10367506491007948,
          "std": 0.00679839406765123,
          "min": 0.09450176493727287,
          "max": 0.11468414071289271,
          "median": 0.10300639160670907,
          "values": [
            0.09450176493727287,
            0.09829710417820166,
            0.10855459641869158,
            0.11468414071289271,
            0.0999787479998068,
            0.10603403521361136
          ]
        },
        "recall": {
          "mean": 0.38063124037961377,
          "std": 0.0216154681065376,
          "min": 0.35153406303252865,
          "max": 0.4238067413167893,
          "median": 0.3768846163428416,
          "values": [
            0.38188838754118026,
            0.4238067413167893,
            0.37773488076639417,
            0.35153406303252865,
            0.376034351919289,
            0.3727890177015011
          ]
        },
        "f1_score": {
          "mean": 0.15958399197939485,
          "std": 0.00670715973950276,
          "min": 0.14893942173234243,
          "max": 0.168990563731321,
          "median": 0.1593777703034296,
          "values": [
            0.14893942173234243,
            0.15682926718908757,
            0.16559036685771542,
            0.168990563731321,
            0.1552280589481312,
            0.1619262734177716
          ]
        },
        "jaccard": {
          "mean": 0.08731605362186638,
          "std": 0.00403347813064126,
          "min": 0.08103738515521011,
          "max": 0.09307741938536879,
          "median": 0.08712984795801296,
          "values": [
            0.08103738515521011,
            0.08559920749288855,
            0.0909111329082894,
            0.09307741938536879,
            0.08461068836630405,
            0.08866048842313738
          ]
        },
        "semantic_similarity": {
          "mean": 0.5618226146697998,
          "std": 0.025670996701692796,
          "min": 0.5240909847617149,
          "max": 0.5997585594654083,
          "median": 0.5619978252053262,
          "values": [
            0.5997585594654083,
            0.548807926774025,
            0.5751877236366272,
            0.5240909847617149,
            0.5808199259638787,
            0.5422705674171447
          ]
        },
        "all_choices_addressed": {
          "mean": 0.30033333333333334,
          "std": 0.07201311608928537,
          "min": 0.2,
          "max": 0.41400000000000003,
          "median": 0.28300000000000003,
          "values": [
            0.41400000000000003,
            0.37,
            0.25199999999999995,
            0.2,
            0.29600000000000004,
            0.27
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8585000000000002,
          "std": 0.26664692635268333,
          "min": 0.267,
          "max": 1.0,
          "median": 0.99,
          "values": [
            0.986,
            0.904,
            1.0,
            1.0,
            0.9940000000000001,
            0.267
          ]
        },
        "explanation_depth": {
          "mean": 0.3349,
          "std": 0.06374161382749366,
          "min": 0.25370000000000004,
          "max": 0.4244,
          "median": 0.3202,
          "values": [
            0.4244,
            0.4129,
            0.278,
            0.25370000000000004,
            0.317,
            0.32339999999999997
          ]
        }
      }
    },
    "ecqa_freeflow_explanation": {
      "num_experiments": 7,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b_ft_ecqa_freeflow_ft__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.22566040409464266,
          "std": 0.1082397009264661,
          "min": 0.1628305079509147,
          "max": 0.4898914464228833,
          "median": 0.18420417725092644,
          "values": [
            0.1835645434735207,
            0.19209061512850714,
            0.18420417725092644,
            0.1896884999747779,
            0.1628305079509147,
            0.1773530384609687,
            0.4898914464228833
          ]
        },
        "recall": {
          "mean": 0.4479705088774989,
          "std": 0.03462691780898772,
          "min": 0.39513423805964415,
          "max": 0.502578757763266,
          "median": 0.43879755377743423,
          "values": [
            0.39513423805964415,
            0.43468391689912195,
            0.46781162855887837,
            0.43879755377743423,
            0.502578757763266,
            0.4797364105036326,
            0.417051056580515
          ]
        },
        "f1_score": {
          "mean": 0.27853292463516743,
          "std": 0.06441887921717335,
          "min": 0.24144336041957373,
          "max": 0.4354941311253262,
          "median": 0.25720573009007575,
          "values": [
            0.24417745411887654,
            0.25993281414743463,
            0.25854952349008764,
            0.25720573009007575,
            0.24144336041957373,
            0.25292745905479747,
            0.4354941311253262
          ]
        },
        "jaccard": {
          "mean": 0.16595830383624627,
          "std": 0.048747983081423044,
          "min": 0.13860822528341799,
          "max": 0.2848766051944887,
          "median": 0.14951504201669757,
          "values": [
            0.1411079925248137,
            0.1512591168452178,
            0.15019295777727315,
            0.14951504201669757,
            0.13860822528341799,
            0.146148187211815,
            0.2848766051944887
          ]
        },
        "semantic_similarity": {
          "mean": 0.7165383051122939,
          "std": 0.006856157183888171,
          "min": 0.7053428494930267,
          "max": 0.7253133010864258,
          "median": 0.7190728631615638,
          "values": [
            0.7116961959004402,
            0.7224323785305024,
            0.7253133010864258,
            0.710413635969162,
            0.7214969116449356,
            0.7190728631615638,
            0.7053428494930267
          ]
        },
        "all_choices_addressed": {
          "mean": 0.8614285714285713,
          "std": 0.09201064712126235,
          "min": 0.674,
          "max": 0.98,
          "median": 0.866,
          "values": [
            0.9039999999999999,
            0.8480000000000001,
            0.9399999999999998,
            0.674,
            0.98,
            0.866,
            0.818
          ]
        },
        "single_paragraph_format": {
          "mean": 0.793142857142857,
          "std": 0.27024720353855314,
          "min": 0.23099999999999998,
          "max": 1.0,
          "median": 0.915,
          "values": [
            0.99,
            1.0,
            0.9940000000000001,
            0.915,
            0.861,
            0.561,
            0.23099999999999998
          ]
        },
        "explanation_depth": {
          "mean": 0.4056340803791992,
          "std": 0.14315678941726992,
          "min": 0.08033856265439387,
          "max": 0.5681,
          "median": 0.4403000000000001,
          "values": [
            0.4403000000000001,
            0.44260000000000005,
            0.4152,
            0.39519999999999994,
            0.5681,
            0.49770000000000003,
            0.08033856265439387
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10273162625675995,
          "std": 0.007485655950307634,
          "min": 0.0938173781271812,
          "max": 0.11732151619218938,
          "median": 0.101820374601925,
          "values": [
            0.10169878430134373,
            0.10194196490250626,
            0.10497446321938876,
            0.11732151619218938,
            0.0938173781271812,
            0.09663565079795029
          ]
        },
        "recall": {
          "mean": 0.3656216783771395,
          "std": 0.020827510119931014,
          "min": 0.3420798601238221,
          "max": 0.40929083684962725,
          "median": 0.3609451623214175,
          "values": [
            0.3557897897627871,
            0.40929083684962725,
            0.36126505710264967,
            0.3420798601238221,
            0.3646792588837652,
            0.36062526754018537
          ]
        },
        "f1_score": {
          "mean": 0.1566436843179111,
          "std": 0.007064938077349129,
          "min": 0.14688288237361224,
          "max": 0.16813189049462218,
          "median": 0.15728464312816545,
          "values": [
            0.15528852842993202,
            0.16056623958804195,
            0.1592807578263989,
            0.16813189049462218,
            0.14688288237361224,
            0.14971180719485916
          ]
        },
        "jaccard": {
          "mean": 0.08556707914121055,
          "std": 0.004300960507226748,
          "min": 0.07967973503882565,
          "max": 0.09268273828787235,
          "median": 0.0858961026074866,
          "values": [
            0.0847125591934511,
            0.08785645893139245,
            0.08707964602152211,
            0.09268273828787235,
            0.07967973503882565,
            0.08139133737419964
          ]
        },
        "semantic_similarity": {
          "mean": 0.43789088299808404,
          "std": 0.016227690452541663,
          "min": 0.42134753093123434,
          "max": 0.4699423840641975,
          "median": 0.432927991328761,
          "values": [
            0.4699423840641975,
            0.4261318787932396,
            0.43809329748153686,
            0.4277626851759851,
            0.4440675215423107,
            0.42134753093123434
          ]
        },
        "all_choices_addressed": {
          "mean": 0.25733333333333336,
          "std": 0.030147784145586696,
          "min": 0.20799999999999996,
          "max": 0.30999999999999994,
          "median": 0.259,
          "values": [
            0.30999999999999994,
            0.262,
            0.264,
            0.256,
            0.244,
            0.20799999999999996
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8666666666666667,
          "std": 0.29104562452570143,
          "min": 0.21599999999999997,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            0.9840000000000001,
            1.0,
            1.0,
            1.0,
            0.21599999999999997
          ]
        },
        "explanation_depth": {
          "mean": 0.25066666666666676,
          "std": 0.028449877953262915,
          "min": 0.21980000000000005,
          "max": 0.30060000000000003,
          "median": 0.24470000000000003,
          "values": [
            0.30060000000000003,
            0.26060000000000005,
            0.22700000000000006,
            0.22880000000000003,
            0.21980000000000005,
            0.26720000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_parent": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10180833537174412,
          "std": 0.01032352871655937,
          "min": 0.09313842837060643,
          "max": 0.11887180013745242,
          "median": 0.0956172191560225,
          "values": [
            0.09410443772872569,
            0.09517211936859704,
            0.11350090768163518,
            0.11887180013745242,
            0.09313842837060643,
            0.09606231894344798
          ]
        },
        "recall": {
          "mean": 0.3632581239752544,
          "std": 0.02824566298646,
          "min": 0.3312279727082102,
          "max": 0.42174494376373195,
          "median": 0.3595994726159424,
          "values": [
            0.3312279727082102,
            0.42174494376373195,
            0.3458292774510594,
            0.3577549998994147,
            0.36144394533247004,
            0.3615476046966404
          ]
        },
        "f1_score": {
          "mean": 0.1550740765845582,
          "std": 0.011112579978694928,
          "min": 0.14367045420676094,
          "max": 0.17490467140552698,
          "median": 0.15087669218997496,
          "values": [
            0.14367045420676094,
            0.15250814850198202,
            0.1644675873681548,
            0.17490467140552698,
            0.14564836214695653,
            0.14924523587796787
          ]
        },
        "jaccard": {
          "mean": 0.08466684813573278,
          "std": 0.006627731786684921,
          "min": 0.07790322868928407,
          "max": 0.09646742000800676,
          "median": 0.08213594024644862,
          "values": [
            0.07790322868928407,
            0.08310825444747387,
            0.09032989211945881,
            0.09646742000800676,
            0.07902866750474985,
            0.08116362604542336
          ]
        },
        "semantic_similarity": {
          "mean": 0.5262208588545522,
          "std": 0.013527151582657545,
          "min": 0.5113372254371643,
          "max": 0.5527061250805855,
          "median": 0.5240846387296916,
          "values": [
            0.5527061250805855,
            0.5269784726202488,
            0.5113372254371643,
            0.5211908048391343,
            0.530372180789709,
            0.5147403443604708
          ]
        },
        "all_choices_addressed": {
          "mean": 0.2823333333333334,
          "std": 0.057136289305095414,
          "min": 0.23200000000000004,
          "max": 0.3880000000000001,
          "median": 0.255,
          "values": [
            0.3880000000000001,
            0.3280000000000001,
            0.23200000000000004,
            0.264,
            0.236,
            0.24600000000000002
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8376666666666667,
          "std": 0.27811788068291393,
          "min": 0.22399999999999995,
          "max": 1.0,
          "median": 0.9609999999999999,
          "values": [
            1.0,
            0.88,
            1.0,
            0.997,
            0.9249999999999998,
            0.22399999999999995
          ]
        },
        "explanation_depth": {
          "mean": 0.25770000000000004,
          "std": 0.04533821052195745,
          "min": 0.1986,
          "max": 0.3159,
          "median": 0.24875000000000008,
          "values": [
            0.3159,
            0.31560000000000005,
            0.1986,
            0.23440000000000008,
            0.21860000000000002,
            0.26310000000000006
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10143653640650276,
          "std": 0.008590284124628447,
          "min": 0.09267020932367967,
          "max": 0.1144162082803993,
          "median": 0.09704074820310465,
          "values": [
            0.09740329080545494,
            0.09267020932367967,
            0.11232939067723872,
            0.1144162082803993,
            0.09667820560075438,
            0.09512191375148955
          ]
        },
        "recall": {
          "mean": 0.35797344928646807,
          "std": 0.018892650483012443,
          "min": 0.3387622980194823,
          "max": 0.39350521618497175,
          "median": 0.35406635995710667,
          "values": [
            0.3391893547719219,
            0.39350521618497175,
            0.3573079271995745,
            0.3387622980194823,
            0.36825110682821893,
            0.35082479271463884
          ]
        },
        "f1_score": {
          "mean": 0.15421093191078486,
          "std": 0.008652023720082055,
          "min": 0.14675153857318823,
          "max": 0.16756314639097386,
          "median": 0.1492749613871256,
          "values": [
            0.14808039843271317,
            0.14737658577530466,
            0.16502439795099125,
            0.16756314639097386,
            0.15046952434153799,
            0.14675153857318823
          ]
        },
        "jaccard": {
          "mean": 0.08412924056339881,
          "std": 0.0051490040661700855,
          "min": 0.07969752174909459,
          "max": 0.09209594482762284,
          "median": 0.08119338207886938,
          "values": [
            0.08049937836942817,
            0.08005199009215799,
            0.0905432225537786,
            0.09209594482762284,
            0.08188738578831059,
            0.07969752174909459
          ]
        },
        "semantic_similarity": {
          "mean": 0.5042471169804533,
          "std": 0.028965391222078668,
          "min": 0.4667537838220596,
          "max": 0.5446191549301147,
          "median": 0.5081539372727275,
          "values": [
            0.5446191549301147,
            0.4667537838220596,
            0.5277336171269417,
            0.4970189430564642,
            0.5192889314889908,
            0.47006827145814895
          ]
        },
        "all_choices_addressed": {
          "mean": 0.29433333333333334,
          "std": 0.06651733274534961,
          "min": 0.25,
          "max": 0.4420000000000001,
          "median": 0.268,
          "values": [
            0.4420000000000001,
            0.27599999999999997,
            0.25,
            0.27,
            0.262,
            0.266
          ]
        },
        "single_paragraph_format": {
          "mean": 0.8648333333333333,
          "std": 0.25551674222163123,
          "min": 0.29600000000000004,
          "max": 1.0,
          "median": 0.9745,
          "values": [
            1.0,
            0.944,
            1.0,
            1.0,
            0.9490000000000001,
            0.29600000000000004
          ]
        },
        "explanation_depth": {
          "mean": 0.3163500000000001,
          "std": 0.035018554605618236,
          "min": 0.26360000000000006,
          "max": 0.3622000000000001,
          "median": 0.31505000000000005,
          "values": [
            0.3622000000000001,
            0.3303000000000001,
            0.2998,
            0.26360000000000006,
            0.28980000000000006,
            0.35239999999999994
          ]
        }
      }
    },
    "ecqa_stakeholder_perspective": {
      "num_experiments": 3,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_perspective__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_perspective__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_perspective__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.11257790819717946,
          "std": 0.004104351589237897,
          "min": 0.10680542256256137,
          "max": 0.11599080094633628,
          "median": 0.11493750108264075,
          "values": [
            0.11599080094633628,
            0.10680542256256137,
            0.11493750108264075
          ]
        },
        "recall": {
          "mean": 0.3913944997902918,
          "std": 0.017623634849414815,
          "min": 0.376333911124093,
          "max": 0.4161228393532694,
          "median": 0.38172674889351305,
          "values": [
            0.4161228393532694,
            0.376333911124093,
            0.38172674889351305
          ]
        },
        "f1_score": {
          "mean": 0.1716167826437993,
          "std": 0.006141406100854089,
          "min": 0.16336908081235735,
          "max": 0.17809793375638397,
          "median": 0.17338333336265654,
          "values": [
            0.17809793375638397,
            0.16336908081235735,
            0.17338333336265654
          ]
        },
        "jaccard": {
          "mean": 0.09456216714827075,
          "std": 0.003627340800619381,
          "min": 0.08971715709668075,
          "max": 0.09844440164412212,
          "median": 0.09552494270400932,
          "values": [
            0.09844440164412212,
            0.08971715709668075,
            0.09552494270400932
          ]
        },
        "semantic_similarity": {
          "mean": 0.5598436073958873,
          "std": 0.01351761478058474,
          "min": 0.5412483015656471,
          "max": 0.572981809079647,
          "median": 0.5653007115423679,
          "values": [
            0.5653007115423679,
            0.572981809079647,
            0.5412483015656471
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3293333333333334,
          "std": 0.07252279335185294,
          "min": 0.262,
          "max": 0.43000000000000016,
          "median": 0.29600000000000004,
          "values": [
            0.43000000000000016,
            0.262,
            0.29600000000000004
          ]
        },
        "single_paragraph_format": {
          "mean": 0.895,
          "std": 0.1213122692338523,
          "min": 0.725,
          "max": 1.0,
          "median": 0.96,
          "values": [
            1.0,
            0.96,
            0.725
          ]
        },
        "explanation_depth": {
          "mean": 0.3416333333333334,
          "std": 0.013945688302203746,
          "min": 0.324,
          "max": 0.35810000000000003,
          "median": 0.3428,
          "values": [
            0.35810000000000003,
            0.324,
            0.3428
          ]
        }
      }
    },
    "ecqa_negative_explanation": {
      "num_experiments": 7,
      "experiments": [
        "ecqa_negative__hf-mistral-nemo-12b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__hf-qwen2.5-3b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__u4b-llama3-8b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__u4b-llama3.2-1b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__u4b-llama3.3-70b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__u4b-mistral-7b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_negative__u4b-mistral-7b_ft_ecqa_negative_ft__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.209083248851498,
          "std": 0.06263145684095225,
          "min": 0.14725606766815846,
          "max": 0.34900191824625354,
          "median": 0.1900079193383912,
          "values": [
            0.1814849448190401,
            0.2360652492606102,
            0.1640683464500342,
            0.14725606766815846,
            0.1900079193383912,
            0.19569829617799822,
            0.34900191824625354
          ]
        },
        "recall": {
          "mean": 0.4282548077379316,
          "std": 0.0464942219326778,
          "min": 0.3443703285419432,
          "max": 0.4785876572797395,
          "median": 0.44029410367759925,
          "values": [
            0.3443703285419432,
            0.44029410367759925,
            0.46629159451760577,
            0.3874905023614697,
            0.4785876572797395,
            0.4721137596409772,
            0.40863570814618677
          ]
        },
        "f1_score": {
          "mean": 0.25654918656973696,
          "std": 0.04440830322839484,
          "min": 0.1973631081199648,
          "max": 0.34303866741771744,
          "median": 0.2540873034091251,
          "values": [
            0.22457098102242015,
            0.2880224527919178,
            0.22847661977633685,
            0.1973631081199648,
            0.2540873034091251,
            0.2602851734506764,
            0.34303866741771744
          ]
        },
        "jaccard": {
          "mean": 0.15091034535097517,
          "std": 0.030593051321285196,
          "min": 0.1128273650124004,
          "max": 0.21255279965018897,
          "median": 0.14906515089190975,
          "values": [
            0.12835014809217857,
            0.17076389996907515,
            0.13105611733710348,
            0.1128273650124004,
            0.14906515089190975,
            0.15175693650396982,
            0.21255279965018897
          ]
        },
        "semantic_similarity": {
          "mean": 0.646120068279228,
          "std": 0.06072714063302582,
          "min": 0.5594649736583233,
          "max": 0.7280371448397637,
          "median": 0.6521517732739448,
          "values": [
            0.6748887285590172,
            0.7280371448397637,
            0.6311278274655342,
            0.566499483268708,
            0.5594649736583233,
            0.7106705468893051,
            0.6521517732739448
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.8784761904761904,
          "std": 0.11165356812201037,
          "min": 0.6975,
          "max": 0.9631666666666667,
          "median": 0.9475,
          "values": [
            0.9631666666666666,
            0.6975,
            0.9576666666666667,
            0.7106666666666668,
            0.9475,
            0.9631666666666667,
            0.9096666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7284376135022647,
          "std": 0.11338153252924395,
          "min": 0.509588888888889,
          "max": 0.9057197691884995,
          "median": 0.7465775691585677,
          "values": [
            0.7615942307692307,
            0.7048857142857142,
            0.509588888888889,
            0.7465775691585677,
            0.9057197691884995,
            0.6698304612054612,
            0.8008666610194901
          ]
        },
        "external_knowledge_usage": {
          "mean": 0.9997343869878778,
          "std": 0.0006506163487429124,
          "min": 0.9981407089151451,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.9981407089151451
          ]
        }
      }
    },
    "ecqa_positive_explanation": {
      "num_experiments": 7,
      "experiments": [
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-mistral-7b_ft_ecqa_positive_ft__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.2824366798757553,
          "std": 0.04291560270076549,
          "min": 0.22993602607248176,
          "max": 0.3746907428025377,
          "median": 0.28283361234475163,
          "values": [
            0.3746907428025377,
            0.28283361234475163,
            0.264387653966471,
            0.22993602607248176,
            0.28863006709410444,
            0.247546451733437,
            0.2890322051165036
          ]
        },
        "recall": {
          "mean": 0.36926287713106876,
          "std": 0.06793213675322739,
          "min": 0.26725973729837293,
          "max": 0.5112916105060541,
          "median": 0.3577853375793798,
          "values": [
            0.26725973729837293,
            0.33624103447413994,
            0.3577853375793798,
            0.3856810967414349,
            0.37233378418672053,
            0.35424753913137935,
            0.5112916105060541
          ]
        },
        "f1_score": {
          "mean": 0.29415251841890566,
          "std": 0.02315767195864518,
          "min": 0.2702972475885435,
          "max": 0.3469011914995081,
          "median": 0.2917493179502968,
          "values": [
            0.2917493179502968,
            0.29677117818000276,
            0.29290683675032814,
            0.2702972475885435,
            0.2804271626322052,
            0.28001469433145515,
            0.3469011914995081
          ]
        },
        "jaccard": {
          "mean": 0.18067513917439412,
          "std": 0.016375895243185803,
          "min": 0.16312343186193573,
          "max": 0.21822543738909142,
          "median": 0.17771054772563183,
          "values": [
            0.18023991309653273,
            0.1812543356959735,
            0.17771054772563183,
            0.16312343186193573,
            0.17314617116284642,
            0.17102613728874708,
            0.21822543738909142
          ]
        },
        "semantic_similarity": {
          "mean": 0.6670997865498067,
          "std": 0.03318285979809009,
          "min": 0.6046197773516178,
          "max": 0.7155673375725746,
          "median": 0.6743487492203712,
          "values": [
            0.6730493840575218,
            0.7155673375725746,
            0.6796818909049034,
            0.6743487492203712,
            0.6046197773516178,
            0.685083657503128,
            0.6373477092385292
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.5602737223611074,
          "std": 0.22501543913421648,
          "min": 0.24783333333333327,
          "max": 0.9223757869933645,
          "median": 0.4531,
          "values": [
            0.7314999999999999,
            0.4531,
            0.24783333333333327,
            0.4222416666666667,
            0.7534486028677206,
            0.39141666666666663,
            0.9223757869933645
          ]
        },
        "correct_answer_focus": {
          "mean": 0.7960000000000002,
          "std": 0.09482916971360361,
          "min": 0.685,
          "max": 0.922,
          "median": 0.7935,
          "values": [
            0.685,
            0.8645,
            0.9045000000000001,
            0.922,
            0.6884999999999999,
            0.714,
            0.7935
          ]
        },
        "concise_justification": {
          "mean": 0.8119857142857142,
          "std": 0.1326412177758528,
          "min": 0.5112000000000001,
          "max": 0.9618999999999999,
          "median": 0.8468999999999999,
          "values": [
            0.9618999999999999,
            0.8898999999999999,
            0.8417999999999998,
            0.8477,
            0.7844999999999998,
            0.8468999999999999,
            0.5112000000000001
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted": {
      "num_experiments": 6,
      "experiments": [
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.20116302387088672,
          "std": 0.029920140784120332,
          "min": 0.15928865884265464,
          "max": 0.24300644826220846,
          "median": 0.20001211164857025,
          "values": [
            0.24300644826220846,
            0.22713080245473005,
            0.1775280103685869,
            0.18224464488739073,
            0.21777957840974974,
            0.15928865884265464
          ]
        },
        "recall": {
          "mean": 0.38671842648634674,
          "std": 0.05524161449216581,
          "min": 0.2886052257904705,
          "max": 0.43983052902111197,
          "median": 0.4064830053465923,
          "values": [
            0.2886052257904705,
            0.33912661300515906,
            0.43983052902111197,
            0.40707478148907894,
            0.40589122920410575,
            0.43978218040815426
          ]
        },
        "f1_score": {
          "mean": 0.24827995222291976,
          "std": 0.015161523564739644,
          "min": 0.22667320491504128,
          "max": 0.2735406292840013,
          "median": 0.24567884533602644,
          "values": [
            0.24871530612969905,
            0.2600337834804731,
            0.24264238454235382,
            0.23807440498595006,
            0.2735406292840013,
            0.22667320491504128
          ]
        },
        "jaccard": {
          "mean": 0.1458952280841877,
          "std": 0.0104581812726702,
          "min": 0.13051938039109187,
          "max": 0.16312967489327948,
          "median": 0.14442558402772188,
          "values": [
            0.1476088298107717,
            0.15342230017879868,
            0.14124233824467206,
            0.13944884498651247,
            0.16312967489327948,
            0.13051938039109187
          ]
        },
        "semantic_similarity": {
          "mean": 0.7014170431842407,
          "std": 0.016200360833817908,
          "min": 0.6837376776337624,
          "max": 0.7356852543354034,
          "median": 0.6960731824487447,
          "values": [
            0.6961110651493072,
            0.7356852543354034,
            0.7010463732481003,
            0.6837376776337624,
            0.6960352997481823,
            0.6958865889906883
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.8224266085766087,
          "std": 0.13830060154149487,
          "min": 0.6207083333333334,
          "max": 0.9596666666666667,
          "median": 0.8784631590631592,
          "values": [
            0.9596666666666667,
            0.9494166666666668,
            0.6478416666666666,
            0.6207083333333334,
            0.9127463578088579,
            0.8441799603174605
          ]
        },
        "correct_answer_focus": {
          "mean": 0.5997500000000001,
          "std": 0.11315209159946334,
          "min": 0.4335,
          "max": 0.7440000000000001,
          "median": 0.64625,
          "values": [
            0.4605,
            0.6680000000000001,
            0.7440000000000001,
            0.6459999999999999,
            0.6465000000000001,
            0.4335
          ]
        },
        "concise_justification": {
          "mean": 0.8608666666666668,
          "std": 0.09218314500071152,
          "min": 0.6778000000000003,
          "max": 0.9515,
          "median": 0.87305,
          "values": [
            0.9515,
            0.9486,
            0.8521,
            0.8412,
            0.894,
            0.6778000000000003
          ]
        }
      }
    },
    "gmeg_few_shot": {
      "num_experiments": 6,
      "experiments": [
        "gmeg__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3-8b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-mistral-7b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.11617798022018971,
          "std": 0.01537175497064551,
          "min": 0.08999826368648688,
          "max": 0.13734193549503154,
          "median": 0.11790826618541211,
          "values": [
            0.12003115342841778,
            0.13734193549503154,
            0.10537446616922469,
            0.08999826368648688,
            0.1285366835995709,
            0.11578537894240647
          ]
        },
        "recall": {
          "mean": 0.3498098232743429,
          "std": 0.04090700795521072,
          "min": 0.27067536678929693,
          "max": 0.39691623353839034,
          "median": 0.3606314319806626,
          "values": [
            0.3778653268817748,
            0.3500143408670139,
            0.3321391484752698,
            0.27067536678929693,
            0.3712485230943114,
            0.39691623353839034
          ]
        },
        "f1_score": {
          "mean": 0.1654081340674688,
          "std": 0.019993616693385115,
          "min": 0.12721664086706144,
          "max": 0.185488221032632,
          "median": 0.17197023795409455,
          "values": [
            0.17316757229496202,
            0.185488221032632,
            0.1531497419523496,
            0.12721664086706144,
            0.1826537246445808,
            0.1707729036132271
          ]
        },
        "jaccard": {
          "mean": 0.09277303637215438,
          "std": 0.012301135842783854,
          "min": 0.0695525443532205,
          "max": 0.10520710932164107,
          "median": 0.09669052955219373,
          "values": [
            0.09716132290198534,
            0.10520710932164107,
            0.0847984098093961,
            0.0695525443532205,
            0.10369909564428108,
            0.09621973620240214
          ]
        },
        "semantic_similarity": {
          "mean": 0.5089311127574183,
          "std": 0.044120972997017865,
          "min": 0.42756740694865586,
          "max": 0.5686166930478066,
          "median": 0.5183886969834566,
          "values": [
            0.5275601665768772,
            0.5092172273900359,
            0.5342562747793272,
            0.42756740694865586,
            0.5686166930478066,
            0.4863689078018069
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5078611111111109,
          "std": 0.22285428341362978,
          "min": 1.0806666666666667,
          "max": 1.745333333333333,
          "median": 1.5260833333333332,
          "values": [
            1.4388333333333332,
            1.5721666666666665,
            1.0806666666666667,
            1.745333333333333,
            1.48,
            1.7301666666666664
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7273611111111111,
          "std": 0.064423038830055,
          "min": 0.6316666666666666,
          "max": 0.8341666666666667,
          "median": 0.7362500000000001,
          "values": [
            0.7558333333333335,
            0.6699999999999998,
            0.7300000000000001,
            0.6316666666666666,
            0.7425,
            0.8341666666666667
          ]
        },
        "structural_format_match": {
          "mean": 0.9083333333333333,
          "std": 0.0037267799624996533,
          "min": 0.9,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.9,
            0.91,
            0.91,
            0.91,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        }
      }
    },
    "gmeg_explaination": {
      "num_experiments": 9,
      "experiments": [
        "gmeg__hf-mistral-nemo-12b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__hf-qwen2.5-3b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__hf-qwen3-30b-thinking__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__u4b-llama3-8b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__u4b-llama3.3-70b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__u4b-mistral-7b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg__u4b-mistral-7b_ft_gmeg_explanation_ft__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_paper__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg_paper",
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.12333954363730275,
          "std": 0.07315330313934729,
          "min": 0.06619874367276073,
          "max": 0.3252198118008294,
          "median": 0.10478612299373015,
          "values": [
            0.11652862486942757,
            0.12051848220273391,
            0.06619874367276073,
            0.10478612299373015,
            0.08418611247134875,
            0.10859725201986996,
            0.09588343999711746,
            0.3252198118008294,
            0.0881373027079068
          ]
        },
        "recall": {
          "mean": 0.34411493417041705,
          "std": 0.05172359379661743,
          "min": 0.2586684277600881,
          "max": 0.43873223702430225,
          "median": 0.3467005139055157,
          "values": [
            0.380493739585753,
            0.3467005139055157,
            0.43873223702430225,
            0.33908495498513547,
            0.2586684277600881,
            0.34937610571185396,
            0.38020979066697336,
            0.3299169805016207,
            0.27385165739251116
          ]
        },
        "f1_score": {
          "mean": 0.16350817350361663,
          "std": 0.05672870579540304,
          "min": 0.11161980964318186,
          "max": 0.31348241920638736,
          "median": 0.15450160147144312,
          "values": [
            0.16958565690582078,
            0.17079180141365133,
            0.11161980964318186,
            0.15450160147144312,
            0.12029279523579862,
            0.15933894110014646,
            0.1461540075876452,
            0.31348241920638736,
            0.12580652896847477
          ]
        },
        "jaccard": {
          "mean": 0.09302744790568082,
          "std": 0.03861373635835605,
          "min": 0.06007680550816749,
          "max": 0.19661618599278566,
          "median": 0.0857823976586241,
          "values": [
            0.09533744963654023,
            0.09600669797746281,
            0.06007680550816749,
            0.0857823976586241,
            0.0652950164091048,
            0.08878555714359121,
            0.08080910737350303,
            0.19661618599278566,
            0.0685378134513481
          ]
        },
        "semantic_similarity": {
          "mean": 0.5158756490258707,
          "std": 0.05781395749661112,
          "min": 0.4586498977150768,
          "max": 0.6538133403286338,
          "median": 0.5151659706793725,
          "values": [
            0.5151659706793725,
            0.5254467269033194,
            0.4586498977150768,
            0.5267234312975779,
            0.4610796781210229,
            0.5469943140354008,
            0.49559973377734423,
            0.6538133403286338,
            0.45940774837508797
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.3395000000000001,
          "std": 0.27952698140489646,
          "min": 0.8035,
          "max": 1.7113333333333336,
          "median": 1.3648333333333333,
          "values": [
            1.4663333333333333,
            1.7113333333333336,
            0.8035,
            1.2633333333333332,
            1.6381666666666665,
            1.4945,
            1.2266666666666666,
            1.1121666666666667
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7196874999999999,
          "std": 0.09715120794339673,
          "min": 0.5225,
          "max": 0.8666666666666667,
          "median": 0.7166666666666666,
          "values": [
            0.8008333333333333,
            0.65,
            0.8666666666666667,
            0.7216666666666667,
            0.5225,
            0.7116666666666666,
            0.7066666666666666,
            0.7775
          ]
        },
        "structural_format_match": {
          "mean": 0.8300000000000001,
          "std": 0.14568802284333465,
          "min": 0.5,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.5,
            0.91,
            0.91,
            0.91,
            0.68,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_original": {
      "num_experiments": 6,
      "experiments": [
        "gmeg_ours__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3-8b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-mistral-7b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10891955577353281,
          "std": 0.007954981227399368,
          "min": 0.09777369094408789,
          "max": 0.11629653123470433,
          "median": 0.11279182781120001,
          "values": [
            0.11534583487662842,
            0.0982685163310635,
            0.11629653123470433,
            0.09777369094408789,
            0.1155949405089411,
            0.1102378207457716
          ]
        },
        "recall": {
          "mean": 0.346864830469425,
          "std": 0.039821118273789334,
          "min": 0.2784053754524449,
          "max": 0.41224016301355076,
          "median": 0.35081983307616493,
          "values": [
            0.3602410515207706,
            0.3532869886921498,
            0.3286627266774538,
            0.2784053754524449,
            0.41224016301355076,
            0.34835267746018006
          ]
        },
        "f1_score": {
          "mean": 0.15760973091188704,
          "std": 0.010985160709028989,
          "min": 0.1382949847372993,
          "max": 0.1685442323280916,
          "median": 0.16211649993295013,
          "values": [
            0.1685442323280916,
            0.14787739182203263,
            0.1646939794235943,
            0.1382949847372993,
            0.16670877671799855,
            0.15953902044230595
          ]
        },
        "jaccard": {
          "mean": 0.08776374594617108,
          "std": 0.006889386932757759,
          "min": 0.07593057669509175,
          "max": 0.09463171337853234,
          "median": 0.09040715129781815,
          "values": [
            0.09463171337853234,
            0.08129240193485901,
            0.09180649463146945,
            0.07593057669509175,
            0.09391348107290702,
            0.08900780796416687
          ]
        },
        "semantic_similarity": {
          "mean": 0.47781414571606246,
          "std": 0.0497706928570592,
          "min": 0.3811633627023548,
          "max": 0.5408527564443648,
          "median": 0.492250216032844,
          "values": [
            0.49668813809752466,
            0.48781229396816345,
            0.5034227759391069,
            0.3811633627023548,
            0.5408527564443648,
            0.45694554714486
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.1953055555555554,
          "std": 0.2778102439360408,
          "min": 0.8246666666666667,
          "max": 1.5923333333333334,
          "median": 1.2392499999999997,
          "values": [
            1.4028333333333336,
            1.1455,
            0.8246666666666667,
            1.3329999999999997,
            0.8734999999999999,
            1.5923333333333334
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6634722222222221,
          "std": 0.06257480091759415,
          "min": 0.565,
          "max": 0.7616666666666666,
          "median": 0.6633333333333334,
          "values": [
            0.7058333333333333,
            0.6216666666666666,
            0.6458333333333335,
            0.565,
            0.7616666666666666,
            0.6808333333333334
          ]
        },
        "structural_format_match": {
          "mean": 0.8650000000000001,
          "std": 0.09622023349240708,
          "min": 0.65,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.91,
            0.91,
            0.65,
            0.9
          ]
        },
        "original_text_mention": {
          "mean": 0.8817370321361621,
          "std": 0.1396698551524306,
          "min": 0.6453567466202336,
          "max": 0.9939732620320855,
          "median": 0.966924323197156,
          "values": [
            0.9939732620320855,
            0.9859064736433157,
            0.9840740740740741,
            0.949774572320238,
            0.7313370641270258,
            0.6453567466202336
          ]
        }
      }
    },
    "gmeg_basic": {
      "num_experiments": 6,
      "experiments": [
        "gmeg_ours__hf-mistral-nemo-12b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_ours__hf-qwen2.5-3b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_ours__u4b-llama3-8b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_ours__u4b-llama3.2-1b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_ours__u4b-llama3.3-70b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_ours__u4b-mistral-7b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.13530476548440135,
          "std": 0.03125299847188406,
          "min": 0.09094990802899909,
          "max": 0.18316898454422095,
          "median": 0.12912615767572772,
          "values": [
            0.16653098620827958,
            0.18316898454422095,
            0.13412336132142902,
            0.09094990802899909,
            0.1241289540300264,
            0.11292639877345302
          ]
        },
        "recall": {
          "mean": 0.2692059771866642,
          "std": 0.04060774495643405,
          "min": 0.19459645234727632,
          "max": 0.3172650134685309,
          "median": 0.2806535953757645,
          "values": [
            0.2932858193706418,
            0.24429637255947415,
            0.26802137138088716,
            0.19459645234727632,
            0.3172650134685309,
            0.2977708339931752
          ]
        },
        "f1_score": {
          "mean": 0.16464816779681146,
          "std": 0.027210731225145775,
          "min": 0.11468630378624185,
          "max": 0.19483243416739132,
          "median": 0.16449650247650907,
          "values": [
            0.1945179387881015,
            0.19483243416739132,
            0.16935422342754966,
            0.11468630378624185,
            0.15963878152546848,
            0.1548593250861159
          ]
        },
        "jaccard": {
          "mean": 0.09288204877014224,
          "std": 0.01686138134028199,
          "min": 0.06226144043722152,
          "max": 0.11225558852135702,
          "median": 0.09271999358590124,
          "values": [
            0.11120084179688913,
            0.11225558852135702,
            0.09514077750810275,
            0.06226144043722152,
            0.09029920966369975,
            0.0861344346935833
          ]
        },
        "semantic_similarity": {
          "mean": 0.47956315261428245,
          "std": 0.0302439814430936,
          "min": 0.4205879895738326,
          "max": 0.5141218559071422,
          "median": 0.48195235589984803,
          "values": [
            0.5141218559071422,
            0.4819990609586239,
            0.5068688761256636,
            0.4205879895738326,
            0.4718954822793603,
            0.4819056508410722
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.2239444444444445,
          "std": 0.42519028057019126,
          "min": 0.505,
          "max": 1.6651666666666665,
          "median": 1.4409166666666668,
          "values": [
            1.3773333333333335,
            1.5055,
            0.7861666666666668,
            1.6651666666666665,
            1.5045,
            0.505
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7858333333333333,
          "std": 0.0390838367652959,
          "min": 0.7316666666666666,
          "max": 0.8383333333333333,
          "median": 0.7866666666666666,
          "values": [
            0.8275,
            0.7441666666666668,
            0.7316666666666666,
            0.7875,
            0.8383333333333333,
            0.7858333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.805,
          "std": 0.22596091107386992,
          "min": 0.3,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.91,
            0.91,
            0.89,
            0.3
          ]
        },
        "original_text_mention": {
          "mean": 0.4616498360685539,
          "std": 0.13915547830780817,
          "min": 0.2660764189742175,
          "max": 0.7313372442761521,
          "median": 0.4352876420236153,
          "values": [
            0.4102483278436294,
            0.42361581295930323,
            0.2660764189742175,
            0.7313372442761521,
            0.4469594710879274,
            0.491661741270094
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.08833333333333333,
          "std": 0.11051646433400271,
          "min": 0.0,
          "max": 0.32,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.32,
            0.04,
            0.0,
            0.12
          ]
        },
        "precision": {
          "mean": 0.08842143285990467,
          "std": 0.11052235595485278,
          "min": 0.00010752688172043012,
          "max": 0.32,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.32,
            0.04,
            0.00010752688172043012,
            0.12042107027770758
          ]
        },
        "recall": {
          "mean": 0.09833333333333333,
          "std": 0.11305111921407748,
          "min": 0.01,
          "max": 0.32,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.32,
            0.04,
            0.01,
            0.17
          ]
        },
        "f1_score": {
          "mean": 0.08850797652047333,
          "std": 0.11052842286274148,
          "min": 0.00021276595744680854,
          "max": 0.32,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.32,
            0.04,
            0.00021276595744680854,
            0.1208350931653932
          ]
        },
        "jaccard": {
          "mean": 0.08842143285990467,
          "std": 0.11052235595485278,
          "min": 0.00010752688172043012,
          "max": 0.32,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.32,
            0.04,
            0.00010752688172043012,
            0.12042107027770758
          ]
        },
        "semantic_similarity": {
          "mean": 0.4712942487502005,
          "std": 0.17816963389844398,
          "min": 0.1547814832930453,
          "max": 0.7084457975625992,
          "median": 0.5223787283152341,
          "values": [
            0.47510350450873373,
            0.5696539521217346,
            0.7084457975625992,
            0.5696539521217346,
            0.3501268028933555,
            0.1547814832930453
          ]
        },
        "answer_correctness": {
          "mean": 0.19166666666666665,
          "std": 0.1720868643705524,
          "min": 0.04,
          "max": 0.5,
          "median": 0.125,
          "values": [
            0.05,
            0.04,
            0.32,
            0.04,
            0.5,
            0.2
          ]
        },
        "strict_format_compliance": {
          "mean": 0.5595,
          "std": 0.39879223914213774,
          "min": 0.030999999999999996,
          "max": 1.0,
          "median": 0.597,
          "values": [
            0.3719999999999999,
            1.0,
            0.8220000000000001,
            1.0,
            0.030999999999999996,
            0.13200000000000003
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.7269999999999999,
          "std": 0.34804932600614724,
          "min": 0.18,
          "max": 1.0,
          "median": 0.94,
          "values": [
            0.88,
            1.0,
            1.0,
            1.0,
            0.30199999999999994,
            0.18
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.09500000000000001,
          "std": 0.15510748961069976,
          "min": 0.0,
          "max": 0.44,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.44,
            0.04,
            0.0,
            0.04
          ]
        },
        "precision": {
          "mean": 0.09575284928195221,
          "std": 0.15467617071966686,
          "min": 0.003968253968253968,
          "max": 0.44,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.44,
            0.04,
            0.003968253968253968,
            0.04054884172345928
          ]
        },
        "recall": {
          "mean": 0.10833333333333334,
          "std": 0.15026827860714834,
          "min": 0.01,
          "max": 0.44,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.44,
            0.04,
            0.03,
            0.09
          ]
        },
        "f1_score": {
          "mean": 0.0963474484354654,
          "std": 0.15434807356674982,
          "min": 0.006999999999999999,
          "max": 0.44,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.44,
            0.04,
            0.006999999999999999,
            0.041084690612792414
          ]
        },
        "jaccard": {
          "mean": 0.09575284928195221,
          "std": 0.15467617071966686,
          "min": 0.003968253968253968,
          "max": 0.44,
          "median": 0.04,
          "values": [
            0.01,
            0.04,
            0.44,
            0.04,
            0.003968253968253968,
            0.04054884172345928
          ]
        },
        "semantic_similarity": {
          "mean": 0.5097260088093268,
          "std": 0.16942815908150954,
          "min": 0.21572590524796398,
          "max": 0.7666562902927399,
          "median": 0.552862131781876,
          "values": [
            0.5360703114420176,
            0.5696539521217346,
            0.7666562902927399,
            0.5696539521217346,
            0.4005956416297704,
            0.21572590524796398
          ]
        },
        "answer_correctness": {
          "mean": 0.24333333333333337,
          "std": 0.23106035767497826,
          "min": 0.04,
          "max": 0.64,
          "median": 0.15,
          "values": [
            0.04,
            0.04,
            0.44,
            0.04,
            0.64,
            0.26
          ]
        },
        "strict_format_compliance": {
          "mean": 0.594,
          "std": 0.3762623198425977,
          "min": 0.102,
          "max": 1.0,
          "median": 0.6749999999999999,
          "values": [
            0.5479999999999999,
            1.0,
            0.8019999999999999,
            1.0,
            0.102,
            0.11200000000000002
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.7653333333333333,
          "std": 0.3316658291446712,
          "min": 0.22,
          "max": 1.0,
          "median": 0.995,
          "values": [
            0.99,
            1.0,
            1.0,
            1.0,
            0.38199999999999995,
            0.22
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.13999999999999999,
          "std": 0.22037846234754127,
          "min": 0.0,
          "max": 0.63,
          "median": 0.045,
          "values": [
            0.05,
            0.04,
            0.63,
            0.04,
            0.0,
            0.08
          ]
        },
        "precision": {
          "mean": 0.14104740633441304,
          "std": 0.21974722351984327,
          "min": 0.00588113777212776,
          "max": 0.63,
          "median": 0.045,
          "values": [
            0.05,
            0.04,
            0.63,
            0.04,
            0.00588113777212776,
            0.08040330023435047
          ]
        },
        "recall": {
          "mean": 0.155,
          "std": 0.2137560915311343,
          "min": 0.04,
          "max": 0.63,
          "median": 0.055,
          "values": [
            0.05,
            0.04,
            0.63,
            0.04,
            0.06,
            0.11
          ]
        },
        "f1_score": {
          "mean": 0.14185446717018,
          "std": 0.2192785749686534,
          "min": 0.01033094017094017,
          "max": 0.63,
          "median": 0.045,
          "values": [
            0.05,
            0.04,
            0.63,
            0.04,
            0.01033094017094017,
            0.08079586285013975
          ]
        },
        "jaccard": {
          "mean": 0.14104740633441304,
          "std": 0.21974722351984327,
          "min": 0.00588113777212776,
          "max": 0.63,
          "median": 0.045,
          "values": [
            0.05,
            0.04,
            0.63,
            0.04,
            0.00588113777212776,
            0.08040330023435047
          ]
        },
        "semantic_similarity": {
          "mean": 0.5551766259595752,
          "std": 0.16027091432844273,
          "min": 0.34886065615341066,
          "max": 0.8586994200944901,
          "median": 0.5680412434786558,
          "values": [
            0.566428534835577,
            0.5696539521217346,
            0.8586994200944901,
            0.5696539521217346,
            0.41776324043050406,
            0.34886065615341066
          ]
        },
        "answer_correctness": {
          "mean": 0.2866666666666667,
          "std": 0.2559730888633065,
          "min": 0.04,
          "max": 0.63,
          "median": 0.205,
          "values": [
            0.06,
            0.04,
            0.63,
            0.04,
            0.6,
            0.35
          ]
        },
        "strict_format_compliance": {
          "mean": 0.6608333333333333,
          "std": 0.34239665918671314,
          "min": 0.16899999999999998,
          "max": 1.0,
          "median": 0.7899999999999998,
          "values": [
            0.7799999999999998,
            1.0,
            0.7999999999999998,
            1.0,
            0.16899999999999998,
            0.21599999999999997
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.8036666666666666,
          "std": 0.27171165271703424,
          "min": 0.38,
          "max": 1.0,
          "median": 0.99,
          "values": [
            0.98,
            1.0,
            1.0,
            1.0,
            0.462,
            0.38
          ]
        }
      }
    },
    "pubmedqa_reasoning_free": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.40666666666666673,
          "std": 0.3346474098045417,
          "min": 0.0,
          "max": 0.8,
          "median": 0.41500000000000004,
          "values": [
            0.73,
            0.15,
            0.8,
            0.08,
            0.0,
            0.68
          ]
        },
        "precision": {
          "mean": 0.4084325396825397,
          "std": 0.33264233342911426,
          "min": 0.009107142857142855,
          "max": 0.8,
          "median": 0.4151190476190476,
          "values": [
            0.73,
            0.15,
            0.8,
            0.08125,
            0.009107142857142855,
            0.6802380952380952
          ]
        },
        "recall": {
          "mean": 0.4200000000000001,
          "std": 0.32269696827415867,
          "min": 0.06,
          "max": 0.8,
          "median": 0.42,
          "values": [
            0.73,
            0.15,
            0.8,
            0.09,
            0.06,
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.4097468315491571,
          "std": 0.33118170035943517,
          "min": 0.01579365079365079,
          "max": 0.8,
          "median": 0.4152325581395349,
          "values": [
            0.73,
            0.15,
            0.8,
            0.08222222222222221,
            0.01579365079365079,
            0.6804651162790698
          ]
        },
        "jaccard": {
          "mean": 0.4084325396825397,
          "std": 0.33264233342911426,
          "min": 0.009107142857142855,
          "max": 0.8,
          "median": 0.4151190476190476,
          "values": [
            0.73,
            0.15,
            0.8,
            0.08125,
            0.009107142857142855,
            0.6802380952380952
          ]
        },
        "semantic_similarity": {
          "mean": 0.7737062723748386,
          "std": 0.13569203973631436,
          "min": 0.5798214510083198,
          "max": 0.933139499425888,
          "median": 0.8076804139651358,
          "values": [
            0.8938113880157471,
            0.6201044678688049,
            0.933139499425888,
            0.5798214510083198,
            0.7479457410424948,
            0.8674150868877768
          ]
        },
        "answer_correctness": {
          "mean": 0.5650000000000001,
          "std": 0.32055940687075984,
          "min": 0.09,
          "max": 0.9,
          "median": 0.725,
          "values": [
            0.73,
            0.15,
            0.8,
            0.09,
            0.9,
            0.72
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7709999999999999,
          "std": 0.2094365456807065,
          "min": 0.34600000000000003,
          "max": 1.0,
          "median": 0.8019999999999998,
          "values": [
            0.7999999999999998,
            1.0,
            0.8039999999999998,
            0.9359999999999999,
            0.34600000000000003,
            0.7399999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.9550000000000001,
          "std": 0.06439202849214591,
          "min": 0.8180000000000001,
          "max": 1.0,
          "median": 0.98,
          "values": [
            1.0,
            1.0,
            1.0,
            0.96,
            0.8180000000000001,
            0.9519999999999998
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.125,
          "std": 0.22640303295966097,
          "min": 0.0,
          "max": 0.62,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.13,
            0.62,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.13697379845544924,
          "std": 0.23176847214761687,
          "min": 0.0004881729019660054,
          "max": 0.645,
          "median": 0.022762522757732044,
          "values": [
            0.0008295723152653063,
            0.0004881729019660054,
            0.13,
            0.645,
            0.044075493950493955,
            0.0014495515649701357
          ]
        },
        "recall": {
          "mean": 0.21,
          "std": 0.227522892621087,
          "min": 0.03,
          "max": 0.67,
          "median": 0.095,
          "values": [
            0.06,
            0.03,
            0.13,
            0.67,
            0.32,
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.14407532671334236,
          "std": 0.23271866392435295,
          "min": 0.0009590556730091613,
          "max": 0.6533333333333333,
          "median": 0.03926232998872632,
          "values": [
            0.001634911296259006,
            0.0009590556730091613,
            0.13,
            0.6533333333333333,
            0.07575177749270576,
            0.0027728824847468914
          ]
        },
        "jaccard": {
          "mean": 0.13697379845544924,
          "std": 0.23176847214761687,
          "min": 0.0004881729019660054,
          "max": 0.645,
          "median": 0.022762522757732044,
          "values": [
            0.0008295723152653063,
            0.0004881729019660054,
            0.13,
            0.645,
            0.044075493950493955,
            0.0014495515649701357
          ]
        },
        "semantic_similarity": {
          "mean": 0.41451590856032755,
          "std": 0.3531343776742253,
          "min": 0.045420683547854425,
          "max": 0.868618765771389,
          "median": 0.3160068358166609,
          "values": [
            0.0827535550808534,
            0.11679093430982902,
            0.868618765771389,
            0.8582887753285467,
            0.5152227373234928,
            0.045420683547854425
          ]
        },
        "answer_correctness": {
          "mean": 0.6366666666666666,
          "std": 0.2328566559543064,
          "min": 0.19,
          "max": 0.87,
          "median": 0.7,
          "values": [
            0.19,
            0.51,
            0.85,
            0.73,
            0.87,
            0.67
          ]
        },
        "strict_format_compliance": {
          "mean": 0.21733333333333335,
          "std": 0.26906669970267383,
          "min": 0.0,
          "max": 0.7079999999999999,
          "median": 0.07300000000000001,
          "values": [
            0.0,
            0.032,
            0.45,
            0.7079999999999999,
            0.114,
            0.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.40833333333333327,
          "std": 0.4177661493653544,
          "min": 0.0,
          "max": 0.9840000000000001,
          "median": 0.266,
          "values": [
            0.0,
            0.08,
            0.9840000000000001,
            0.9319999999999999,
            0.452,
            0.002
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation": {
      "num_experiments": 5,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.146,
          "std": 0.23846173697262207,
          "min": 0.0,
          "max": 0.62,
          "median": 0.04,
          "values": [
            0.04,
            0.62,
            0.0,
            0.0,
            0.07
          ]
        },
        "precision": {
          "mean": 0.1460578522607736,
          "std": 0.2384333992699439,
          "min": 0.0,
          "max": 0.62,
          "median": 0.04,
          "values": [
            0.04,
            0.62,
            0.00016877937615724257,
            0.0,
            0.07012048192771085
          ]
        },
        "recall": {
          "mean": 0.152,
          "std": 0.2354909764725604,
          "min": 0.0,
          "max": 0.62,
          "median": 0.04,
          "values": [
            0.04,
            0.62,
            0.02,
            0.0,
            0.08
          ]
        },
        "f1_score": {
          "mean": 0.14611456582633053,
          "std": 0.2384056082966044,
          "min": 0.0,
          "max": 0.62,
          "median": 0.04,
          "values": [
            0.04,
            0.62,
            0.00033473389355742297,
            0.0,
            0.07023809523809524
          ]
        },
        "jaccard": {
          "mean": 0.1460578522607736,
          "std": 0.2384333992699439,
          "min": 0.0,
          "max": 0.62,
          "median": 0.04,
          "values": [
            0.04,
            0.62,
            0.00016877937615724257,
            0.0,
            0.07012048192771085
          ]
        },
        "semantic_similarity": {
          "mean": 0.4758170088445768,
          "std": 0.2351515222241927,
          "min": 0.23036548116244376,
          "max": 0.8741695193341001,
          "median": 0.44549427420832216,
          "values": [
            0.5712407356500626,
            0.8741695193341001,
            0.25781503386795523,
            0.44549427420832216,
            0.23036548116244376
          ]
        },
        "answer_correctness": {
          "mean": 0.392,
          "std": 0.30629397643440526,
          "min": 0.04,
          "max": 0.79,
          "median": 0.38,
          "values": [
            0.04,
            0.68,
            0.07,
            0.79,
            0.38
          ]
        },
        "strict_format_compliance": {
          "mean": 0.41419999999999996,
          "std": 0.33664129277318317,
          "min": 0.10000000000000002,
          "max": 0.8820000000000001,
          "median": 0.165,
          "values": [
            0.8820000000000001,
            0.7639999999999999,
            0.16,
            0.165,
            0.10000000000000002
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.5955999999999999,
          "std": 0.3376137437960724,
          "min": 0.17,
          "max": 1.0,
          "median": 0.414,
          "values": [
            1.0,
            0.99,
            0.414,
            0.40399999999999997,
            0.17
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.06833333333333334,
          "std": 0.06335525936249405,
          "min": 0.0,
          "max": 0.17,
          "median": 0.04,
          "values": [
            0.02,
            0.04,
            0.17,
            0.04,
            0.0,
            0.14
          ]
        },
        "precision": {
          "mean": 0.06924818936715736,
          "std": 0.06251496973858793,
          "min": 0.0051536861138175545,
          "max": 0.17,
          "median": 0.04,
          "values": [
            0.02,
            0.04,
            0.17,
            0.04,
            0.0051536861138175545,
            0.14033545008912657
          ]
        },
        "recall": {
          "mean": 0.10333333333333333,
          "std": 0.0703957069398096,
          "min": 0.02,
          "max": 0.18,
          "median": 0.10500000000000001,
          "values": [
            0.02,
            0.04,
            0.17,
            0.04,
            0.18,
            0.17
          ]
        },
        "f1_score": {
          "mean": 0.07004940142578689,
          "std": 0.06182972797954054,
          "min": 0.009632945747052552,
          "max": 0.17,
          "median": 0.04,
          "values": [
            0.02,
            0.04,
            0.17,
            0.04,
            0.009632945747052552,
            0.14066346280766878
          ]
        },
        "jaccard": {
          "mean": 0.06924818936715736,
          "std": 0.06251496973858793,
          "min": 0.0051536861138175545,
          "max": 0.17,
          "median": 0.04,
          "values": [
            0.02,
            0.04,
            0.17,
            0.04,
            0.0051536861138175545,
            0.14033545008912657
          ]
        },
        "semantic_similarity": {
          "mean": 0.487738784753019,
          "std": 0.14438186944588768,
          "min": 0.21163585524074732,
          "max": 0.6345584142208099,
          "median": 0.5607888621091843,
          "values": [
            0.5519237720966339,
            0.5696539521217346,
            0.6345584142208099,
            0.5696539521217346,
            0.3890067627164535,
            0.21163585524074732
          ]
        },
        "answer_correctness": {
          "mean": 0.19666666666666666,
          "std": 0.2266176417571138,
          "min": 0.04,
          "max": 0.68,
          "median": 0.10500000000000001,
          "values": [
            0.04,
            0.04,
            0.17,
            0.04,
            0.68,
            0.21
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.24,
          "std": 0.2797022226106423,
          "min": 0.04,
          "max": 0.83,
          "median": 0.13,
          "values": [
            0.04,
            0.04,
            0.22,
            0.04,
            0.83,
            0.27
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5633333333333334,
          "std": 0.09762456430404981,
          "min": 0.5,
          "max": 0.775,
          "median": 0.5225,
          "values": [
            0.5,
            0.5,
            0.545,
            0.5,
            0.775,
            0.56
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.085,
          "std": 0.09447221813845592,
          "min": 0.0,
          "max": 0.28,
          "median": 0.04,
          "values": [
            0.03,
            0.04,
            0.28,
            0.04,
            0.0,
            0.12
          ]
        },
        "precision": {
          "mean": 0.08651627392433271,
          "std": 0.0932385494413225,
          "min": 0.00870145948314553,
          "max": 0.28,
          "median": 0.04,
          "values": [
            0.03,
            0.04,
            0.28,
            0.04,
            0.00870145948314553,
            0.12039618406285073
          ]
        },
        "recall": {
          "mean": 0.11,
          "std": 0.08831760866327848,
          "min": 0.03,
          "max": 0.28,
          "median": 0.08,
          "values": [
            0.03,
            0.04,
            0.28,
            0.04,
            0.12,
            0.15
          ]
        },
        "f1_score": {
          "mean": 0.08775076379107762,
          "std": 0.0923167666720319,
          "min": 0.01572257991018602,
          "max": 0.28,
          "median": 0.04,
          "values": [
            0.03,
            0.04,
            0.28,
            0.04,
            0.01572257991018602,
            0.12078200283627971
          ]
        },
        "jaccard": {
          "mean": 0.08651627392433271,
          "std": 0.0932385494413225,
          "min": 0.00870145948314553,
          "max": 0.28,
          "median": 0.04,
          "values": [
            0.03,
            0.04,
            0.28,
            0.04,
            0.00870145948314553,
            0.12039618406285073
          ]
        },
        "semantic_similarity": {
          "mean": 0.5046397745163025,
          "std": 0.13895928519417455,
          "min": 0.2613967066071928,
          "max": 0.6790171325206756,
          "median": 0.5665206781029701,
          "values": [
            0.5633874040842056,
            0.5696539521217346,
            0.6790171325206756,
            0.5696539521217346,
            0.38472949964227154,
            0.2613967066071928
          ]
        },
        "answer_correctness": {
          "mean": 0.23166666666666666,
          "std": 0.2520857437901283,
          "min": 0.04,
          "max": 0.75,
          "median": 0.13999999999999999,
          "values": [
            0.04,
            0.04,
            0.28,
            0.04,
            0.75,
            0.24
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.26333333333333336,
          "std": 0.3009245014211298,
          "min": 0.04,
          "max": 0.89,
          "median": 0.16,
          "values": [
            0.04,
            0.04,
            0.28,
            0.04,
            0.89,
            0.29
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.585,
          "std": 0.1124722187920199,
          "min": 0.5,
          "max": 0.815,
          "median": 0.5375,
          "values": [
            0.5,
            0.5,
            0.62,
            0.5,
            0.815,
            0.575
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.11833333333333335,
          "std": 0.1525796695354776,
          "min": 0.0,
          "max": 0.45,
          "median": 0.05,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.0,
            0.12
          ]
        },
        "precision": {
          "mean": 0.12032437605263924,
          "std": 0.15114497496068943,
          "min": 0.01151375664086748,
          "max": 0.45,
          "median": 0.05,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.01151375664086748,
            0.12043249967496779
          ]
        },
        "recall": {
          "mean": 0.15333333333333335,
          "std": 0.14325579297962865,
          "min": 0.04,
          "max": 0.45,
          "median": 0.105,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.18,
            0.15
          ]
        },
        "f1_score": {
          "mean": 0.12193673468775534,
          "std": 0.15006956576690592,
          "min": 0.020767816951360228,
          "max": 0.45,
          "median": 0.05,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.020767816951360228,
            0.12085259117517183
          ]
        },
        "jaccard": {
          "mean": 0.12032437605263924,
          "std": 0.15114497496068943,
          "min": 0.01151375664086748,
          "max": 0.45,
          "median": 0.05,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.01151375664086748,
            0.12043249967496779
          ]
        },
        "semantic_similarity": {
          "mean": 0.5434307639483208,
          "std": 0.13246727251807375,
          "min": 0.3236174924112856,
          "max": 0.7599697810411453,
          "median": 0.5696539521217346,
          "values": [
            0.5800582206249237,
            0.5696539521217346,
            0.7599697810411453,
            0.5696539521217346,
            0.4576311853691004,
            0.3236174924112856
          ]
        },
        "answer_correctness": {
          "mean": 0.2733333333333333,
          "std": 0.2881936077631764,
          "min": 0.04,
          "max": 0.83,
          "median": 0.14,
          "values": [
            0.06,
            0.04,
            0.45,
            0.04,
            0.83,
            0.22
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.3083333333333333,
          "std": 0.33572889194838273,
          "min": 0.04,
          "max": 0.97,
          "median": 0.155,
          "values": [
            0.07,
            0.04,
            0.49,
            0.04,
            0.97,
            0.24
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.6033333333333334,
          "std": 0.12746459203332594,
          "min": 0.5,
          "max": 0.845,
          "median": 0.5425,
          "values": [
            0.505,
            0.5,
            0.69,
            0.5,
            0.845,
            0.58
          ]
        }
      }
    },
    "pubmedqa_reasoning_required": {
      "num_experiments": 7,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b_ft_pubmedqa_reasoning_ft__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.31857142857142856,
          "std": 0.28492748809019425,
          "min": 0.0,
          "max": 0.77,
          "median": 0.36,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.0,
            0.36,
            0.77
          ]
        },
        "precision": {
          "mean": 0.3197300615484289,
          "std": 0.28374458472808545,
          "min": 0.00753968253968254,
          "max": 0.77,
          "median": 0.36,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.00753968253968254,
            0.3605707482993197,
            0.77
          ]
        },
        "recall": {
          "mean": 0.33,
          "std": 0.2781058380236869,
          "min": 0.04,
          "max": 0.77,
          "median": 0.36,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.05,
            0.39,
            0.77
          ]
        },
        "f1_score": {
          "mean": 0.3205987791838047,
          "std": 0.28289157304987417,
          "min": 0.013071428571428569,
          "max": 0.77,
          "median": 0.36,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.013071428571428569,
            0.3611200257152041,
            0.77
          ]
        },
        "jaccard": {
          "mean": 0.3197300615484289,
          "std": 0.28374458472808545,
          "min": 0.00753968253968254,
          "max": 0.77,
          "median": 0.36,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.00753968253968254,
            0.3605707482993197,
            0.77
          ]
        },
        "semantic_similarity": {
          "mean": 0.7035756534431129,
          "std": 0.14037399909505588,
          "min": 0.5370984437223524,
          "max": 0.9230746108293534,
          "median": 0.7236178785562515,
          "values": [
            0.7236178785562515,
            0.5696539521217346,
            0.8574998915195465,
            0.568500058054924,
            0.7455847392976284,
            0.5370984437223524,
            0.9230746108293534
          ]
        },
        "answer_correctness": {
          "mean": 0.4428571428571429,
          "std": 0.2930487178276134,
          "min": 0.04,
          "max": 0.8,
          "median": 0.43,
          "values": [
            0.36,
            0.04,
            0.65,
            0.05,
            0.8,
            0.43,
            0.77
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.54,
          "std": 0.36469165057620934,
          "min": 0.04,
          "max": 0.99,
          "median": 0.52,
          "values": [
            0.44,
            0.04,
            0.75,
            0.06,
            0.98,
            0.52,
            0.99
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.6614285714285716,
          "std": 0.11903523745136162,
          "min": 0.5,
          "max": 0.81,
          "median": 0.655,
          "values": [
            0.625,
            0.5,
            0.765,
            0.5,
            0.81,
            0.655,
            0.775
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min": {
      "num_experiments": 6,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.10166666666666667,
          "std": 0.2185113777866549,
          "min": 0.0,
          "max": 0.59,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.02,
            0.59,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10588679334109467,
          "std": 0.2215334142960351,
          "min": 0.0008062354312354311,
          "max": 0.601,
          "median": 0.006210192204411017,
          "values": [
            0.0013026776619167902,
            0.000930205780280948,
            0.020163934426229504,
            0.601,
            0.011117706746905244,
            0.0008062354312354311
          ]
        },
        "recall": {
          "mean": 0.19000000000000003,
          "std": 0.20960279896349984,
          "min": 0.03,
          "max": 0.62,
          "median": 0.085,
          "values": [
            0.1,
            0.07,
            0.03,
            0.62,
            0.28,
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.1085404651663101,
          "std": 0.2222382816509183,
          "min": 0.0015774319688448113,
          "max": 0.6051515151515151,
          "median": 0.011178035154980871,
          "values": [
            0.0025698495375862185,
            0.0018351929223775132,
            0.02032258064516129,
            0.6051515151515151,
            0.019786220772375525,
            0.0015774319688448113
          ]
        },
        "jaccard": {
          "mean": 0.10588679334109467,
          "std": 0.2215334142960351,
          "min": 0.0008062354312354311,
          "max": 0.601,
          "median": 0.006210192204411017,
          "values": [
            0.0013026776619167902,
            0.000930205780280948,
            0.020163934426229504,
            0.601,
            0.011117706746905244,
            0.0008062354312354311
          ]
        },
        "semantic_similarity": {
          "mean": 0.341274274312503,
          "std": 0.3562663887041279,
          "min": 0.04247692535398528,
          "max": 0.8452368076331913,
          "median": 0.14244051541434602,
          "values": [
            0.07627987329848111,
            0.04247692535398528,
            0.8324580839648843,
            0.8452368076331913,
            0.2086011575302109,
            0.042592798094265166
          ]
        },
        "answer_correctness": {
          "mean": 0.5766666666666668,
          "std": 0.2375336233509316,
          "min": 0.17,
          "max": 0.84,
          "median": 0.64,
          "values": [
            0.17,
            0.37,
            0.84,
            0.68,
            0.8,
            0.6
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.7433333333333333,
          "std": 0.2693613846778256,
          "min": 0.3,
          "max": 1.0,
          "median": 0.845,
          "values": [
            0.3,
            0.48,
            0.99,
            1.0,
            0.95,
            0.74
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.6991666666666666,
          "std": 0.11681240896792124,
          "min": 0.51,
          "max": 0.845,
          "median": 0.7025,
          "values": [
            0.51,
            0.61,
            0.845,
            0.68,
            0.825,
            0.725
          ]
        }
      }
    }
  },
  "by_model": {
    "hf-mistral-nemo-12b": {
      "num_experiments": 44,
      "experiments": [
        "RHAI_cose_choice_selection__hf-mistral-nemo-12b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__hf-mistral-nemo-12b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__hf-mistral-nemo-12b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__hf-mistral-nemo-12b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__hf-mistral-nemo-12b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__hf-mistral-nemo-12b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__hf-mistral-nemo-12b__zero-shot__gmeg_basic__100__0p100",
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.03636363636363637,
          "std": 0.12686949108668735,
          "min": 0.0,
          "max": 0.73,
          "median": 0.0,
          "values": [
            0.31,
            0.0,
            0.01,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.01,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.01,
            0.01,
            0.05,
            0.73,
            0.0,
            0.02,
            0.03,
            0.06,
            0.36,
            0.0
          ]
        },
        "precision": {
          "mean": 0.10343270633809575,
          "std": 0.1386585196273291,
          "min": 0.0008295723152653063,
          "max": 0.73,
          "median": 0.04122453835951439,
          "values": [
            0.34,
            0.2043470206264324,
            0.2710679820056553,
            0.14020504346682058,
            0.020452365268034355,
            0.005985092345049565,
            0.008865008079372771,
            0.004004692475962245,
            0.004203346237664929,
            0.032449076719028774,
            0.02539934680838183,
            0.02266541758495709,
            0.01935852294089931,
            0.020226702046395123,
            0.01392248062886549,
            0.0075244364532344185,
            0.007321897345832062,
            0.008851904336562421,
            0.015070599419300978,
            0.025522110573418848,
            0.09650138003581887,
            0.09607084698848553,
            0.09450176493727287,
            0.1835645434735207,
            0.10169878430134373,
            0.09410443772872569,
            0.09740329080545494,
            0.1814849448190401,
            0.3746907428025377,
            0.24300644826220846,
            0.12003115342841778,
            0.11652862486942757,
            0.11534583487662842,
            0.16653098620827958,
            0.01,
            0.01,
            0.05,
            0.73,
            0.0008295723152653063,
            0.02,
            0.03,
            0.06,
            0.36,
            0.0013026776619167902
          ]
        },
        "recall": {
          "mean": 0.3359516533298168,
          "std": 0.18422373636669073,
          "min": 0.01,
          "max": 0.82,
          "median": 0.35789489488139353,
          "values": [
            0.3416666666666666,
            0.07638833259103094,
            0.1742063311299156,
            0.3745891674788984,
            0.82,
            0.41,
            0.57,
            0.34,
            0.4,
            0.5604792103321515,
            0.5344611466964408,
            0.5440752074722662,
            0.521779503176562,
            0.4908949808361573,
            0.33,
            0.435,
            0.42,
            0.465,
            0.325,
            0.3516666666666666,
            0.38084291264397324,
            0.37047164898773843,
            0.38188838754118026,
            0.39513423805964415,
            0.3557897897627871,
            0.3312279727082102,
            0.3391893547719219,
            0.3443703285419432,
            0.26725973729837293,
            0.2886052257904705,
            0.3778653268817748,
            0.380493739585753,
            0.3602410515207706,
            0.2932858193706418,
            0.01,
            0.01,
            0.05,
            0.73,
            0.06,
            0.02,
            0.03,
            0.06,
            0.36,
            0.1
          ]
        },
        "f1_score": {
          "mean": 0.11763872193444662,
          "std": 0.1346975698237195,
          "min": 0.001634911296259006,
          "max": 0.73,
          "median": 0.055,
          "values": [
            0.3406666666666666,
            0.104005606130473,
            0.19887381666910323,
            0.1953615087241507,
            0.03984708344301362,
            0.011789023708306517,
            0.017433196194303512,
            0.00790448794836974,
            0.008314979305548883,
            0.06052878394901255,
            0.047977696170655594,
            0.04316994111214483,
            0.037057229303371096,
            0.03854518039754913,
            0.026068600412325373,
            0.014743151517810566,
            0.014357012832301902,
            0.01731503299164184,
            0.028028060643947743,
            0.0385387951016717,
            0.15099882675974377,
            0.14936706195967742,
            0.14893942173234243,
            0.24417745411887654,
            0.15528852842993202,
            0.14367045420676094,
            0.14808039843271317,
            0.22457098102242015,
            0.2917493179502968,
            0.24871530612969905,
            0.17316757229496202,
            0.16958565690582078,
            0.1685442323280916,
            0.1945179387881015,
            0.01,
            0.01,
            0.05,
            0.73,
            0.001634911296259006,
            0.02,
            0.03,
            0.06,
            0.36,
            0.0025698495375862185
          ]
        },
        "jaccard": {
          "mean": 0.08208666439163706,
          "std": 0.12493143674619962,
          "min": 0.0008295723152653063,
          "max": 0.73,
          "median": 0.04085394183621987,
          "values": [
            0.3291666666666667,
            0.05832851534349138,
            0.1198940158325367,
            0.11013022879812855,
            0.020452365268034355,
            0.005985092345049565,
            0.008865008079372771,
            0.004004692475962245,
            0.004203346237664929,
            0.031707883672439735,
            0.02488708714681504,
            0.02232481265604236,
            0.019046013172166105,
            0.019853574130229187,
            0.013670879765186989,
            0.007500635352431935,
            0.007296665108754196,
            0.008818959330429315,
            0.014757363766174309,
            0.02539481979440683,
            0.08225184840743578,
            0.08134974725981296,
            0.08103738515521011,
            0.1411079925248137,
            0.0847125591934511,
            0.07790322868928407,
            0.08049937836942817,
            0.12835014809217857,
            0.18023991309653273,
            0.1476088298107717,
            0.09716132290198534,
            0.09533744963654023,
            0.09463171337853234,
            0.11120084179688913,
            0.01,
            0.01,
            0.05,
            0.73,
            0.0008295723152653063,
            0.02,
            0.03,
            0.06,
            0.36,
            0.0013026776619167902
          ]
        },
        "semantic_similarity": {
          "mean": 0.4805999288730725,
          "std": 0.19995331135705616,
          "min": 0.06692904483061284,
          "max": 0.8938113880157471,
          "median": 0.5278152510570362,
          "values": [
            0.7559962159395218,
            0.4240999024733901,
            0.5996882364153862,
            0.6157916417717934,
            0.10024355170316994,
            0.08034662605263293,
            0.08063821282004938,
            0.07979790750890971,
            0.06692904483061284,
            0.5460975639522075,
            0.5280703355371952,
            0.5488447332382203,
            0.4713038282096386,
            0.4902907335758209,
            0.4548057132959366,
            0.3790281207114458,
            0.38342820569872854,
            0.4007721272110939,
            0.47166656263172624,
            0.47294129610061647,
            0.6301788941025734,
            0.5396952456235886,
            0.5997585594654083,
            0.7116961959004402,
            0.4699423840641975,
            0.5527061250805855,
            0.5446191549301147,
            0.6748887285590172,
            0.6730493840575218,
            0.6961110651493072,
            0.5275601665768772,
            0.5151659706793725,
            0.49668813809752466,
            0.5141218559071422,
            0.47510350450873373,
            0.5360703114420176,
            0.566428534835577,
            0.8938113880157471,
            0.0827535550808534,
            0.5519237720966339,
            0.5633874040842056,
            0.5800582206249237,
            0.7236178785562515,
            0.07627987329848111
          ]
        },
        "answer_correctness": {
          "mean": 0.3727272727272727,
          "std": 0.2530573384214052,
          "min": 0.04,
          "max": 0.74,
          "median": 0.365,
          "values": [
            0.7,
            0.59,
            0.51,
            0.48,
            0.48,
            0.49,
            0.7,
            0.34,
            0.34,
            0.37,
            0.72,
            0.74,
            0.05,
            0.04,
            0.06,
            0.73,
            0.19,
            0.04,
            0.04,
            0.06,
            0.36,
            0.17
          ]
        },
        "response_conciseness": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.655,
          "std": 0.0,
          "min": 0.655,
          "max": 0.655,
          "median": 0.655,
          "values": [
            0.655
          ]
        },
        "explanation_quality": {
          "mean": 0.09,
          "std": 0.0,
          "min": 0.09,
          "max": 0.09,
          "median": 0.09,
          "values": [
            0.09
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_correctness": {
          "mean": 0.8300000000000001,
          "std": 0.16999999999999998,
          "min": 0.66,
          "max": 1.0,
          "median": 0.8300000000000001,
          "values": [
            0.66,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.7685,
          "std": 0.1875000000000001,
          "min": 0.5809999999999998,
          "max": 0.9560000000000001,
          "median": 0.7685,
          "values": [
            0.9560000000000001,
            0.5809999999999998
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "follows_format_instruction": {
          "mean": 0.1766363636363636,
          "std": 0.05317086134216531,
          "min": 0.09999999999999998,
          "max": 0.275,
          "median": 0.19599999999999995,
          "values": [
            0.19799999999999998,
            0.19599999999999995,
            0.19199999999999995,
            0.19599999999999995,
            0.19599999999999995,
            0.161,
            0.09999999999999998,
            0.09999999999999998,
            0.10399999999999998,
            0.275,
            0.22500000000000003
          ]
        },
        "answer_extractability": {
          "mean": 0.7277545454545454,
          "std": 0.08423608990792011,
          "min": 0.5799,
          "max": 0.8159999999999998,
          "median": 0.7439999999999998,
          "values": [
            0.8039999999999998,
            0.7439999999999998,
            0.7339999999999999,
            0.8159999999999998,
            0.8059999999999999,
            0.7632000000000001,
            0.6062,
            0.6078000000000001,
            0.5799,
            0.742,
            0.8022
          ]
        },
        "response_brevity": {
          "mean": 0.1312,
          "std": 0.03971599174136286,
          "min": 0.09799999999999998,
          "max": 0.20400000000000007,
          "median": 0.10899999999999999,
          "values": [
            0.20400000000000007,
            0.14300000000000002,
            0.10199999999999998,
            0.09799999999999998,
            0.10899999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.5854000000000001,
          "std": 0.06840862518717937,
          "min": 0.4770000000000001,
          "max": 0.6744999999999999,
          "median": 0.607,
          "values": [
            0.6245,
            0.544,
            0.607,
            0.4770000000000001,
            0.6744999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.0132,
          "std": 0.020143485299222674,
          "min": 0.0,
          "max": 0.052000000000000005,
          "median": 0.0,
          "values": [
            0.052000000000000005,
            0.013999999999999999,
            0.0,
            0.0,
            0.0
          ]
        },
        "all_choices_addressed": {
          "mean": 0.45371428571428574,
          "std": 0.19044437829584202,
          "min": 0.302,
          "max": 0.9039999999999999,
          "median": 0.41400000000000003,
          "values": [
            0.302,
            0.4160000000000001,
            0.41400000000000003,
            0.9039999999999999,
            0.30999999999999994,
            0.3880000000000001,
            0.4420000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9925714285714287,
          "std": 0.009955000794391065,
          "min": 0.972,
          "max": 1.0,
          "median": 1.0,
          "values": [
            0.972,
            1.0,
            0.986,
            0.99,
            1.0,
            1.0,
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.4047285714285715,
          "std": 0.07844455210594323,
          "min": 0.30060000000000003,
          "max": 0.543,
          "median": 0.4244,
          "values": [
            0.543,
            0.4467000000000001,
            0.4244,
            0.4403000000000001,
            0.30060000000000003,
            0.3159,
            0.3622000000000001
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9631666666666666,
          "std": 0.0,
          "min": 0.9631666666666666,
          "max": 0.9631666666666666,
          "median": 0.9631666666666666,
          "values": [
            0.9631666666666666
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7615942307692307,
          "std": 0.0,
          "min": 0.7615942307692307,
          "max": 0.7615942307692307,
          "median": 0.7615942307692307,
          "values": [
            0.7615942307692307
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.8455833333333334,
          "std": 0.11408333333333337,
          "min": 0.7314999999999999,
          "max": 0.9596666666666667,
          "median": 0.8455833333333334,
          "values": [
            0.7314999999999999,
            0.9596666666666667
          ]
        },
        "correct_answer_focus": {
          "mean": 0.5727500000000001,
          "std": 0.11225000000000002,
          "min": 0.4605,
          "max": 0.685,
          "median": 0.5727500000000001,
          "values": [
            0.685,
            0.4605
          ]
        },
        "concise_justification": {
          "mean": 0.9566999999999999,
          "std": 0.005199999999999927,
          "min": 0.9515,
          "max": 0.9618999999999999,
          "median": 0.9566999999999999,
          "values": [
            0.9618999999999999,
            0.9515
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.4213333333333333,
          "std": 0.033946649319188935,
          "min": 1.3773333333333335,
          "max": 1.4663333333333333,
          "median": 1.4208333333333334,
          "values": [
            1.4388333333333332,
            1.4663333333333333,
            1.4028333333333336,
            1.3773333333333335
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7725000000000001,
          "std": 0.046233105022267316,
          "min": 0.7058333333333333,
          "max": 0.8275,
          "median": 0.7783333333333333,
          "values": [
            0.7558333333333335,
            0.8008333333333333,
            0.7058333333333333,
            0.8275
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.91,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.3510553974689287,
          "std": 0.40722443681737985,
          "min": 0.0,
          "max": 0.9939732620320855,
          "median": 0.2051241639218147,
          "values": [
            0.0,
            0.0,
            0.9939732620320855,
            0.4102483278436294
          ]
        },
        "strict_format_compliance": {
          "mean": 0.4999999999999999,
          "std": 0.29566467492752657,
          "min": 0.0,
          "max": 0.7999999999999998,
          "median": 0.5479999999999999,
          "values": [
            0.3719999999999999,
            0.5479999999999999,
            0.7799999999999998,
            0.7999999999999998,
            0.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.77,
          "std": 0.387401600409704,
          "min": 0.0,
          "max": 1.0,
          "median": 0.98,
          "values": [
            0.88,
            0.99,
            0.98,
            1.0,
            0.0
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.17800000000000002,
          "std": 0.16326665305566843,
          "min": 0.04,
          "max": 0.44,
          "median": 0.07,
          "values": [
            0.04,
            0.04,
            0.07,
            0.44,
            0.3
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5279999999999999,
          "std": 0.048641546028061235,
          "min": 0.5,
          "max": 0.625,
          "median": 0.505,
          "values": [
            0.5,
            0.5,
            0.505,
            0.625,
            0.51
          ]
        }
      }
    },
    "hf-qwen2.5-3b": {
      "num_experiments": 46,
      "experiments": [
        "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__hf-qwen2.5-3b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__hf-qwen2.5-3b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen2.5-3b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_perspective__100__0p100",
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__hf-qwen2.5-3b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__hf-qwen2.5-3b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__hf-qwen2.5-3b__zero-shot__gmeg_basic__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.07586956521739131,
          "std": 0.20940470580398723,
          "min": 0.0,
          "max": 0.88,
          "median": 0.0,
          "values": [
            0.68,
            0.0,
            0.0,
            0.0,
            0.04,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.71,
            0.0,
            0.0,
            0.0,
            0.88,
            0.71,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.0,
            0.04,
            0.04,
            0.04,
            0.04,
            0.0
          ]
        },
        "precision": {
          "mean": 0.1410848096941988,
          "std": 0.2039798589680102,
          "min": 0.0004881729019660054,
          "max": 0.8861134676564157,
          "median": 0.07385734714704598,
          "values": [
            0.69,
            0.24690214964218196,
            0.21715598631224858,
            0.1300239358349212,
            0.05504448497041228,
            0.004097150807079523,
            0.008009306615740935,
            0.0035154361016346003,
            0.003621677298628642,
            0.04557084842706161,
            0.024600590783897664,
            0.02190535877702346,
            0.0189175577179169,
            0.019163296167254287,
            0.7200303295285826,
            0.008522336538908305,
            0.008787499082289697,
            0.008311672919133647,
            0.8861134676564157,
            0.713981449444274,
            0.10244197370875899,
            0.10027596243556176,
            0.09829710417820166,
            0.19209061512850714,
            0.10194196490250626,
            0.09517211936859704,
            0.11599080094633628,
            0.09267020932367967,
            0.2360652492606102,
            0.28283361234475163,
            0.22713080245473005,
            0.13734193549503154,
            0.12051848220273391,
            0.0982685163310635,
            0.18316898454422095,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.0004881729019660054,
            0.04,
            0.04,
            0.04,
            0.04,
            0.000930205780280948
          ]
        },
        "recall": {
          "mean": 0.3773460892582879,
          "std": 0.23526143610053102,
          "min": 0.03,
          "max": 0.915,
          "median": 0.4013980265172995,
          "values": [
            0.69,
            0.20070787972517093,
            0.2642219724126587,
            0.43467512639630457,
            0.87,
            0.37,
            0.69,
            0.34,
            0.38,
            0.5241906949259891,
            0.5406409015820781,
            0.536275461466638,
            0.5358647153500096,
            0.4955000603971192,
            0.81,
            0.54,
            0.53,
            0.585,
            0.915,
            0.775,
            0.4260019471045889,
            0.4257268849721244,
            0.4238067413167893,
            0.43468391689912195,
            0.40929083684962725,
            0.42174494376373195,
            0.4161228393532694,
            0.39350521618497175,
            0.44029410367759925,
            0.33624103447413994,
            0.33912661300515906,
            0.3500143408670139,
            0.3467005139055157,
            0.3532869886921498,
            0.24429637255947415,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.03,
            0.04,
            0.04,
            0.04,
            0.04,
            0.07
          ]
        },
        "f1_score": {
          "mean": 0.16180893973794236,
          "std": 0.2029601076422047,
          "min": 0.0009590556730091613,
          "max": 0.8871428571428572,
          "median": 0.11485414379144371,
          "values": [
            0.69,
            0.20911999062887965,
            0.2260900319435403,
            0.19304835868081727,
            0.06951497669012922,
            0.008099777960585737,
            0.015820790405372404,
            0.0069559761868289,
            0.007172080855749655,
            0.08233170180758277,
            0.04658093408689554,
            0.041750485357798885,
            0.03627560367279813,
            0.036631992125233236,
            0.7257207795377792,
            0.016716160835231995,
            0.017228950058844103,
            0.01633992298797158,
            0.8871428571428572,
            0.7170445398747617,
            0.1623083618713913,
            0.1593944547925764,
            0.15682926718908757,
            0.25993281414743463,
            0.16056623958804195,
            0.15250814850198202,
            0.17809793375638397,
            0.14737658577530466,
            0.2880224527919178,
            0.29677117818000276,
            0.2600337834804731,
            0.185488221032632,
            0.17079180141365133,
            0.14787739182203263,
            0.19483243416739132,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.0009590556730091613,
            0.04,
            0.04,
            0.04,
            0.04,
            0.0018351929223775132
          ]
        },
        "jaccard": {
          "mean": 0.12447747964745363,
          "std": 0.20086884957068218,
          "min": 0.0004881729019660054,
          "max": 0.8844468009897491,
          "median": 0.06754823753128514,
          "values": [
            0.6866666666666665,
            0.12121886485500809,
            0.13051404237270547,
            0.10845905009479939,
            0.05504448497041228,
            0.004097150807079523,
            0.008009306615740935,
            0.0035154361016346003,
            0.003621677298628642,
            0.04393898211276162,
            0.024130243659895724,
            0.021547826665100175,
            0.01862308872016518,
            0.018844493331047055,
            0.7198487332966234,
            0.00849899385853161,
            0.00875311524941499,
            0.008289133337773414,
            0.8844468009897491,
            0.7138028780157026,
            0.0889264163712685,
            0.08722653665749008,
            0.08559920749288855,
            0.1512591168452178,
            0.08785645893139245,
            0.08310825444747387,
            0.09844440164412212,
            0.08005199009215799,
            0.17076389996907515,
            0.1812543356959735,
            0.15342230017879868,
            0.10520710932164107,
            0.09600669797746281,
            0.08129240193485901,
            0.11225558852135702,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.0004881729019660054,
            0.04,
            0.04,
            0.04,
            0.04,
            0.000930205780280948
          ]
        },
        "semantic_similarity": {
          "mean": 0.5128664944426703,
          "std": 0.20856918355807585,
          "min": 0.04247692535398528,
          "max": 0.9308395344018936,
          "median": 0.5620297814905644,
          "values": [
            0.8124932911247015,
            0.5534891698509454,
            0.6366396707296371,
            0.6268906626105308,
            0.20189217027276754,
            0.08523711950518191,
            0.0891882423334755,
            0.08658209849148989,
            0.0709951462643221,
            0.5832784813642502,
            0.5587588514387608,
            0.5653236883878708,
            0.53038289681077,
            0.49335808366537093,
            0.8337197145819664,
            0.41642322421073913,
            0.4014344811439514,
            0.4062493397295475,
            0.9308395344018936,
            0.8369075068831444,
            0.5942050370573998,
            0.4992094074189663,
            0.548807926774025,
            0.7224323785305024,
            0.4261318787932396,
            0.5269784726202488,
            0.5653007115423679,
            0.4667537838220596,
            0.7280371448397637,
            0.7155673375725746,
            0.7356852543354034,
            0.5092172273900359,
            0.5254467269033194,
            0.48781229396816345,
            0.4819990609586239,
            0.5696539521217346,
            0.5696539521217346,
            0.5712407356500626,
            0.5696539521217346,
            0.6201044678688049,
            0.11679093430982902,
            0.5696539521217346,
            0.5696539521217346,
            0.5696539521217346,
            0.5696539521217346,
            0.04247692535398528
          ]
        },
        "answer_correctness": {
          "mean": 0.3547826086956522,
          "std": 0.2760414204145803,
          "min": 0.04,
          "max": 0.9,
          "median": 0.43,
          "values": [
            0.68,
            0.5,
            0.45,
            0.46,
            0.47,
            0.46,
            0.8,
            0.47,
            0.43,
            0.38,
            0.9,
            0.81,
            0.04,
            0.04,
            0.04,
            0.04,
            0.15,
            0.51,
            0.04,
            0.04,
            0.04,
            0.04,
            0.37
          ]
        },
        "response_conciseness": {
          "mean": 0.992,
          "std": 0.0,
          "min": 0.992,
          "max": 0.992,
          "median": 0.992,
          "values": [
            0.992
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "explanation_quality": {
          "mean": 0.64,
          "std": 0.0,
          "min": 0.64,
          "max": 0.64,
          "median": 0.64,
          "values": [
            0.64
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_correctness": {
          "mean": 0.995,
          "std": 0.0050000000000000044,
          "min": 0.99,
          "max": 1.0,
          "median": 0.995,
          "values": [
            0.99,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.67,
          "std": 0.29799999999999993,
          "min": 0.37200000000000005,
          "max": 0.968,
          "median": 0.67,
          "values": [
            0.968,
            0.37200000000000005
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "follows_format_instruction": {
          "mean": 0.37290909090909086,
          "std": 0.33629274437215473,
          "min": 0.09999999999999998,
          "max": 0.9710000000000001,
          "median": 0.19999999999999996,
          "values": [
            0.256,
            0.19999999999999996,
            0.19999999999999996,
            0.19999999999999996,
            0.19999999999999996,
            0.889,
            0.09999999999999998,
            0.09999999999999998,
            0.09999999999999998,
            0.9710000000000001,
            0.8859999999999999
          ]
        },
        "answer_extractability": {
          "mean": 0.8176272727272728,
          "std": 0.12993783192469108,
          "min": 0.5955999999999999,
          "max": 0.995,
          "median": 0.8271999999999999,
          "values": [
            0.8044,
            0.8271999999999999,
            0.8079999999999999,
            0.9076000000000001,
            0.862,
            0.9538,
            0.6435999999999998,
            0.6483999999999999,
            0.5955999999999999,
            0.995,
            0.9483
          ]
        },
        "response_brevity": {
          "mean": 0.1292,
          "std": 0.09927215118047965,
          "min": 0.04899999999999999,
          "max": 0.32400000000000007,
          "median": 0.09299999999999999,
          "values": [
            0.32400000000000007,
            0.106,
            0.09299999999999999,
            0.04899999999999999,
            0.07399999999999998
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.772,
          "std": 0.05781781732303642,
          "min": 0.709,
          "max": 0.8715000000000002,
          "median": 0.7765,
          "values": [
            0.8715000000000002,
            0.7765,
            0.7829999999999999,
            0.709,
            0.7199999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.057199999999999994,
          "std": 0.11340264547178783,
          "min": 0.0,
          "max": 0.284,
          "median": 0.0,
          "values": [
            0.284,
            0.002,
            0.0,
            0.0,
            0.0
          ]
        },
        "all_choices_addressed": {
          "mean": 0.41425000000000006,
          "std": 0.1734832484708538,
          "min": 0.262,
          "max": 0.8480000000000001,
          "median": 0.37900000000000006,
          "values": [
            0.3880000000000001,
            0.41200000000000014,
            0.37,
            0.8480000000000001,
            0.262,
            0.3280000000000001,
            0.43000000000000016,
            0.27599999999999997
          ]
        },
        "single_paragraph_format": {
          "mean": 0.953625,
          "std": 0.040699930896747234,
          "min": 0.88,
          "max": 1.0,
          "median": 0.9585,
          "values": [
            0.96,
            0.9570000000000001,
            0.904,
            1.0,
            0.9840000000000001,
            0.88,
            1.0,
            0.944
          ]
        },
        "explanation_depth": {
          "mean": 0.38430000000000003,
          "std": 0.075134080150089,
          "min": 0.26060000000000005,
          "max": 0.48340000000000005,
          "median": 0.3855,
          "values": [
            0.48340000000000005,
            0.4709,
            0.4129,
            0.44260000000000005,
            0.26060000000000005,
            0.31560000000000005,
            0.35810000000000003,
            0.3303000000000001
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.6975,
          "std": 0.0,
          "min": 0.6975,
          "max": 0.6975,
          "median": 0.6975,
          "values": [
            0.6975
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7048857142857142,
          "std": 0.0,
          "min": 0.7048857142857142,
          "max": 0.7048857142857142,
          "median": 0.7048857142857142,
          "values": [
            0.7048857142857142
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.7012583333333334,
          "std": 0.2481583333333334,
          "min": 0.4531,
          "max": 0.9494166666666668,
          "median": 0.7012583333333334,
          "values": [
            0.4531,
            0.9494166666666668
          ]
        },
        "correct_answer_focus": {
          "mean": 0.7662500000000001,
          "std": 0.09824999999999995,
          "min": 0.6680000000000001,
          "max": 0.8645,
          "median": 0.7662500000000001,
          "values": [
            0.8645,
            0.6680000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.9192499999999999,
          "std": 0.029350000000000043,
          "min": 0.8898999999999999,
          "max": 0.9486,
          "median": 0.9192499999999999,
          "values": [
            0.8898999999999999,
            0.9486
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.483625,
          "std": 0.20886463502443134,
          "min": 1.1455,
          "max": 1.7113333333333336,
          "median": 1.5388333333333333,
          "values": [
            1.5721666666666665,
            1.7113333333333336,
            1.1455,
            1.5055
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6714583333333333,
          "std": 0.0453549848724237,
          "min": 0.6216666666666666,
          "max": 0.7441666666666668,
          "median": 0.6599999999999999,
          "values": [
            0.6699999999999998,
            0.65,
            0.6216666666666666,
            0.7441666666666668
          ]
        },
        "structural_format_match": {
          "mean": 0.9075000000000001,
          "std": 0.004330127018922197,
          "min": 0.9,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.9,
            0.91,
            0.91,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.35238057165065473,
          "std": 0.4045904295252177,
          "min": 0.0,
          "max": 0.9859064736433157,
          "median": 0.21180790647965161,
          "values": [
            0.0,
            0.0,
            0.9859064736433157,
            0.42361581295930323
          ]
        },
        "strict_format_compliance": {
          "mean": 0.819,
          "std": 0.3545847336439251,
          "min": 0.032,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            0.8820000000000001,
            1.0,
            1.0,
            0.032
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.8466666666666667,
          "std": 0.34286375654996776,
          "min": 0.08,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.08
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.128,
          "std": 0.176,
          "min": 0.04,
          "max": 0.48,
          "median": 0.04,
          "values": [
            0.04,
            0.04,
            0.04,
            0.04,
            0.48
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.522,
          "std": 0.044,
          "min": 0.5,
          "max": 0.61,
          "median": 0.5,
          "values": [
            0.5,
            0.5,
            0.5,
            0.5,
            0.61
          ]
        }
      }
    },
    "u4b-llama3-8b": {
      "num_experiments": 45,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3-8b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__u4b-llama3-8b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3-8b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3-8b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3-8b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3-8b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__u4b-llama3-8b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__u4b-llama3-8b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3-8b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__u4b-llama3-8b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3-8b__zero-shot__gmeg_basic__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.10311111111111113,
          "std": 0.21110105239194724,
          "min": 0.0,
          "max": 0.8,
          "median": 0.0,
          "values": [
            0.13,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.32,
            0.44,
            0.62,
            0.63,
            0.8,
            0.13,
            0.17,
            0.28,
            0.45,
            0.65,
            0.02
          ]
        },
        "precision": {
          "mean": 0.16442960870897347,
          "std": 0.19252397310781397,
          "min": 0.0004219207887564308,
          "max": 0.8,
          "median": 0.11232939067723872,
          "values": [
            0.17066666666666663,
            0.15689040508315139,
            0.21116309727705734,
            0.14095966309739427,
            0.004871921526766584,
            0.0006477361291654041,
            0.0016166789530344982,
            0.0004219207887564308,
            0.0005192621365700335,
            0.03794357996519728,
            0.026672471833243463,
            0.027553456553650088,
            0.02628224892419033,
            0.024979105849702088,
            0.025762460610937087,
            0.006771988861443903,
            0.008099413110993587,
            0.010349293673168674,
            0.05439928258493284,
            0.034395576029195414,
            0.1192233368466715,
            0.10885086322362555,
            0.10855459641869158,
            0.18420417725092644,
            0.10497446321938876,
            0.11350090768163518,
            0.11232939067723872,
            0.1640683464500342,
            0.264387653966471,
            0.1775280103685869,
            0.10537446616922469,
            0.10478612299373015,
            0.11629653123470433,
            0.13412336132142902,
            0.32,
            0.44,
            0.62,
            0.63,
            0.8,
            0.13,
            0.17,
            0.28,
            0.45,
            0.65,
            0.020163934426229504
          ]
        },
        "recall": {
          "mean": 0.37428695227865894,
          "std": 0.1778946548207815,
          "min": 0.03,
          "max": 0.8,
          "median": 0.3867849572008764,
          "values": [
            0.18333333333333332,
            0.1314184130551263,
            0.29967265965370204,
            0.4189656730716152,
            0.21,
            0.06,
            0.14,
            0.05,
            0.05,
            0.5060214556685145,
            0.4560957343310284,
            0.479000930442107,
            0.4550645335057101,
            0.40925482524011936,
            0.54,
            0.485,
            0.425,
            0.47,
            0.72,
            0.6,
            0.4055359033221203,
            0.3867849572008764,
            0.37773488076639417,
            0.46781162855887837,
            0.36126505710264967,
            0.3458292774510594,
            0.3573079271995745,
            0.46629159451760577,
            0.3577853375793798,
            0.43983052902111197,
            0.3321391484752698,
            0.33908495498513547,
            0.3286627266774538,
            0.26802137138088716,
            0.32,
            0.44,
            0.62,
            0.63,
            0.8,
            0.13,
            0.17,
            0.28,
            0.45,
            0.65,
            0.03
          ]
        },
        "f1_score": {
          "mean": 0.18759156983150627,
          "std": 0.18790623177403895,
          "min": 0.0008367523270071996,
          "max": 0.8,
          "median": 0.1644675873681548,
          "values": [
            0.16916666666666663,
            0.12107995168359142,
            0.23781148459618684,
            0.20248926343199922,
            0.009484041067423092,
            0.0012807210728379142,
            0.0031952622714167545,
            0.0008367523270071996,
            0.0010277187254434798,
            0.06938919762058396,
            0.049912306747785413,
            0.05161889758064091,
            0.04923023302997629,
            0.04625390969398709,
            0.04777438834102674,
            0.013331199194835587,
            0.015852074479994788,
            0.020178023662466838,
            0.0937732139126142,
            0.061479280912402456,
            0.18065707063999145,
            0.16683838327643852,
            0.16559036685771542,
            0.25854952349008764,
            0.1592807578263989,
            0.1644675873681548,
            0.16502439795099125,
            0.22847661977633685,
            0.29290683675032814,
            0.24264238454235382,
            0.1531497419523496,
            0.15450160147144312,
            0.1646939794235943,
            0.16935422342754966,
            0.32,
            0.44,
            0.62,
            0.63,
            0.8,
            0.13,
            0.17,
            0.28,
            0.45,
            0.65,
            0.02032258064516129
          ]
        },
        "jaccard": {
          "mean": 0.15070026799644637,
          "std": 0.19371645006996357,
          "min": 0.0004219207887564308,
          "max": 0.8,
          "median": 0.0905432225537786,
          "values": [
            0.1540952380952381,
            0.0690678591308944,
            0.1381444301898855,
            0.11485640211464439,
            0.004871921526766584,
            0.0006477361291654041,
            0.0016166789530344982,
            0.0004219207887564308,
            0.0005192621365700335,
            0.03659897824736949,
            0.025979021207328982,
            0.02686778350922517,
            0.025596235273989052,
            0.024047286391003904,
            0.02571569986949813,
            0.006752608745969655,
            0.008069842468604512,
            0.010298951629070146,
            0.05436054496686274,
            0.034357544259054885,
            0.10006887092421408,
            0.09179937456138797,
            0.0909111329082894,
            0.15019295777727315,
            0.08707964602152211,
            0.09032989211945881,
            0.0905432225537786,
            0.13105611733710348,
            0.17771054772563183,
            0.14124233824467206,
            0.0847984098093961,
            0.0857823976586241,
            0.09180649463146945,
            0.09514077750810275,
            0.32,
            0.44,
            0.62,
            0.63,
            0.8,
            0.13,
            0.17,
            0.28,
            0.45,
            0.65,
            0.020163934426229504
          ]
        },
        "semantic_similarity": {
          "mean": 0.5475190552856979,
          "std": 0.221208666265498,
          "min": 0.04626693273894489,
          "max": 0.933139499425888,
          "median": 0.5342562747793272,
          "values": [
            0.7020514233410359,
            0.45225615467876196,
            0.6308293950557708,
            0.6239275026321411,
            0.12052421312779188,
            0.06372932297177612,
            0.04626693273894489,
            0.05541439942084253,
            0.059290827624499796,
            0.519278349801898,
            0.5351960927993059,
            0.5556939855962991,
            0.4980191664397717,
            0.4677891075611115,
            0.4356939936056733,
            0.36877889998257163,
            0.3479398596659303,
            0.4352716864645481,
            0.4854020461440086,
            0.4459284994006157,
            0.6502753323316575,
            0.5047750861942768,
            0.5751877236366272,
            0.7253133010864258,
            0.43809329748153686,
            0.5113372254371643,
            0.5277336171269417,
            0.6311278274655342,
            0.6796818909049034,
            0.7010463732481003,
            0.5342562747793272,
            0.5267234312975779,
            0.5034227759391069,
            0.5068688761256636,
            0.7084457975625992,
            0.7666562902927399,
            0.8741695193341001,
            0.8586994200944901,
            0.933139499425888,
            0.868618765771389,
            0.6345584142208099,
            0.6790171325206756,
            0.7599697810411453,
            0.8574998915195465,
            0.8324580839648843
          ]
        },
        "answer_correctness": {
          "mean": 0.5334782608695652,
          "std": 0.18061750831527632,
          "min": 0.17,
          "max": 0.85,
          "median": 0.51,
          "values": [
            0.65,
            0.51,
            0.27,
            0.46,
            0.47,
            0.41,
            0.63,
            0.38,
            0.48,
            0.58,
            0.71,
            0.61,
            0.32,
            0.44,
            0.68,
            0.63,
            0.8,
            0.85,
            0.17,
            0.28,
            0.45,
            0.65,
            0.84
          ]
        },
        "response_conciseness": {
          "mean": 0.9740000000000001,
          "std": 0.0,
          "min": 0.9740000000000001,
          "max": 0.9740000000000001,
          "median": 0.9740000000000001,
          "values": [
            0.9740000000000001
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.695,
          "std": 0.0,
          "min": 0.695,
          "max": 0.695,
          "median": 0.695,
          "values": [
            0.695
          ]
        },
        "explanation_quality": {
          "mean": 0.41,
          "std": 0.0,
          "min": 0.41,
          "max": 0.41,
          "median": 0.41,
          "values": [
            0.41
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_correctness": {
          "mean": 0.985,
          "std": 0.015000000000000013,
          "min": 0.97,
          "max": 1.0,
          "median": 0.985,
          "values": [
            1.0,
            0.97
          ]
        },
        "brevity_compliance": {
          "mean": 0.669,
          "std": 0.22900000000000004,
          "min": 0.44000000000000006,
          "max": 0.8980000000000001,
          "median": 0.669,
          "values": [
            0.8980000000000001,
            0.44000000000000006
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "follows_format_instruction": {
          "mean": 0.17563636363636362,
          "std": 0.06188445645243419,
          "min": 0.09799999999999998,
          "max": 0.293,
          "median": 0.17999999999999994,
          "values": [
            0.17999999999999994,
            0.13599999999999998,
            0.17999999999999997,
            0.19599999999999995,
            0.17399999999999996,
            0.21300000000000008,
            0.09899999999999999,
            0.09799999999999998,
            0.10099999999999998,
            0.293,
            0.262
          ]
        },
        "answer_extractability": {
          "mean": 0.7296545454545454,
          "std": 0.09222658228156813,
          "min": 0.5259999999999999,
          "max": 0.8299000000000002,
          "median": 0.7571000000000001,
          "values": [
            0.7364,
            0.5259999999999999,
            0.7467999999999999,
            0.7900000000000001,
            0.5954,
            0.8299000000000002,
            0.6574999999999999,
            0.7571000000000001,
            0.8152000000000001,
            0.7715000000000001,
            0.8004000000000001
          ]
        },
        "response_brevity": {
          "mean": 0.194,
          "std": 0.04939635614091386,
          "min": 0.138,
          "max": 0.27399999999999997,
          "median": 0.17200000000000007,
          "values": [
            0.27399999999999997,
            0.22600000000000006,
            0.17200000000000007,
            0.16000000000000003,
            0.138
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7826000000000001,
          "std": 0.060303731227843604,
          "min": 0.7325,
          "max": 0.875,
          "median": 0.7369999999999999,
          "values": [
            0.7369999999999999,
            0.875,
            0.7340000000000001,
            0.7325,
            0.8345
          ]
        },
        "single_sentence_format": {
          "mean": 0.020800000000000003,
          "std": 0.023650792798551177,
          "min": 0.0,
          "max": 0.06000000000000001,
          "median": 0.008,
          "values": [
            0.06000000000000001,
            0.008,
            0.0,
            0.0,
            0.036000000000000004
          ]
        },
        "all_choices_addressed": {
          "mean": 0.37685714285714284,
          "std": 0.23518225672348453,
          "min": 0.23200000000000004,
          "max": 0.9399999999999998,
          "median": 0.264,
          "values": [
            0.30999999999999994,
            0.39000000000000007,
            0.25199999999999995,
            0.9399999999999998,
            0.264,
            0.23200000000000004,
            0.25
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9991428571428571,
          "std": 0.0020995626366712584,
          "min": 0.9940000000000001,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            0.9940000000000001,
            1.0,
            1.0,
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.3043857142857143,
          "std": 0.07690507002739394,
          "min": 0.1986,
          "max": 0.4152,
          "median": 0.2998,
          "values": [
            0.4103,
            0.3018,
            0.278,
            0.4152,
            0.22700000000000006,
            0.1986,
            0.2998
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9576666666666667,
          "std": 0.0,
          "min": 0.9576666666666667,
          "max": 0.9576666666666667,
          "median": 0.9576666666666667,
          "values": [
            0.9576666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.509588888888889,
          "std": 0.0,
          "min": 0.509588888888889,
          "max": 0.509588888888889,
          "median": 0.509588888888889,
          "values": [
            0.509588888888889
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.44783749999999994,
          "std": 0.20000416666666668,
          "min": 0.24783333333333327,
          "max": 0.6478416666666666,
          "median": 0.44783749999999994,
          "values": [
            0.24783333333333327,
            0.6478416666666666
          ]
        },
        "correct_answer_focus": {
          "mean": 0.8242500000000001,
          "std": 0.08024999999999999,
          "min": 0.7440000000000001,
          "max": 0.9045000000000001,
          "median": 0.8242500000000001,
          "values": [
            0.9045000000000001,
            0.7440000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.8469499999999999,
          "std": 0.005150000000000099,
          "min": 0.8417999999999998,
          "max": 0.8521,
          "median": 0.8469499999999999,
          "values": [
            0.8417999999999998,
            0.8521
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.9887083333333334,
          "std": 0.19481274395771264,
          "min": 0.7861666666666668,
          "max": 1.2633333333333332,
          "median": 0.9526666666666667,
          "values": [
            1.0806666666666667,
            1.2633333333333332,
            0.8246666666666667,
            0.7861666666666668
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7072916666666667,
          "std": 0.03568464792067056,
          "min": 0.6458333333333335,
          "max": 0.7316666666666666,
          "median": 0.7258333333333333,
          "values": [
            0.7300000000000001,
            0.7216666666666667,
            0.6458333333333335,
            0.7316666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.91,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.3125376232620729,
          "std": 0.4026410871302344,
          "min": 0.0,
          "max": 0.9840740740740741,
          "median": 0.13303820948710876,
          "values": [
            0.0,
            0.0,
            0.9840740740740741,
            0.2660764189742175
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7403333333333332,
          "std": 0.13098303537311826,
          "min": 0.45,
          "max": 0.8220000000000001,
          "median": 0.8009999999999999,
          "values": [
            0.8220000000000001,
            0.8019999999999999,
            0.7639999999999999,
            0.7999999999999998,
            0.8039999999999998,
            0.45
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.9956666666666667,
          "std": 0.006368324391514238,
          "min": 0.9840000000000001,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            0.99,
            1.0,
            1.0,
            0.9840000000000001
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.546,
          "std": 0.2894546596619236,
          "min": 0.22,
          "max": 0.99,
          "median": 0.49,
          "values": [
            0.22,
            0.28,
            0.49,
            0.75,
            0.99
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.693,
          "std": 0.10538500842150175,
          "min": 0.545,
          "max": 0.845,
          "median": 0.69,
          "values": [
            0.545,
            0.62,
            0.69,
            0.765,
            0.845
          ]
        }
      }
    },
    "u4b-llama3.2-1b": {
      "num_experiments": 46,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3.2-1b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__u4b-llama3.2-1b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3.2-1b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.2-1b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__u4b-llama3.2-1b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3.2-1b__zero-shot__gmeg_basic__100__0p100",
        "gmeg_paper__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_paper",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.03434782608695652,
          "std": 0.12307036117450021,
          "min": 0.0,
          "max": 0.62,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.04,
            0.04,
            0.0,
            0.04,
            0.08,
            0.62,
            0.04,
            0.04,
            0.04,
            0.05,
            0.59
          ]
        },
        "precision": {
          "mean": 0.0929361966114726,
          "std": 0.1285403483666344,
          "min": 0.00016877937615724257,
          "max": 0.645,
          "median": 0.04,
          "values": [
            0.017889339826839826,
            0.1575692080430046,
            0.21263816083201761,
            0.14005320774344646,
            0.009739789776005309,
            0.0031231891411951083,
            0.0039019365517186364,
            0.00569389276392284,
            0.0055592669872044235,
            0.029972077692139765,
            0.029267218060280285,
            0.031800801952624004,
            0.027251699997913034,
            0.025944357927579927,
            0.010596463102426823,
            0.004456594705640506,
            0.005083942634255996,
            0.00624003478957561,
            0.01796684672017075,
            0.012120390341419882,
            0.12267641724595217,
            0.11263724615167627,
            0.11468414071289271,
            0.1896884999747779,
            0.11732151619218938,
            0.11887180013745242,
            0.1144162082803993,
            0.14725606766815846,
            0.22993602607248176,
            0.18224464488739073,
            0.08999826368648688,
            0.08418611247134875,
            0.09777369094408789,
            0.09094990802899909,
            0.0881373027079068,
            0.04,
            0.04,
            0.00016877937615724257,
            0.04,
            0.08125,
            0.645,
            0.04,
            0.04,
            0.04,
            0.05,
            0.601
          ]
        },
        "recall": {
          "mean": 0.27999673822973187,
          "std": 0.15446971950303823,
          "min": 0.02,
          "max": 0.67,
          "median": 0.2975,
          "values": [
            0.07416666666666666,
            0.160118435573066,
            0.258322573271235,
            0.3776197880578145,
            0.51,
            0.23,
            0.22,
            0.34,
            0.37,
            0.20926401049930457,
            0.247400302312067,
            0.3685907785025432,
            0.3883164858017799,
            0.28173212898212896,
            0.2833333333333333,
            0.335,
            0.34,
            0.405,
            0.475,
            0.31166666666666665,
            0.34810781854230255,
            0.3708385351724781,
            0.35153406303252865,
            0.43879755377743423,
            0.3420798601238221,
            0.3577549998994147,
            0.3387622980194823,
            0.3874905023614697,
            0.3856810967414349,
            0.40707478148907894,
            0.27067536678929693,
            0.2586684277600881,
            0.2784053754524449,
            0.19459645234727632,
            0.27385165739251116,
            0.04,
            0.04,
            0.02,
            0.04,
            0.09,
            0.67,
            0.04,
            0.04,
            0.04,
            0.05,
            0.62
          ]
        },
        "f1_score": {
          "mean": 0.11389346934865399,
          "std": 0.13413679065126724,
          "min": 0.00033473389355742297,
          "max": 0.6533333333333333,
          "median": 0.04970905770368958,
          "values": [
            0.027241593047475404,
            0.13946256212663946,
            0.22402495042812845,
            0.19510097900672346,
            0.019050075906730475,
            0.006159422614456698,
            0.007605957379241352,
            0.011139450081063778,
            0.01094425530777698,
            0.04631395111031909,
            0.04864732050897201,
            0.05553081088872693,
            0.049418115407379164,
            0.0441087072533744,
            0.020271237832488128,
            0.008781695072649216,
            0.009984408741376986,
            0.01226328848181181,
            0.034374142890135184,
            0.02315734635492113,
            0.17635583715970152,
            0.1692936614355088,
            0.168990563731321,
            0.25720573009007575,
            0.16813189049462218,
            0.17490467140552698,
            0.16756314639097386,
            0.1973631081199648,
            0.2702972475885435,
            0.23807440498595006,
            0.12721664086706144,
            0.12029279523579862,
            0.1382949847372993,
            0.11468630378624185,
            0.12580652896847477,
            0.04,
            0.04,
            0.00033473389355742297,
            0.04,
            0.08222222222222221,
            0.6533333333333333,
            0.04,
            0.04,
            0.04,
            0.05,
            0.6051515151515151
          ]
        },
        "jaccard": {
          "mean": 0.07904716986958621,
          "std": 0.12387713789999852,
          "min": 0.00016877937615724257,
          "max": 0.645,
          "median": 0.04,
          "values": [
            0.015567133520074695,
            0.07837163580719397,
            0.12987108632341812,
            0.11014193094810602,
            0.009739789776005309,
            0.0031231891411951083,
            0.0039019365517186364,
            0.00569389276392284,
            0.0055592669872044235,
            0.024981592147678998,
            0.025602282422562926,
            0.029320485292624158,
            0.025848739188611792,
            0.02329791715520711,
            0.010506532081025269,
            0.004442679345564411,
            0.00507128939950518,
            0.0062284589483395745,
            0.017903803683989073,
            0.012029112390921175,
            0.09760465403467201,
            0.09312802898286413,
            0.09307741938536879,
            0.14951504201669757,
            0.09268273828787235,
            0.09646742000800676,
            0.09209594482762284,
            0.1128273650124004,
            0.16312343186193573,
            0.13944884498651247,
            0.0695525443532205,
            0.0652950164091048,
            0.07593057669509175,
            0.06226144043722152,
            0.0685378134513481,
            0.04,
            0.04,
            0.00016877937615724257,
            0.04,
            0.08125,
            0.645,
            0.04,
            0.04,
            0.04,
            0.05,
            0.601
          ]
        },
        "semantic_similarity": {
          "mean": 0.4546888041677478,
          "std": 0.1821913584584839,
          "min": 0.06017413009889424,
          "max": 0.8582887753285467,
          "median": 0.4704778587049805,
          "values": [
            0.48915969584137203,
            0.4500850836187601,
            0.5673414608836174,
            0.6150726193189621,
            0.09544986858265475,
            0.07076871836092323,
            0.06017413009889424,
            0.1094636170216836,
            0.10359871821478009,
            0.2561993674002588,
            0.38778922430239615,
            0.47987603928893807,
            0.44903883112594484,
            0.34247431508963927,
            0.35456543466076257,
            0.33140153259038924,
            0.28721057476475836,
            0.31469399413093924,
            0.43138746194541455,
            0.37241387432906775,
            0.5743074625730514,
            0.4907579793035984,
            0.5240909847617149,
            0.710413635969162,
            0.4277626851759851,
            0.5211908048391343,
            0.4970189430564642,
            0.566499483268708,
            0.6743487492203712,
            0.6837376776337624,
            0.42756740694865586,
            0.4610796781210229,
            0.3811633627023548,
            0.4205879895738326,
            0.45940774837508797,
            0.5696539521217346,
            0.5696539521217346,
            0.25781503386795523,
            0.5696539521217346,
            0.5798214510083198,
            0.8582887753285467,
            0.5696539521217346,
            0.5696539521217346,
            0.5696539521217346,
            0.568500058054924,
            0.8452368076331913
          ]
        },
        "answer_correctness": {
          "mean": 0.2504347826086956,
          "std": 0.2043968488838524,
          "min": 0.04,
          "max": 0.73,
          "median": 0.24,
          "values": [
            0.46,
            0.36,
            0.29,
            0.36,
            0.44,
            0.3,
            0.28,
            0.24,
            0.18,
            0.22,
            0.51,
            0.26,
            0.04,
            0.04,
            0.07,
            0.04,
            0.09,
            0.73,
            0.04,
            0.04,
            0.04,
            0.05,
            0.68
          ]
        },
        "response_conciseness": {
          "mean": 0.665,
          "std": 0.0,
          "min": 0.665,
          "max": 0.665,
          "median": 0.665,
          "values": [
            0.665
          ]
        },
        "instruction_following_penalty": {
          "mean": 0.9940000000000001,
          "std": 0.0,
          "min": 0.9940000000000001,
          "max": 0.9940000000000001,
          "median": 0.9940000000000001,
          "values": [
            0.9940000000000001
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.615,
          "std": 0.0,
          "min": 0.615,
          "max": 0.615,
          "median": 0.615,
          "values": [
            0.615
          ]
        },
        "explanation_quality": {
          "mean": 0.506,
          "std": 0.0,
          "min": 0.506,
          "max": 0.506,
          "median": 0.506,
          "values": [
            0.506
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.6885,
          "std": 0.2555,
          "min": 0.433,
          "max": 0.9440000000000001,
          "median": 0.6885,
          "values": [
            0.9440000000000001,
            0.433
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "follows_format_instruction": {
          "mean": 0.15145454545454548,
          "std": 0.04123626743615525,
          "min": 0.09799999999999998,
          "max": 0.21300000000000005,
          "median": 0.16599999999999998,
          "values": [
            0.16999999999999996,
            0.11299999999999998,
            0.16599999999999998,
            0.18699999999999994,
            0.13799999999999998,
            0.18600000000000008,
            0.09799999999999998,
            0.09799999999999998,
            0.10099999999999998,
            0.1960000000000001,
            0.21300000000000005
          ]
        },
        "answer_extractability": {
          "mean": 0.7408545454545454,
          "std": 0.1224713574205404,
          "min": 0.48800000000000004,
          "max": 0.8846000000000003,
          "median": 0.758,
          "values": [
            0.836,
            0.48800000000000004,
            0.758,
            0.7447999999999999,
            0.637,
            0.8846000000000003,
            0.7006,
            0.5879000000000001,
            0.7683000000000001,
            0.8757,
            0.8685
          ]
        },
        "response_brevity": {
          "mean": 0.44139999999999996,
          "std": 0.17299549127072641,
          "min": 0.17500000000000007,
          "max": 0.6809999999999999,
          "median": 0.44799999999999995,
          "values": [
            0.6809999999999999,
            0.5539999999999999,
            0.349,
            0.17500000000000007,
            0.44799999999999995
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8858,
          "std": 0.04636442601823084,
          "min": 0.8035,
          "max": 0.9294999999999999,
          "median": 0.8980000000000001,
          "values": [
            0.927,
            0.9294999999999999,
            0.8709999999999998,
            0.8035,
            0.8980000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.324,
          "std": 0.185101053481605,
          "min": 0.06,
          "max": 0.602,
          "median": 0.32599999999999996,
          "values": [
            0.602,
            0.42599999999999993,
            0.20600000000000002,
            0.06,
            0.32599999999999996
          ]
        },
        "all_choices_addressed": {
          "mean": 0.31428571428571433,
          "std": 0.14876416070480178,
          "min": 0.2,
          "max": 0.674,
          "median": 0.264,
          "values": [
            0.256,
            0.28,
            0.2,
            0.674,
            0.256,
            0.264,
            0.27
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9874285714285714,
          "std": 0.029586952447016913,
          "min": 0.915,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0,
            1.0,
            0.915,
            1.0,
            0.997,
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.30350000000000005,
          "std": 0.07125345104424263,
          "min": 0.22880000000000003,
          "max": 0.4108,
          "median": 0.26360000000000006,
          "values": [
            0.4108,
            0.338,
            0.25370000000000004,
            0.39519999999999994,
            0.22880000000000003,
            0.23440000000000008,
            0.26360000000000006
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.7106666666666668,
          "std": 0.0,
          "min": 0.7106666666666668,
          "max": 0.7106666666666668,
          "median": 0.7106666666666668,
          "values": [
            0.7106666666666668
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7465775691585677,
          "std": 0.0,
          "min": 0.7465775691585677,
          "max": 0.7465775691585677,
          "median": 0.7465775691585677,
          "values": [
            0.7465775691585677
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.521475,
          "std": 0.09923333333333337,
          "min": 0.4222416666666667,
          "max": 0.6207083333333334,
          "median": 0.521475,
          "values": [
            0.4222416666666667,
            0.6207083333333334
          ]
        },
        "correct_answer_focus": {
          "mean": 0.784,
          "std": 0.13800000000000007,
          "min": 0.6459999999999999,
          "max": 0.922,
          "median": 0.784,
          "values": [
            0.922,
            0.6459999999999999
          ]
        },
        "concise_justification": {
          "mean": 0.8444499999999999,
          "std": 0.0032500000000000306,
          "min": 0.8412,
          "max": 0.8477,
          "median": 0.8444499999999999,
          "values": [
            0.8477,
            0.8412
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5954166666666665,
          "std": 0.1565488253620008,
          "min": 1.3329999999999997,
          "max": 1.745333333333333,
          "median": 1.6516666666666664,
          "values": [
            1.745333333333333,
            1.6381666666666665,
            1.3329999999999997,
            1.6651666666666665
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6266666666666666,
          "std": 0.10067997980399745,
          "min": 0.5225,
          "max": 0.7875,
          "median": 0.5983333333333333,
          "values": [
            0.6316666666666666,
            0.5225,
            0.565,
            0.7875
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91,
            0.91,
            0.91,
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.4202779541490975,
          "std": 0.42731477511213073,
          "min": 0.0,
          "max": 0.949774572320238,
          "median": 0.36566862213807605,
          "values": [
            0.0,
            0.0,
            0.949774572320238,
            0.7313372442761521
          ]
        },
        "strict_format_compliance": {
          "mean": 0.8006666666666667,
          "std": 0.3045645343035783,
          "min": 0.16,
          "max": 1.0,
          "median": 0.968,
          "values": [
            1.0,
            1.0,
            0.16,
            1.0,
            0.9359999999999999,
            0.7079999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.8843333333333335,
          "std": 0.21187548754450625,
          "min": 0.414,
          "max": 1.0,
          "median": 0.98,
          "values": [
            1.0,
            1.0,
            0.414,
            1.0,
            0.96,
            0.9319999999999999
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.236,
          "std": 0.38207852596030567,
          "min": 0.04,
          "max": 1.0,
          "median": 0.04,
          "values": [
            0.04,
            0.04,
            0.04,
            0.06,
            1.0
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.536,
          "std": 0.07200000000000002,
          "min": 0.5,
          "max": 0.68,
          "median": 0.5,
          "values": [
            0.5,
            0.5,
            0.5,
            0.5,
            0.68
          ]
        }
      }
    },
    "u4b-llama3.3-70b": {
      "num_experiments": 46,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3.3-70b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-llama3.3-70b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.3-70b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_perspective__100__0p100",
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__u4b-llama3.3-70b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-llama3.3-70b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-llama3.3-70b__zero-shot__gmeg_basic__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.004130434782608695,
          "std": 0.019289114632072283,
          "min": 0.0,
          "max": 0.13,
          "median": 0.0,
          "values": [
            0.13,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.02,
            0.01,
            0.02,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.01,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.07029589761650056,
          "std": 0.07091844382852738,
          "min": 0.0,
          "max": 0.28863006709410444,
          "median": 0.034630944333923414,
          "values": [
            0.200942016740964,
            0.17232543568044964,
            0.205745266140876,
            0.11423420907346031,
            0.021432659648304653,
            0.008092042376722132,
            0.02470466625104554,
            0.016072413841084866,
            0.029661899198626177,
            0.036954425637616285,
            0.028358256961141675,
            0.03178070791227817,
            0.026071979794883696,
            0.02803827301960644,
            0.04550674217708795,
            0.010897709452009716,
            0.015900449904138577,
            0.013395739682222275,
            0.03487299022950155,
            0.03438889843834528,
            0.10190939493255025,
            0.09863518069947419,
            0.0999787479998068,
            0.1628305079509147,
            0.0938173781271812,
            0.09313842837060643,
            0.10680542256256137,
            0.09667820560075438,
            0.1900079193383912,
            0.28863006709410444,
            0.21777957840974974,
            0.1285366835995709,
            0.10859725201986996,
            0.1155949405089411,
            0.1241289540300264,
            0.00010752688172043012,
            0.003968253968253968,
            0.0,
            0.00588113777212776,
            0.009107142857142855,
            0.044075493950493955,
            0.0051536861138175545,
            0.00870145948314553,
            0.01151375664086748,
            0.00753968253968254,
            0.011117706746905244
          ]
        },
        "recall": {
          "mean": 0.39102133628332786,
          "std": 0.2017423842532439,
          "min": 0.0,
          "max": 0.92,
          "median": 0.3843968151624297,
          "values": [
            0.5925,
            0.30343662607300603,
            0.29189238677750157,
            0.4325517097370235,
            0.92,
            0.81,
            0.49,
            0.65,
            0.51,
            0.4921035696329814,
            0.44613055082172726,
            0.48813311361840783,
            0.44494033417562834,
            0.4213058118352236,
            0.54,
            0.615,
            0.595,
            0.675,
            0.585,
            0.57,
            0.3924597192007664,
            0.3752638393508984,
            0.376034351919289,
            0.502578757763266,
            0.3646792588837652,
            0.36144394533247004,
            0.376333911124093,
            0.36825110682821893,
            0.4785876572797395,
            0.37233378418672053,
            0.40589122920410575,
            0.3712485230943114,
            0.34937610571185396,
            0.41224016301355076,
            0.3172650134685309,
            0.01,
            0.03,
            0.0,
            0.06,
            0.06,
            0.32,
            0.18,
            0.12,
            0.18,
            0.05,
            0.28
          ]
        },
        "f1_score": {
          "mean": 0.0975874720716177,
          "std": 0.08463685141917718,
          "min": 0.0,
          "max": 0.2804271626322052,
          "median": 0.058807527405236426,
          "values": [
            0.233500611191643,
            0.20334576845697389,
            0.21782890538768335,
            0.17254248915981138,
            0.03951849233226848,
            0.01601538833777067,
            0.029313988628386235,
            0.022026359114911825,
            0.03762034093377349,
            0.06693846813737894,
            0.052499853913394796,
            0.05890285424088801,
            0.04867357754273953,
            0.05191380799356806,
            0.06863770129462438,
            0.020587236977757863,
            0.028667053839553005,
            0.024727815174296598,
            0.05871220056958485,
            0.05706266295843628,
            0.1590321106210627,
            0.15344931376506538,
            0.1552280589481312,
            0.24144336041957373,
            0.14688288237361224,
            0.14564836214695653,
            0.16336908081235735,
            0.15046952434153799,
            0.2540873034091251,
            0.2804271626322052,
            0.2735406292840013,
            0.1826537246445808,
            0.15933894110014646,
            0.16670877671799855,
            0.15963878152546848,
            0.00021276595744680854,
            0.006999999999999999,
            0.0,
            0.01033094017094017,
            0.01579365079365079,
            0.07575177749270576,
            0.009632945747052552,
            0.01572257991018602,
            0.020767816951360228,
            0.013071428571428569,
            0.019786220772375525
          ]
        },
        "jaccard": {
          "mean": 0.05691349292448766,
          "std": 0.051857880060275724,
          "min": 0.0,
          "max": 0.19509040973011188,
          "median": 0.03195063811121151,
          "values": [
            0.19509040973011188,
            0.11565473176818783,
            0.12570849113518245,
            0.09609102135918876,
            0.021432659648304653,
            0.008092042376722132,
            0.02470466625104554,
            0.016072413841084866,
            0.029661899198626177,
            0.03562790305928242,
            0.027414583850661388,
            0.030851910116611946,
            0.025276762124913837,
            0.02711375677050899,
            0.04336012654998884,
            0.01063337095222363,
            0.01513401512745671,
            0.012910120331113056,
            0.03216381606537508,
            0.03173746015704794,
            0.08694414786914884,
            0.08360831973868428,
            0.08461068836630405,
            0.13860822528341799,
            0.07967973503882565,
            0.07902866750474985,
            0.08971715709668075,
            0.08188738578831059,
            0.14906515089190975,
            0.17314617116284642,
            0.16312967489327948,
            0.10369909564428108,
            0.08878555714359121,
            0.09391348107290702,
            0.09029920966369975,
            0.00010752688172043012,
            0.003968253968253968,
            0.0,
            0.00588113777212776,
            0.009107142857142855,
            0.044075493950493955,
            0.0051536861138175545,
            0.00870145948314553,
            0.01151375664086748,
            0.00753968253968254,
            0.011117706746905244
          ]
        },
        "semantic_similarity": {
          "mean": 0.4713093212251957,
          "std": 0.15603902464251618,
          "min": 0.10301524182315916,
          "max": 0.7479457410424948,
          "median": 0.5142473728116601,
          "values": [
            0.56256989993155,
            0.5585999847948551,
            0.5475268509984016,
            0.557304115742445,
            0.18492254968732597,
            0.11910263258963823,
            0.10996505566872657,
            0.10301524182315916,
            0.11566578415920958,
            0.43876771464943887,
            0.5359494306147099,
            0.5241868364065886,
            0.4558450558036566,
            0.4574032038450241,
            0.5132720082998276,
            0.43076386410743,
            0.4604719139635563,
            0.4487741879373789,
            0.5449167729914188,
            0.5219494701176882,
            0.6231924968957901,
            0.445855346173048,
            0.5808199259638787,
            0.7214969116449356,
            0.4440675215423107,
            0.530372180789709,
            0.572981809079647,
            0.5192889314889908,
            0.5594649736583233,
            0.6046197773516178,
            0.6960352997481823,
            0.5686166930478066,
            0.5469943140354008,
            0.5408527564443648,
            0.4718954822793603,
            0.3501268028933555,
            0.4005956416297704,
            0.44549427420832216,
            0.41776324043050406,
            0.7479457410424948,
            0.5152227373234928,
            0.3890067627164535,
            0.38472949964227154,
            0.4576311853691004,
            0.7455847392976284,
            0.2086011575302109
          ]
        },
        "answer_correctness": {
          "mean": 0.673913043478261,
          "std": 0.13123954634758475,
          "min": 0.46,
          "max": 0.9,
          "median": 0.65,
          "values": [
            0.81,
            0.58,
            0.53,
            0.47,
            0.46,
            0.48,
            0.64,
            0.64,
            0.65,
            0.59,
            0.77,
            0.72,
            0.5,
            0.64,
            0.79,
            0.6,
            0.9,
            0.87,
            0.68,
            0.75,
            0.83,
            0.8,
            0.8
          ]
        },
        "response_conciseness": {
          "mean": 0.523,
          "std": 0.0,
          "min": 0.523,
          "max": 0.523,
          "median": 0.523,
          "values": [
            0.523
          ]
        },
        "instruction_following_penalty": {
          "mean": 0.8059999999999999,
          "std": 0.0,
          "min": 0.8059999999999999,
          "max": 0.8059999999999999,
          "median": 0.8059999999999999,
          "values": [
            0.8059999999999999
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.705,
          "std": 0.0,
          "min": 0.705,
          "max": 0.705,
          "median": 0.705,
          "values": [
            0.705
          ]
        },
        "explanation_quality": {
          "mean": 0.828,
          "std": 0.0,
          "min": 0.828,
          "max": 0.828,
          "median": 0.828,
          "values": [
            0.828
          ]
        },
        "response_cleanliness": {
          "mean": 0.9430000000000001,
          "std": 0.0,
          "min": 0.9430000000000001,
          "max": 0.9430000000000001,
          "median": 0.9430000000000001,
          "values": [
            0.9430000000000001
          ]
        },
        "explanation_correctness": {
          "mean": 0.9884999999999999,
          "std": 0.01150000000000001,
          "min": 0.977,
          "max": 1.0,
          "median": 0.9884999999999999,
          "values": [
            0.977,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.476,
          "std": 0.281,
          "min": 0.195,
          "max": 0.757,
          "median": 0.476,
          "values": [
            0.757,
            0.195
          ]
        },
        "format_cleanliness": {
          "mean": 0.8625,
          "std": 0.06650000000000006,
          "min": 0.7959999999999999,
          "max": 0.929,
          "median": 0.8625,
          "values": [
            0.929,
            0.7959999999999999
          ]
        },
        "follows_format_instruction": {
          "mean": 0.2351818181818182,
          "std": 0.07620166805835706,
          "min": 0.12,
          "max": 0.36599999999999994,
          "median": 0.21599999999999994,
          "values": [
            0.22099999999999997,
            0.19999999999999996,
            0.21599999999999994,
            0.20799999999999996,
            0.23299999999999998,
            0.36599999999999994,
            0.12,
            0.20300000000000007,
            0.14300000000000002,
            0.3440000000000001,
            0.333
          ]
        },
        "answer_extractability": {
          "mean": 0.7765363636363637,
          "std": 0.05994155142117075,
          "min": 0.6979,
          "max": 0.8719999999999999,
          "median": 0.7589000000000001,
          "values": [
            0.8719999999999999,
            0.8539999999999999,
            0.8259999999999998,
            0.7899999999999998,
            0.8439999999999999,
            0.6979,
            0.7589000000000001,
            0.7271,
            0.7260999999999999,
            0.7243999999999998,
            0.7214999999999999
          ]
        },
        "response_brevity": {
          "mean": 0.18960000000000005,
          "std": 0.03725372464600017,
          "min": 0.132,
          "max": 0.24200000000000002,
          "median": 0.19800000000000004,
          "values": [
            0.20800000000000007,
            0.19800000000000004,
            0.16800000000000007,
            0.132,
            0.24200000000000002
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8545000000000001,
          "std": 0.028227646023003754,
          "min": 0.8155000000000002,
          "max": 0.8935000000000001,
          "median": 0.8655000000000002,
          "values": [
            0.8295,
            0.8655000000000002,
            0.8685000000000003,
            0.8155000000000002,
            0.8935000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.1068,
          "std": 0.04399272667157607,
          "min": 0.04800000000000001,
          "max": 0.168,
          "median": 0.12,
          "values": [
            0.132,
            0.12,
            0.066,
            0.04800000000000001,
            0.168
          ]
        },
        "all_choices_addressed": {
          "mean": 0.358,
          "std": 0.2363535487357869,
          "min": 0.236,
          "max": 0.98,
          "median": 0.265,
          "values": [
            0.268,
            0.31600000000000006,
            0.29600000000000004,
            0.98,
            0.244,
            0.236,
            0.262,
            0.262
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9581249999999999,
          "std": 0.044038158169932624,
          "min": 0.861,
          "max": 1.0,
          "median": 0.971,
          "values": [
            0.9940000000000001,
            0.982,
            0.9940000000000001,
            0.861,
            1.0,
            0.9249999999999998,
            0.96,
            0.9490000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.3324750000000001,
          "std": 0.1037360924413485,
          "min": 0.21860000000000002,
          "max": 0.5681,
          "median": 0.3205,
          "values": [
            0.3849,
            0.33760000000000007,
            0.317,
            0.5681,
            0.21980000000000005,
            0.21860000000000002,
            0.324,
            0.28980000000000006
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9475,
          "std": 0.0,
          "min": 0.9475,
          "max": 0.9475,
          "median": 0.9475,
          "values": [
            0.9475
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.9057197691884995,
          "std": 0.0,
          "min": 0.9057197691884995,
          "max": 0.9057197691884995,
          "median": 0.9057197691884995,
          "values": [
            0.9057197691884995
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.8330974803382893,
          "std": 0.07964887747056865,
          "min": 0.7534486028677206,
          "max": 0.9127463578088579,
          "median": 0.8330974803382893,
          "values": [
            0.7534486028677206,
            0.9127463578088579
          ]
        },
        "correct_answer_focus": {
          "mean": 0.6675,
          "std": 0.020999999999999908,
          "min": 0.6465000000000001,
          "max": 0.6884999999999999,
          "median": 0.6675,
          "values": [
            0.6884999999999999,
            0.6465000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.8392499999999998,
          "std": 0.05475000000000013,
          "min": 0.7844999999999998,
          "max": 0.894,
          "median": 0.8392499999999998,
          "values": [
            0.7844999999999998,
            0.894
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.338125,
          "std": 0.26839275674838914,
          "min": 0.8734999999999999,
          "max": 1.5045,
          "median": 1.48725,
          "values": [
            1.48,
            1.4945,
            0.8734999999999999,
            1.5045
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7635416666666667,
          "std": 0.04672011448211811,
          "min": 0.7116666666666666,
          "max": 0.8383333333333333,
          "median": 0.7520833333333333,
          "values": [
            0.7425,
            0.7116666666666666,
            0.7616666666666666,
            0.8383333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.8400000000000001,
          "std": 0.11,
          "min": 0.65,
          "max": 0.91,
          "median": 0.9,
          "values": [
            0.91,
            0.91,
            0.65,
            0.89
          ]
        },
        "original_text_mention": {
          "mean": 0.2945741338037383,
          "std": 0.3112599351571956,
          "min": 0.0,
          "max": 0.7313370641270258,
          "median": 0.2234797355439637,
          "values": [
            0.0,
            0.0,
            0.7313370641270258,
            0.4469594710879274
          ]
        },
        "strict_format_compliance": {
          "mean": 0.1545,
          "std": 0.09716094894555118,
          "min": 0.030999999999999996,
          "max": 0.34600000000000003,
          "median": 0.1395,
          "values": [
            0.030999999999999996,
            0.102,
            0.165,
            0.16899999999999998,
            0.34600000000000003,
            0.114
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.47,
          "std": 0.16422342504445991,
          "min": 0.30199999999999994,
          "max": 0.8180000000000001,
          "median": 0.428,
          "values": [
            0.30199999999999994,
            0.38199999999999995,
            0.40399999999999997,
            0.462,
            0.8180000000000001,
            0.452
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.924,
          "std": 0.05642694391866354,
          "min": 0.83,
          "max": 0.98,
          "median": 0.95,
          "values": [
            0.83,
            0.89,
            0.97,
            0.98,
            0.95
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.8139999999999998,
          "std": 0.022891046284519174,
          "min": 0.775,
          "max": 0.845,
          "median": 0.815,
          "values": [
            0.775,
            0.815,
            0.845,
            0.81,
            0.825
          ]
        }
      }
    },
    "u4b-mistral-7b": {
      "num_experiments": 46,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-mistral-7b__zero-shot__cose_choice_selection__100__0p100",
        "RHAI_cose_explanation__u4b-mistral-7b__zero-shot__cose_explanation__100__0p100",
        "RHAI_cose_explanation_with_answer__u4b-mistral-7b__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__u4b-mistral-7b__zero-shot__cose_self_rationalization__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_medical__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100",
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100",
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_choice_selection__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_senior_european__100__0p100",
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_young_american__100__0p100",
        "ecqa_choice_selection_3choices__u4b-mistral-7b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
        "ecqa_choice_selection_no_concept__u4b-mistral-7b__zero-shot__ecqa_choice_selection_no_concept__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_anthropologist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_psychologist__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_freeflow_explanation__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_parent__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_perspective__100__0p100",
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_small_business__100__0p100",
        "ecqa_negative__u4b-mistral-7b__zero-shot__ecqa_negative_explanation__100__0p100",
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation__100__0p100",
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
        "gmeg__u4b-mistral-7b__few-shot-456__gmeg_few_shot__100__0p100",
        "gmeg__u4b-mistral-7b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_ours__u4b-mistral-7b__few-shot-456__gmeg_few_shot_original__100__0p100",
        "gmeg_ours__u4b-mistral-7b__zero-shot__gmeg_basic__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_impersonation__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free__100__0p100",
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free_min__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required__100__0p100",
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept",
        "ecare_choice_selection",
        "ecare_explanation_generation",
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation",
        "ecqa_positive",
        "gmeg",
        "gmeg_ours",
        "RHAI_cose_explanation_with_answer",
        "pubmedqa_reasoning_required",
        "ecqa_freeflow",
        "pubmedqa_reasoning_free",
        "ecqa_choice",
        "RHAI_cose_choice_selection",
        "ecqa_choice_selection_3choices",
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.07195652173913045,
          "std": 0.13769246711765018,
          "min": 0.0,
          "max": 0.68,
          "median": 0.0,
          "values": [
            0.18,
            0.0,
            0.0,
            0.0,
            0.11,
            0.06,
            0.0,
            0.01,
            0.03,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.24,
            0.05,
            0.05,
            0.04,
            0.43,
            0.38,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.12,
            0.04,
            0.07,
            0.08,
            0.68,
            0.0,
            0.14,
            0.12,
            0.12,
            0.36,
            0.0
          ]
        },
        "precision": {
          "mean": 0.134945946489059,
          "std": 0.12645226107880772,
          "min": 0.0008062354312354311,
          "max": 0.6802380952380952,
          "median": 0.10699909877183719,
          "values": [
            0.19582437932437935,
            0.19782797322544143,
            0.25302236360721825,
            0.1445024106698146,
            0.13377257475840934,
            0.08278881113019149,
            0.02402122648029591,
            0.02139867556280669,
            0.04214882710953342,
            0.048265080545658645,
            0.03471821090973017,
            0.03246709410402508,
            0.02929200460140548,
            0.02496805721240127,
            0.2597808004396367,
            0.058802392947091234,
            0.06235942907808319,
            0.05081169626119044,
            0.4432781474748814,
            0.3943153894018689,
            0.10796416233006301,
            0.10595046933584781,
            0.10603403521361136,
            0.1773530384609687,
            0.09663565079795029,
            0.09606231894344798,
            0.11493750108264075,
            0.09512191375148955,
            0.19569829617799822,
            0.247546451733437,
            0.15928865884265464,
            0.11578537894240647,
            0.09588343999711746,
            0.1102378207457716,
            0.11292639877345302,
            0.12042107027770758,
            0.04054884172345928,
            0.07012048192771085,
            0.08040330023435047,
            0.6802380952380952,
            0.0014495515649701357,
            0.14033545008912657,
            0.12039618406285073,
            0.12043249967496779,
            0.3605707482993197,
            0.0008062354312354311
          ]
        },
        "recall": {
          "mean": 0.39680062135312394,
          "std": 0.19071333040180363,
          "min": 0.04,
          "max": 0.76,
          "median": 0.38657819096291113,
          "values": [
            0.23916666666666664,
            0.22221080322289843,
            0.28141934071330515,
            0.4267943266174317,
            0.75,
            0.76,
            0.72,
            0.72,
            0.69,
            0.49142961287078935,
            0.5147183143653732,
            0.4878397125750067,
            0.467410169895464,
            0.425678641619818,
            0.5,
            0.45,
            0.58,
            0.4666666666666666,
            0.655,
            0.56,
            0.37469508821511915,
            0.3831563819258223,
            0.3727890177015011,
            0.4797364105036326,
            0.36062526754018537,
            0.3615476046966404,
            0.38172674889351305,
            0.35082479271463884,
            0.4721137596409772,
            0.35424753913137935,
            0.43978218040815426,
            0.39691623353839034,
            0.38020979066697336,
            0.34835267746018006,
            0.2977708339931752,
            0.17,
            0.09,
            0.08,
            0.11,
            0.69,
            0.05,
            0.17,
            0.15,
            0.15,
            0.39,
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.16040382864163522,
          "std": 0.12544711673219267,
          "min": 0.0015774319688448113,
          "max": 0.6804651162790698,
          "median": 0.14799838722557807,
          "values": [
            0.19862846370718848,
            0.19313490438109585,
            0.25214153009130524,
            0.2079784619571892,
            0.1530097313245453,
            0.10109790316320959,
            0.042532834690145994,
            0.0324165497585378,
            0.05364486570022528,
            0.0862324113979467,
            0.0642778376021098,
            0.060162972182120834,
            0.054526152588432734,
            0.04675197749346605,
            0.2708388738163502,
            0.06717507399267747,
            0.07405976287381924,
            0.06100051377676524,
            0.4534595060833736,
            0.4045523737232297,
            0.16456267846367328,
            0.16295950670669915,
            0.1619262734177716,
            0.25292745905479747,
            0.14971180719485916,
            0.14924523587796787,
            0.17338333336265654,
            0.14675153857318823,
            0.2602851734506764,
            0.28001469433145515,
            0.22667320491504128,
            0.1707729036132271,
            0.1461540075876452,
            0.15953902044230595,
            0.1548593250861159,
            0.1208350931653932,
            0.041084690612792414,
            0.07023809523809524,
            0.08079586285013975,
            0.6804651162790698,
            0.0027728824847468914,
            0.14066346280766878,
            0.12078200283627971,
            0.12085259117517183,
            0.3611200257152041,
            0.0015774319688448113
          ]
        },
        "jaccard": {
          "mean": 0.12169034156002873,
          "std": 0.12408802010452084,
          "min": 0.0008062354312354311,
          "max": 0.6802380952380952,
          "median": 0.08883414819365212,
          "values": [
            0.1909575746314877,
            0.11113775093441064,
            0.14976971623070223,
            0.11794179670312226,
            0.13377257475840934,
            0.08278881113019149,
            0.02402122648029591,
            0.02139867556280669,
            0.04214882710953342,
            0.04608547698920304,
            0.03374377868076859,
            0.031521197770612505,
            0.02844595563828819,
            0.024265730515130007,
            0.25772324526474283,
            0.05877274591003262,
            0.06230743797721426,
            0.050740899211527586,
            0.4430084735608898,
            0.39381479792301577,
            0.09031078948217973,
            0.08929612684782043,
            0.08866048842313738,
            0.146148187211815,
            0.08139133737419964,
            0.08116362604542336,
            0.09552494270400932,
            0.07969752174909459,
            0.15175693650396982,
            0.17102613728874708,
            0.13051938039109187,
            0.09621973620240214,
            0.08080910737350303,
            0.08900780796416687,
            0.0861344346935833,
            0.12042107027770758,
            0.04054884172345928,
            0.07012048192771085,
            0.08040330023435047,
            0.6802380952380952,
            0.0014495515649701357,
            0.14033545008912657,
            0.12039618406285073,
            0.12043249967496779,
            0.3605707482993197,
            0.0008062354312354311
          ]
        },
        "semantic_similarity": {
          "mean": 0.4478371959838652,
          "std": 0.20696381343511014,
          "min": 0.042592798094265166,
          "max": 0.8674150868877768,
          "median": 0.49351937929168344,
          "values": [
            0.6934634025394917,
            0.5622329947352409,
            0.6856044900417327,
            0.6387710550427437,
            0.25062844025669617,
            0.15466225712327286,
            0.07868204438360409,
            0.08075127707328647,
            0.10947090022265911,
            0.5523444086313247,
            0.5751970493793488,
            0.5680893646925688,
            0.5020062896609306,
            0.4963519271463156,
            0.5654132095724345,
            0.4076335646212101,
            0.39791543390601875,
            0.3956282104551792,
            0.6941603437066078,
            0.6547163240611553,
            0.5852298989892006,
            0.49143902480602264,
            0.5422705674171447,
            0.7190728631615638,
            0.42134753093123434,
            0.5147403443604708,
            0.5412483015656471,
            0.47006827145814895,
            0.7106705468893051,
            0.685083657503128,
            0.6958865889906883,
            0.4863689078018069,
            0.49559973377734423,
            0.45694554714486,
            0.4819056508410722,
            0.1547814832930453,
            0.21572590524796398,
            0.23036548116244376,
            0.34886065615341066,
            0.8674150868877768,
            0.045420683547854425,
            0.21163585524074732,
            0.2613967066071928,
            0.3236174924112856,
            0.5370984437223524,
            0.042592798094265166
          ]
        },
        "answer_correctness": {
          "mean": 0.45521739130434785,
          "std": 0.16521052617315915,
          "min": 0.2,
          "max": 0.72,
          "median": 0.47,
          "values": [
            0.69,
            0.58,
            0.5,
            0.48,
            0.49,
            0.47,
            0.49,
            0.37,
            0.42,
            0.33,
            0.72,
            0.65,
            0.2,
            0.26,
            0.38,
            0.35,
            0.72,
            0.67,
            0.21,
            0.24,
            0.22,
            0.43,
            0.6
          ]
        },
        "response_conciseness": {
          "mean": 0.9109999999999999,
          "std": 0.0,
          "min": 0.9109999999999999,
          "max": 0.9109999999999999,
          "median": 0.9109999999999999,
          "values": [
            0.9109999999999999
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "explanation_quality": {
          "mean": 0.7559999999999999,
          "std": 0.0,
          "min": 0.7559999999999999,
          "max": 0.7559999999999999,
          "median": 0.7559999999999999,
          "values": [
            0.7559999999999999
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_correctness": {
          "mean": 0.9550000000000001,
          "std": 0.04499999999999999,
          "min": 0.91,
          "max": 1.0,
          "median": 0.9550000000000001,
          "values": [
            0.91,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.7404999999999999,
          "std": 0.23149999999999998,
          "min": 0.509,
          "max": 0.972,
          "median": 0.7404999999999999,
          "values": [
            0.972,
            0.509
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "follows_format_instruction": {
          "mean": 0.3239999999999999,
          "std": 0.18526590227414902,
          "min": 0.153,
          "max": 0.6529999999999998,
          "median": 0.22699999999999998,
          "values": [
            0.36500000000000016,
            0.274,
            0.206,
            0.19999999999999996,
            0.22699999999999998,
            0.5309999999999999,
            0.153,
            0.158,
            0.158,
            0.639,
            0.6529999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8260181818181817,
          "std": 0.1322233841110759,
          "min": 0.6086,
          "max": 0.9640000000000001,
          "median": 0.8578999999999999,
          "values": [
            0.9619999999999999,
            0.9460000000000001,
            0.9359999999999999,
            0.88,
            0.9640000000000001,
            0.811,
            0.616,
            0.6086,
            0.6476999999999999,
            0.8578999999999999,
            0.8569999999999999
          ]
        },
        "response_brevity": {
          "mean": 0.3134,
          "std": 0.05607708979610118,
          "min": 0.276,
          "max": 0.4249999999999999,
          "median": 0.29,
          "values": [
            0.4249999999999999,
            0.292,
            0.29,
            0.284,
            0.276
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8451000000000001,
          "std": 0.04108576395784801,
          "min": 0.77,
          "max": 0.884,
          "median": 0.8640000000000001,
          "values": [
            0.8640000000000001,
            0.8735,
            0.884,
            0.77,
            0.8339999999999997
          ]
        },
        "single_sentence_format": {
          "mean": 0.10919999999999999,
          "std": 0.13705823579778045,
          "min": 0.012000000000000002,
          "max": 0.37199999999999994,
          "median": 0.03,
          "values": [
            0.37199999999999994,
            0.118,
            0.03,
            0.012000000000000002,
            0.014000000000000002
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3315,
          "std": 0.20425413092517858,
          "min": 0.20799999999999996,
          "max": 0.866,
          "median": 0.268,
          "values": [
            0.212,
            0.28800000000000003,
            0.27,
            0.866,
            0.20799999999999996,
            0.24600000000000002,
            0.29600000000000004,
            0.266
          ]
        },
        "single_paragraph_format": {
          "mean": 0.35850000000000004,
          "std": 0.17670809262736104,
          "min": 0.205,
          "max": 0.725,
          "median": 0.28150000000000003,
          "values": [
            0.3740000000000002,
            0.205,
            0.267,
            0.561,
            0.21599999999999997,
            0.22399999999999995,
            0.725,
            0.29600000000000004
          ]
        },
        "explanation_depth": {
          "mean": 0.3569875,
          "std": 0.07412758321805722,
          "min": 0.26310000000000006,
          "max": 0.49770000000000003,
          "median": 0.34759999999999996,
          "values": [
            0.431,
            0.37829999999999997,
            0.32339999999999997,
            0.49770000000000003,
            0.26720000000000005,
            0.26310000000000006,
            0.3428,
            0.35239999999999994
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9631666666666667,
          "std": 0.0,
          "min": 0.9631666666666667,
          "max": 0.9631666666666667,
          "median": 0.9631666666666667,
          "values": [
            0.9631666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.6698304612054612,
          "std": 0.0,
          "min": 0.6698304612054612,
          "max": 0.6698304612054612,
          "median": 0.6698304612054612,
          "values": [
            0.6698304612054612
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.6177983134920635,
          "std": 0.2263816468253969,
          "min": 0.39141666666666663,
          "max": 0.8441799603174605,
          "median": 0.6177983134920635,
          "values": [
            0.39141666666666663,
            0.8441799603174605
          ]
        },
        "correct_answer_focus": {
          "mean": 0.57375,
          "std": 0.14024999999999999,
          "min": 0.4335,
          "max": 0.714,
          "median": 0.57375,
          "values": [
            0.714,
            0.4335
          ]
        },
        "concise_justification": {
          "mean": 0.7623500000000001,
          "std": 0.08454999999999979,
          "min": 0.6778000000000003,
          "max": 0.8468999999999999,
          "median": 0.7623500000000001,
          "values": [
            0.8468999999999999,
            0.6778000000000003
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.2635416666666666,
          "std": 0.4750231774316093,
          "min": 0.505,
          "max": 1.7301666666666664,
          "median": 1.4095,
          "values": [
            1.7301666666666664,
            1.2266666666666666,
            1.5923333333333334,
            0.505
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.751875,
          "std": 0.06126948386250515,
          "min": 0.6808333333333334,
          "max": 0.8341666666666667,
          "median": 0.7462499999999999,
          "values": [
            0.8341666666666667,
            0.7066666666666666,
            0.6808333333333334,
            0.7858333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.6975,
          "std": 0.24722206616724166,
          "min": 0.3,
          "max": 0.91,
          "median": 0.79,
          "values": [
            0.91,
            0.68,
            0.9,
            0.3
          ]
        },
        "original_text_mention": {
          "mean": 0.2842546219725819,
          "std": 0.28940189952118983,
          "min": 0.0,
          "max": 0.6453567466202336,
          "median": 0.245830870635047,
          "values": [
            0.0,
            0.0,
            0.6453567466202336,
            0.491661741270094
          ]
        },
        "strict_format_compliance": {
          "mean": 0.21666666666666665,
          "std": 0.2424037036754091,
          "min": 0.0,
          "max": 0.7399999999999999,
          "median": 0.12200000000000003,
          "values": [
            0.13200000000000003,
            0.11200000000000002,
            0.10000000000000002,
            0.21599999999999997,
            0.7399999999999999,
            0.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.3173333333333333,
          "std": 0.30446820669634594,
          "min": 0.002,
          "max": 0.9519999999999998,
          "median": 0.2,
          "values": [
            0.18,
            0.22,
            0.17,
            0.38,
            0.9519999999999998,
            0.002
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.41200000000000003,
          "std": 0.1917706964058899,
          "min": 0.24,
          "max": 0.74,
          "median": 0.29,
          "values": [
            0.27,
            0.29,
            0.24,
            0.52,
            0.74
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.619,
          "std": 0.062401923047290773,
          "min": 0.56,
          "max": 0.725,
          "median": 0.58,
          "values": [
            0.56,
            0.575,
            0.58,
            0.655,
            0.725
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_cose_explanation_ft": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-mistral-7b_ft_cose_explanation_ft__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.25560764835925204,
          "std": 0.0,
          "min": 0.25560764835925204,
          "max": 0.25560764835925204,
          "median": 0.25560764835925204,
          "values": [
            0.25560764835925204
          ]
        },
        "recall": {
          "mean": 0.16833561645792386,
          "std": 0.0,
          "min": 0.16833561645792386,
          "max": 0.16833561645792386,
          "median": 0.16833561645792386,
          "values": [
            0.16833561645792386
          ]
        },
        "f1_score": {
          "mean": 0.17768690410135687,
          "std": 0.0,
          "min": 0.17768690410135687,
          "max": 0.17768690410135687,
          "median": 0.17768690410135687,
          "values": [
            0.17768690410135687
          ]
        },
        "jaccard": {
          "mean": 0.10270769847885078,
          "std": 0.0,
          "min": 0.10270769847885078,
          "max": 0.10270769847885078,
          "median": 0.10270769847885078,
          "values": [
            0.10270769847885078
          ]
        },
        "semantic_similarity": {
          "mean": 0.5429833361133933,
          "std": 0.0,
          "min": 0.5429833361133933,
          "max": 0.5429833361133933,
          "median": 0.5429833361133933,
          "values": [
            0.5429833361133933
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.625,
          "std": 0.0,
          "min": 0.625,
          "max": 0.625,
          "median": 0.625,
          "values": [
            0.625
          ]
        },
        "explanation_quality": {
          "mean": 0.506,
          "std": 0.0,
          "min": 0.506,
          "max": 0.506,
          "median": 0.506,
          "values": [
            0.506
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "hf-qwen3-30b-thinking": {
      "num_experiments": 3,
      "experiments": [
        "RHAI_cose_explanation_with_answer__hf-qwen3-30b-thinking__zero-shot__cose_explanation_with_answer__100__0p100",
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen3-30b-thinking__zero-shot__cose_self_rationalization__100__0p100",
        "gmeg__hf-qwen3-30b-thinking__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions",
        "RHAI_cose_explanation_with_answer",
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.07174297243906035,
          "std": 0.00460044586457343,
          "min": 0.06619874367276073,
          "max": 0.07746335729309779,
          "median": 0.07156681635132248,
          "values": [
            0.07746335729309779,
            0.07156681635132248,
            0.06619874367276073
          ]
        },
        "recall": {
          "mean": 0.49759439221041113,
          "std": 0.04168755989936337,
          "min": 0.43873223702430225,
          "max": 0.529891475853589,
          "median": 0.524159463753342,
          "values": [
            0.529891475853589,
            0.524159463753342,
            0.43873223702430225
          ]
        },
        "f1_score": {
          "mean": 0.12272602738462168,
          "std": 0.00866710908677869,
          "min": 0.11161980964318186,
          "max": 0.13276991742268632,
          "median": 0.12378835508799688,
          "values": [
            0.13276991742268632,
            0.12378835508799688,
            0.11161980964318186
          ]
        },
        "jaccard": {
          "mean": 0.0661554618617963,
          "std": 0.004792815821764323,
          "min": 0.06007680550816749,
          "max": 0.07179172630138222,
          "median": 0.0665978537758392,
          "values": [
            0.07179172630138222,
            0.0665978537758392,
            0.06007680550816749
          ]
        },
        "semantic_similarity": {
          "mean": 0.4176253400215258,
          "std": 0.05293884404222578,
          "min": 0.34287726983428,
          "max": 0.4586498977150768,
          "median": 0.45134885251522067,
          "values": [
            0.45134885251522067,
            0.34287726983428,
            0.4586498977150768
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0,
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0
          ]
        },
        "format_cleanliness": {
          "mean": 0.9824999999999999,
          "std": 0.0025000000000000022,
          "min": 0.98,
          "max": 0.985,
          "median": 0.9824999999999999,
          "values": [
            0.985,
            0.98
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.8035,
          "std": 0.0,
          "min": 0.8035,
          "max": 0.8035,
          "median": 0.8035,
          "values": [
            0.8035
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8666666666666667,
          "std": 0.0,
          "min": 0.8666666666666667,
          "max": 0.8666666666666667,
          "median": 0.8666666666666667,
          "values": [
            0.8666666666666667
          ]
        },
        "structural_format_match": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_ecqa_freeflow_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b_ft_ecqa_freeflow_ft__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.4898914464228833,
          "std": 0.0,
          "min": 0.4898914464228833,
          "max": 0.4898914464228833,
          "median": 0.4898914464228833,
          "values": [
            0.4898914464228833
          ]
        },
        "recall": {
          "mean": 0.417051056580515,
          "std": 0.0,
          "min": 0.417051056580515,
          "max": 0.417051056580515,
          "median": 0.417051056580515,
          "values": [
            0.417051056580515
          ]
        },
        "f1_score": {
          "mean": 0.4354941311253262,
          "std": 0.0,
          "min": 0.4354941311253262,
          "max": 0.4354941311253262,
          "median": 0.4354941311253262,
          "values": [
            0.4354941311253262
          ]
        },
        "jaccard": {
          "mean": 0.2848766051944887,
          "std": 0.0,
          "min": 0.2848766051944887,
          "max": 0.2848766051944887,
          "median": 0.2848766051944887,
          "values": [
            0.2848766051944887
          ]
        },
        "semantic_similarity": {
          "mean": 0.7053428494930267,
          "std": 0.0,
          "min": 0.7053428494930267,
          "max": 0.7053428494930267,
          "median": 0.7053428494930267,
          "values": [
            0.7053428494930267
          ]
        },
        "all_choices_addressed": {
          "mean": 0.818,
          "std": 0.0,
          "min": 0.818,
          "max": 0.818,
          "median": 0.818,
          "values": [
            0.818
          ]
        },
        "single_paragraph_format": {
          "mean": 0.23099999999999998,
          "std": 0.0,
          "min": 0.23099999999999998,
          "max": 0.23099999999999998,
          "median": 0.23099999999999998,
          "values": [
            0.23099999999999998
          ]
        },
        "explanation_depth": {
          "mean": 0.08033856265439387,
          "std": 0.0,
          "min": 0.08033856265439387,
          "max": 0.08033856265439387,
          "median": 0.08033856265439387,
          "values": [
            0.08033856265439387
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_ecqa_negative_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-mistral-7b_ft_ecqa_negative_ft__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.34900191824625354,
          "std": 0.0,
          "min": 0.34900191824625354,
          "max": 0.34900191824625354,
          "median": 0.34900191824625354,
          "values": [
            0.34900191824625354
          ]
        },
        "recall": {
          "mean": 0.40863570814618677,
          "std": 0.0,
          "min": 0.40863570814618677,
          "max": 0.40863570814618677,
          "median": 0.40863570814618677,
          "values": [
            0.40863570814618677
          ]
        },
        "f1_score": {
          "mean": 0.34303866741771744,
          "std": 0.0,
          "min": 0.34303866741771744,
          "max": 0.34303866741771744,
          "median": 0.34303866741771744,
          "values": [
            0.34303866741771744
          ]
        },
        "jaccard": {
          "mean": 0.21255279965018897,
          "std": 0.0,
          "min": 0.21255279965018897,
          "max": 0.21255279965018897,
          "median": 0.21255279965018897,
          "values": [
            0.21255279965018897
          ]
        },
        "semantic_similarity": {
          "mean": 0.6521517732739448,
          "std": 0.0,
          "min": 0.6521517732739448,
          "max": 0.6521517732739448,
          "median": 0.6521517732739448,
          "values": [
            0.6521517732739448
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9096666666666667,
          "std": 0.0,
          "min": 0.9096666666666667,
          "max": 0.9096666666666667,
          "median": 0.9096666666666667,
          "values": [
            0.9096666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.8008666610194901,
          "std": 0.0,
          "min": 0.8008666610194901,
          "max": 0.8008666610194901,
          "median": 0.8008666610194901,
          "values": [
            0.8008666610194901
          ]
        },
        "external_knowledge_usage": {
          "mean": 0.9981407089151451,
          "std": 0.0,
          "min": 0.9981407089151451,
          "max": 0.9981407089151451,
          "median": 0.9981407089151451,
          "values": [
            0.9981407089151451
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_ecqa_positive_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-mistral-7b_ft_ecqa_positive_ft__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.2890322051165036,
          "std": 0.0,
          "min": 0.2890322051165036,
          "max": 0.2890322051165036,
          "median": 0.2890322051165036,
          "values": [
            0.2890322051165036
          ]
        },
        "recall": {
          "mean": 0.5112916105060541,
          "std": 0.0,
          "min": 0.5112916105060541,
          "max": 0.5112916105060541,
          "median": 0.5112916105060541,
          "values": [
            0.5112916105060541
          ]
        },
        "f1_score": {
          "mean": 0.3469011914995081,
          "std": 0.0,
          "min": 0.3469011914995081,
          "max": 0.3469011914995081,
          "median": 0.3469011914995081,
          "values": [
            0.3469011914995081
          ]
        },
        "jaccard": {
          "mean": 0.21822543738909142,
          "std": 0.0,
          "min": 0.21822543738909142,
          "max": 0.21822543738909142,
          "median": 0.21822543738909142,
          "values": [
            0.21822543738909142
          ]
        },
        "semantic_similarity": {
          "mean": 0.6373477092385292,
          "std": 0.0,
          "min": 0.6373477092385292,
          "max": 0.6373477092385292,
          "median": 0.6373477092385292,
          "values": [
            0.6373477092385292
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.9223757869933645,
          "std": 0.0,
          "min": 0.9223757869933645,
          "max": 0.9223757869933645,
          "median": 0.9223757869933645,
          "values": [
            0.9223757869933645
          ]
        },
        "correct_answer_focus": {
          "mean": 0.7935,
          "std": 0.0,
          "min": 0.7935,
          "max": 0.7935,
          "median": 0.7935,
          "values": [
            0.7935
          ]
        },
        "concise_justification": {
          "mean": 0.5112000000000001,
          "std": 0.0,
          "min": 0.5112000000000001,
          "max": 0.5112000000000001,
          "median": 0.5112000000000001,
          "values": [
            0.5112000000000001
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_gmeg_explanation_ft": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-mistral-7b_ft_gmeg_explanation_ft__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.3252198118008294,
          "std": 0.0,
          "min": 0.3252198118008294,
          "max": 0.3252198118008294,
          "median": 0.3252198118008294,
          "values": [
            0.3252198118008294
          ]
        },
        "recall": {
          "mean": 0.3299169805016207,
          "std": 0.0,
          "min": 0.3299169805016207,
          "max": 0.3299169805016207,
          "median": 0.3299169805016207,
          "values": [
            0.3299169805016207
          ]
        },
        "f1_score": {
          "mean": 0.31348241920638736,
          "std": 0.0,
          "min": 0.31348241920638736,
          "max": 0.31348241920638736,
          "median": 0.31348241920638736,
          "values": [
            0.31348241920638736
          ]
        },
        "jaccard": {
          "mean": 0.19661618599278566,
          "std": 0.0,
          "min": 0.19661618599278566,
          "max": 0.19661618599278566,
          "median": 0.19661618599278566,
          "values": [
            0.19661618599278566
          ]
        },
        "semantic_similarity": {
          "mean": 0.6538133403286338,
          "std": 0.0,
          "min": 0.6538133403286338,
          "max": 0.6538133403286338,
          "median": 0.6538133403286338,
          "values": [
            0.6538133403286338
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.1121666666666667,
          "std": 0.0,
          "min": 1.1121666666666667,
          "max": 1.1121666666666667,
          "median": 1.1121666666666667,
          "values": [
            1.1121666666666667
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7775,
          "std": 0.0,
          "min": 0.7775,
          "max": 0.7775,
          "median": 0.7775,
          "values": [
            0.7775
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "u4b-mistral-7b_ft_pubmedqa_reasoning_ft": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b_ft_pubmedqa_reasoning_ft__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "precision": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "recall": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "f1_score": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "jaccard": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "semantic_similarity": {
          "mean": 0.9230746108293534,
          "std": 0.0,
          "min": 0.9230746108293534,
          "max": 0.9230746108293534,
          "median": 0.9230746108293534,
          "values": [
            0.9230746108293534
          ]
        },
        "answer_correctness": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.775,
          "std": 0.0,
          "min": 0.775,
          "max": 0.775,
          "median": 0.775,
          "values": [
            0.775
          ]
        }
      }
    }
  },
  "by_prompt_model": {
    "cose_choice_selection_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__hf-mistral-nemo-12b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.31,
          "std": 0.0,
          "min": 0.31,
          "max": 0.31,
          "median": 0.31,
          "values": [
            0.31
          ]
        },
        "precision": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "recall": {
          "mean": 0.3416666666666666,
          "std": 0.0,
          "min": 0.3416666666666666,
          "max": 0.3416666666666666,
          "median": 0.3416666666666666,
          "values": [
            0.3416666666666666
          ]
        },
        "f1_score": {
          "mean": 0.3406666666666666,
          "std": 0.0,
          "min": 0.3406666666666666,
          "max": 0.3406666666666666,
          "median": 0.3406666666666666,
          "values": [
            0.3406666666666666
          ]
        },
        "jaccard": {
          "mean": 0.3291666666666667,
          "std": 0.0,
          "min": 0.3291666666666667,
          "max": 0.3291666666666667,
          "median": 0.3291666666666667,
          "values": [
            0.3291666666666667
          ]
        },
        "semantic_similarity": {
          "mean": 0.7559962159395218,
          "std": 0.0,
          "min": 0.7559962159395218,
          "max": 0.7559962159395218,
          "median": 0.7559962159395218,
          "values": [
            0.7559962159395218
          ]
        },
        "answer_correctness": {
          "mean": 0.7,
          "std": 0.0,
          "min": 0.7,
          "max": 0.7,
          "median": 0.7,
          "values": [
            0.7
          ]
        },
        "response_conciseness": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_choice_selection_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "precision": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "recall": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "jaccard": {
          "mean": 0.6866666666666665,
          "std": 0.0,
          "min": 0.6866666666666665,
          "max": 0.6866666666666665,
          "median": 0.6866666666666665,
          "values": [
            0.6866666666666665
          ]
        },
        "semantic_similarity": {
          "mean": 0.8124932911247015,
          "std": 0.0,
          "min": 0.8124932911247015,
          "max": 0.8124932911247015,
          "median": 0.8124932911247015,
          "values": [
            0.8124932911247015
          ]
        },
        "answer_correctness": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "response_conciseness": {
          "mean": 0.992,
          "std": 0.0,
          "min": 0.992,
          "max": 0.992,
          "median": 0.992,
          "values": [
            0.992
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_choice_selection_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3-8b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "precision": {
          "mean": 0.17066666666666663,
          "std": 0.0,
          "min": 0.17066666666666663,
          "max": 0.17066666666666663,
          "median": 0.17066666666666663,
          "values": [
            0.17066666666666663
          ]
        },
        "recall": {
          "mean": 0.18333333333333332,
          "std": 0.0,
          "min": 0.18333333333333332,
          "max": 0.18333333333333332,
          "median": 0.18333333333333332,
          "values": [
            0.18333333333333332
          ]
        },
        "f1_score": {
          "mean": 0.16916666666666663,
          "std": 0.0,
          "min": 0.16916666666666663,
          "max": 0.16916666666666663,
          "median": 0.16916666666666663,
          "values": [
            0.16916666666666663
          ]
        },
        "jaccard": {
          "mean": 0.1540952380952381,
          "std": 0.0,
          "min": 0.1540952380952381,
          "max": 0.1540952380952381,
          "median": 0.1540952380952381,
          "values": [
            0.1540952380952381
          ]
        },
        "semantic_similarity": {
          "mean": 0.7020514233410359,
          "std": 0.0,
          "min": 0.7020514233410359,
          "max": 0.7020514233410359,
          "median": 0.7020514233410359,
          "values": [
            0.7020514233410359
          ]
        },
        "answer_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "response_conciseness": {
          "mean": 0.9740000000000001,
          "std": 0.0,
          "min": 0.9740000000000001,
          "max": 0.9740000000000001,
          "median": 0.9740000000000001,
          "values": [
            0.9740000000000001
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_choice_selection_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3.2-1b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.017889339826839826,
          "std": 0.0,
          "min": 0.017889339826839826,
          "max": 0.017889339826839826,
          "median": 0.017889339826839826,
          "values": [
            0.017889339826839826
          ]
        },
        "recall": {
          "mean": 0.07416666666666666,
          "std": 0.0,
          "min": 0.07416666666666666,
          "max": 0.07416666666666666,
          "median": 0.07416666666666666,
          "values": [
            0.07416666666666666
          ]
        },
        "f1_score": {
          "mean": 0.027241593047475404,
          "std": 0.0,
          "min": 0.027241593047475404,
          "max": 0.027241593047475404,
          "median": 0.027241593047475404,
          "values": [
            0.027241593047475404
          ]
        },
        "jaccard": {
          "mean": 0.015567133520074695,
          "std": 0.0,
          "min": 0.015567133520074695,
          "max": 0.015567133520074695,
          "median": 0.015567133520074695,
          "values": [
            0.015567133520074695
          ]
        },
        "semantic_similarity": {
          "mean": 0.48915969584137203,
          "std": 0.0,
          "min": 0.48915969584137203,
          "max": 0.48915969584137203,
          "median": 0.48915969584137203,
          "values": [
            0.48915969584137203
          ]
        },
        "answer_correctness": {
          "mean": 0.46,
          "std": 0.0,
          "min": 0.46,
          "max": 0.46,
          "median": 0.46,
          "values": [
            0.46
          ]
        },
        "response_conciseness": {
          "mean": 0.665,
          "std": 0.0,
          "min": 0.665,
          "max": 0.665,
          "median": 0.665,
          "values": [
            0.665
          ]
        },
        "instruction_following_penalty": {
          "mean": 0.9940000000000001,
          "std": 0.0,
          "min": 0.9940000000000001,
          "max": 0.9940000000000001,
          "median": 0.9940000000000001,
          "values": [
            0.9940000000000001
          ]
        }
      }
    },
    "cose_choice_selection_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-llama3.3-70b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "precision": {
          "mean": 0.200942016740964,
          "std": 0.0,
          "min": 0.200942016740964,
          "max": 0.200942016740964,
          "median": 0.200942016740964,
          "values": [
            0.200942016740964
          ]
        },
        "recall": {
          "mean": 0.5925,
          "std": 0.0,
          "min": 0.5925,
          "max": 0.5925,
          "median": 0.5925,
          "values": [
            0.5925
          ]
        },
        "f1_score": {
          "mean": 0.233500611191643,
          "std": 0.0,
          "min": 0.233500611191643,
          "max": 0.233500611191643,
          "median": 0.233500611191643,
          "values": [
            0.233500611191643
          ]
        },
        "jaccard": {
          "mean": 0.19509040973011188,
          "std": 0.0,
          "min": 0.19509040973011188,
          "max": 0.19509040973011188,
          "median": 0.19509040973011188,
          "values": [
            0.19509040973011188
          ]
        },
        "semantic_similarity": {
          "mean": 0.56256989993155,
          "std": 0.0,
          "min": 0.56256989993155,
          "max": 0.56256989993155,
          "median": 0.56256989993155,
          "values": [
            0.56256989993155
          ]
        },
        "answer_correctness": {
          "mean": 0.81,
          "std": 0.0,
          "min": 0.81,
          "max": 0.81,
          "median": 0.81,
          "values": [
            0.81
          ]
        },
        "response_conciseness": {
          "mean": 0.523,
          "std": 0.0,
          "min": 0.523,
          "max": 0.523,
          "median": 0.523,
          "values": [
            0.523
          ]
        },
        "instruction_following_penalty": {
          "mean": 0.8059999999999999,
          "std": 0.0,
          "min": 0.8059999999999999,
          "max": 0.8059999999999999,
          "median": 0.8059999999999999,
          "values": [
            0.8059999999999999
          ]
        }
      }
    },
    "cose_choice_selection_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_choice_selection__u4b-mistral-7b__zero-shot__cose_choice_selection__100__0p100"
      ],
      "setups": [
        "RHAI_cose_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.18,
          "std": 0.0,
          "min": 0.18,
          "max": 0.18,
          "median": 0.18,
          "values": [
            0.18
          ]
        },
        "precision": {
          "mean": 0.19582437932437935,
          "std": 0.0,
          "min": 0.19582437932437935,
          "max": 0.19582437932437935,
          "median": 0.19582437932437935,
          "values": [
            0.19582437932437935
          ]
        },
        "recall": {
          "mean": 0.23916666666666664,
          "std": 0.0,
          "min": 0.23916666666666664,
          "max": 0.23916666666666664,
          "median": 0.23916666666666664,
          "values": [
            0.23916666666666664
          ]
        },
        "f1_score": {
          "mean": 0.19862846370718848,
          "std": 0.0,
          "min": 0.19862846370718848,
          "max": 0.19862846370718848,
          "median": 0.19862846370718848,
          "values": [
            0.19862846370718848
          ]
        },
        "jaccard": {
          "mean": 0.1909575746314877,
          "std": 0.0,
          "min": 0.1909575746314877,
          "max": 0.1909575746314877,
          "median": 0.1909575746314877,
          "values": [
            0.1909575746314877
          ]
        },
        "semantic_similarity": {
          "mean": 0.6934634025394917,
          "std": 0.0,
          "min": 0.6934634025394917,
          "max": 0.6934634025394917,
          "median": 0.6934634025394917,
          "values": [
            0.6934634025394917
          ]
        },
        "answer_correctness": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "response_conciseness": {
          "mean": 0.9109999999999999,
          "std": 0.0,
          "min": 0.9109999999999999,
          "max": 0.9109999999999999,
          "median": 0.9109999999999999,
          "values": [
            0.9109999999999999
          ]
        },
        "instruction_following_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__hf-mistral-nemo-12b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.2043470206264324,
          "std": 0.0,
          "min": 0.2043470206264324,
          "max": 0.2043470206264324,
          "median": 0.2043470206264324,
          "values": [
            0.2043470206264324
          ]
        },
        "recall": {
          "mean": 0.07638833259103094,
          "std": 0.0,
          "min": 0.07638833259103094,
          "max": 0.07638833259103094,
          "median": 0.07638833259103094,
          "values": [
            0.07638833259103094
          ]
        },
        "f1_score": {
          "mean": 0.104005606130473,
          "std": 0.0,
          "min": 0.104005606130473,
          "max": 0.104005606130473,
          "median": 0.104005606130473,
          "values": [
            0.104005606130473
          ]
        },
        "jaccard": {
          "mean": 0.05832851534349138,
          "std": 0.0,
          "min": 0.05832851534349138,
          "max": 0.05832851534349138,
          "median": 0.05832851534349138,
          "values": [
            0.05832851534349138
          ]
        },
        "semantic_similarity": {
          "mean": 0.4240999024733901,
          "std": 0.0,
          "min": 0.4240999024733901,
          "max": 0.4240999024733901,
          "median": 0.4240999024733901,
          "values": [
            0.4240999024733901
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.655,
          "std": 0.0,
          "min": 0.655,
          "max": 0.655,
          "median": 0.655,
          "values": [
            0.655
          ]
        },
        "explanation_quality": {
          "mean": 0.09,
          "std": 0.0,
          "min": 0.09,
          "max": 0.09,
          "median": 0.09,
          "values": [
            0.09
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__hf-qwen2.5-3b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.24690214964218196,
          "std": 0.0,
          "min": 0.24690214964218196,
          "max": 0.24690214964218196,
          "median": 0.24690214964218196,
          "values": [
            0.24690214964218196
          ]
        },
        "recall": {
          "mean": 0.20070787972517093,
          "std": 0.0,
          "min": 0.20070787972517093,
          "max": 0.20070787972517093,
          "median": 0.20070787972517093,
          "values": [
            0.20070787972517093
          ]
        },
        "f1_score": {
          "mean": 0.20911999062887965,
          "std": 0.0,
          "min": 0.20911999062887965,
          "max": 0.20911999062887965,
          "median": 0.20911999062887965,
          "values": [
            0.20911999062887965
          ]
        },
        "jaccard": {
          "mean": 0.12121886485500809,
          "std": 0.0,
          "min": 0.12121886485500809,
          "max": 0.12121886485500809,
          "median": 0.12121886485500809,
          "values": [
            0.12121886485500809
          ]
        },
        "semantic_similarity": {
          "mean": 0.5534891698509454,
          "std": 0.0,
          "min": 0.5534891698509454,
          "max": 0.5534891698509454,
          "median": 0.5534891698509454,
          "values": [
            0.5534891698509454
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "explanation_quality": {
          "mean": 0.64,
          "std": 0.0,
          "min": 0.64,
          "max": 0.64,
          "median": 0.64,
          "values": [
            0.64
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-llama3-8b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.15689040508315139,
          "std": 0.0,
          "min": 0.15689040508315139,
          "max": 0.15689040508315139,
          "median": 0.15689040508315139,
          "values": [
            0.15689040508315139
          ]
        },
        "recall": {
          "mean": 0.1314184130551263,
          "std": 0.0,
          "min": 0.1314184130551263,
          "max": 0.1314184130551263,
          "median": 0.1314184130551263,
          "values": [
            0.1314184130551263
          ]
        },
        "f1_score": {
          "mean": 0.12107995168359142,
          "std": 0.0,
          "min": 0.12107995168359142,
          "max": 0.12107995168359142,
          "median": 0.12107995168359142,
          "values": [
            0.12107995168359142
          ]
        },
        "jaccard": {
          "mean": 0.0690678591308944,
          "std": 0.0,
          "min": 0.0690678591308944,
          "max": 0.0690678591308944,
          "median": 0.0690678591308944,
          "values": [
            0.0690678591308944
          ]
        },
        "semantic_similarity": {
          "mean": 0.45225615467876196,
          "std": 0.0,
          "min": 0.45225615467876196,
          "max": 0.45225615467876196,
          "median": 0.45225615467876196,
          "values": [
            0.45225615467876196
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.695,
          "std": 0.0,
          "min": 0.695,
          "max": 0.695,
          "median": 0.695,
          "values": [
            0.695
          ]
        },
        "explanation_quality": {
          "mean": 0.41,
          "std": 0.0,
          "min": 0.41,
          "max": 0.41,
          "median": 0.41,
          "values": [
            0.41
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-llama3.2-1b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1575692080430046,
          "std": 0.0,
          "min": 0.1575692080430046,
          "max": 0.1575692080430046,
          "median": 0.1575692080430046,
          "values": [
            0.1575692080430046
          ]
        },
        "recall": {
          "mean": 0.160118435573066,
          "std": 0.0,
          "min": 0.160118435573066,
          "max": 0.160118435573066,
          "median": 0.160118435573066,
          "values": [
            0.160118435573066
          ]
        },
        "f1_score": {
          "mean": 0.13946256212663946,
          "std": 0.0,
          "min": 0.13946256212663946,
          "max": 0.13946256212663946,
          "median": 0.13946256212663946,
          "values": [
            0.13946256212663946
          ]
        },
        "jaccard": {
          "mean": 0.07837163580719397,
          "std": 0.0,
          "min": 0.07837163580719397,
          "max": 0.07837163580719397,
          "median": 0.07837163580719397,
          "values": [
            0.07837163580719397
          ]
        },
        "semantic_similarity": {
          "mean": 0.4500850836187601,
          "std": 0.0,
          "min": 0.4500850836187601,
          "max": 0.4500850836187601,
          "median": 0.4500850836187601,
          "values": [
            0.4500850836187601
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.615,
          "std": 0.0,
          "min": 0.615,
          "max": 0.615,
          "median": 0.615,
          "values": [
            0.615
          ]
        },
        "explanation_quality": {
          "mean": 0.506,
          "std": 0.0,
          "min": 0.506,
          "max": 0.506,
          "median": 0.506,
          "values": [
            0.506
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.17232543568044964,
          "std": 0.0,
          "min": 0.17232543568044964,
          "max": 0.17232543568044964,
          "median": 0.17232543568044964,
          "values": [
            0.17232543568044964
          ]
        },
        "recall": {
          "mean": 0.30343662607300603,
          "std": 0.0,
          "min": 0.30343662607300603,
          "max": 0.30343662607300603,
          "median": 0.30343662607300603,
          "values": [
            0.30343662607300603
          ]
        },
        "f1_score": {
          "mean": 0.20334576845697389,
          "std": 0.0,
          "min": 0.20334576845697389,
          "max": 0.20334576845697389,
          "median": 0.20334576845697389,
          "values": [
            0.20334576845697389
          ]
        },
        "jaccard": {
          "mean": 0.11565473176818783,
          "std": 0.0,
          "min": 0.11565473176818783,
          "max": 0.11565473176818783,
          "median": 0.11565473176818783,
          "values": [
            0.11565473176818783
          ]
        },
        "semantic_similarity": {
          "mean": 0.5585999847948551,
          "std": 0.0,
          "min": 0.5585999847948551,
          "max": 0.5585999847948551,
          "median": 0.5585999847948551,
          "values": [
            0.5585999847948551
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.705,
          "std": 0.0,
          "min": 0.705,
          "max": 0.705,
          "median": 0.705,
          "values": [
            0.705
          ]
        },
        "explanation_quality": {
          "mean": 0.828,
          "std": 0.0,
          "min": 0.828,
          "max": 0.828,
          "median": 0.828,
          "values": [
            0.828
          ]
        },
        "response_cleanliness": {
          "mean": 0.9430000000000001,
          "std": 0.0,
          "min": 0.9430000000000001,
          "max": 0.9430000000000001,
          "median": 0.9430000000000001,
          "values": [
            0.9430000000000001
          ]
        }
      }
    },
    "cose_explanation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-mistral-7b__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.19782797322544143,
          "std": 0.0,
          "min": 0.19782797322544143,
          "max": 0.19782797322544143,
          "median": 0.19782797322544143,
          "values": [
            0.19782797322544143
          ]
        },
        "recall": {
          "mean": 0.22221080322289843,
          "std": 0.0,
          "min": 0.22221080322289843,
          "max": 0.22221080322289843,
          "median": 0.22221080322289843,
          "values": [
            0.22221080322289843
          ]
        },
        "f1_score": {
          "mean": 0.19313490438109585,
          "std": 0.0,
          "min": 0.19313490438109585,
          "max": 0.19313490438109585,
          "median": 0.19313490438109585,
          "values": [
            0.19313490438109585
          ]
        },
        "jaccard": {
          "mean": 0.11113775093441064,
          "std": 0.0,
          "min": 0.11113775093441064,
          "max": 0.11113775093441064,
          "median": 0.11113775093441064,
          "values": [
            0.11113775093441064
          ]
        },
        "semantic_similarity": {
          "mean": 0.5622329947352409,
          "std": 0.0,
          "min": 0.5622329947352409,
          "max": 0.5622329947352409,
          "median": 0.5622329947352409,
          "values": [
            0.5622329947352409
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "explanation_quality": {
          "mean": 0.7559999999999999,
          "std": 0.0,
          "min": 0.7559999999999999,
          "max": 0.7559999999999999,
          "median": 0.7559999999999999,
          "values": [
            0.7559999999999999
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_u4b-mistral-7b_ft_cose_explanation_ft": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation__u4b-mistral-7b_ft_cose_explanation_ft__zero-shot__cose_explanation__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.25560764835925204,
          "std": 0.0,
          "min": 0.25560764835925204,
          "max": 0.25560764835925204,
          "median": 0.25560764835925204,
          "values": [
            0.25560764835925204
          ]
        },
        "recall": {
          "mean": 0.16833561645792386,
          "std": 0.0,
          "min": 0.16833561645792386,
          "max": 0.16833561645792386,
          "median": 0.16833561645792386,
          "values": [
            0.16833561645792386
          ]
        },
        "f1_score": {
          "mean": 0.17768690410135687,
          "std": 0.0,
          "min": 0.17768690410135687,
          "max": 0.17768690410135687,
          "median": 0.17768690410135687,
          "values": [
            0.17768690410135687
          ]
        },
        "jaccard": {
          "mean": 0.10270769847885078,
          "std": 0.0,
          "min": 0.10270769847885078,
          "max": 0.10270769847885078,
          "median": 0.10270769847885078,
          "values": [
            0.10270769847885078
          ]
        },
        "semantic_similarity": {
          "mean": 0.5429833361133933,
          "std": 0.0,
          "min": 0.5429833361133933,
          "max": 0.5429833361133933,
          "median": 0.5429833361133933,
          "values": [
            0.5429833361133933
          ]
        },
        "answer_identification_correctness": {
          "mean": 0.625,
          "std": 0.0,
          "min": 0.625,
          "max": 0.625,
          "median": 0.625,
          "values": [
            0.625
          ]
        },
        "explanation_quality": {
          "mean": 0.506,
          "std": 0.0,
          "min": 0.506,
          "max": 0.506,
          "median": 0.506,
          "values": [
            0.506
          ]
        },
        "response_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__hf-mistral-nemo-12b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.2710679820056553,
          "std": 0.0,
          "min": 0.2710679820056553,
          "max": 0.2710679820056553,
          "median": 0.2710679820056553,
          "values": [
            0.2710679820056553
          ]
        },
        "recall": {
          "mean": 0.1742063311299156,
          "std": 0.0,
          "min": 0.1742063311299156,
          "max": 0.1742063311299156,
          "median": 0.1742063311299156,
          "values": [
            0.1742063311299156
          ]
        },
        "f1_score": {
          "mean": 0.19887381666910323,
          "std": 0.0,
          "min": 0.19887381666910323,
          "max": 0.19887381666910323,
          "median": 0.19887381666910323,
          "values": [
            0.19887381666910323
          ]
        },
        "jaccard": {
          "mean": 0.1198940158325367,
          "std": 0.0,
          "min": 0.1198940158325367,
          "max": 0.1198940158325367,
          "median": 0.1198940158325367,
          "values": [
            0.1198940158325367
          ]
        },
        "semantic_similarity": {
          "mean": 0.5996882364153862,
          "std": 0.0,
          "min": 0.5996882364153862,
          "max": 0.5996882364153862,
          "median": 0.5996882364153862,
          "values": [
            0.5996882364153862
          ]
        },
        "explanation_correctness": {
          "mean": 0.66,
          "std": 0.0,
          "min": 0.66,
          "max": 0.66,
          "median": 0.66,
          "values": [
            0.66
          ]
        },
        "brevity_compliance": {
          "mean": 0.9560000000000001,
          "std": 0.0,
          "min": 0.9560000000000001,
          "max": 0.9560000000000001,
          "median": 0.9560000000000001,
          "values": [
            0.9560000000000001
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__hf-qwen2.5-3b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.21715598631224858,
          "std": 0.0,
          "min": 0.21715598631224858,
          "max": 0.21715598631224858,
          "median": 0.21715598631224858,
          "values": [
            0.21715598631224858
          ]
        },
        "recall": {
          "mean": 0.2642219724126587,
          "std": 0.0,
          "min": 0.2642219724126587,
          "max": 0.2642219724126587,
          "median": 0.2642219724126587,
          "values": [
            0.2642219724126587
          ]
        },
        "f1_score": {
          "mean": 0.2260900319435403,
          "std": 0.0,
          "min": 0.2260900319435403,
          "max": 0.2260900319435403,
          "median": 0.2260900319435403,
          "values": [
            0.2260900319435403
          ]
        },
        "jaccard": {
          "mean": 0.13051404237270547,
          "std": 0.0,
          "min": 0.13051404237270547,
          "max": 0.13051404237270547,
          "median": 0.13051404237270547,
          "values": [
            0.13051404237270547
          ]
        },
        "semantic_similarity": {
          "mean": 0.6366396707296371,
          "std": 0.0,
          "min": 0.6366396707296371,
          "max": 0.6366396707296371,
          "median": 0.6366396707296371,
          "values": [
            0.6366396707296371
          ]
        },
        "explanation_correctness": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "brevity_compliance": {
          "mean": 0.968,
          "std": 0.0,
          "min": 0.968,
          "max": 0.968,
          "median": 0.968,
          "values": [
            0.968
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer_hf-qwen3-30b-thinking": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__hf-qwen3-30b-thinking__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.07746335729309779,
          "std": 0.0,
          "min": 0.07746335729309779,
          "max": 0.07746335729309779,
          "median": 0.07746335729309779,
          "values": [
            0.07746335729309779
          ]
        },
        "recall": {
          "mean": 0.529891475853589,
          "std": 0.0,
          "min": 0.529891475853589,
          "max": 0.529891475853589,
          "median": 0.529891475853589,
          "values": [
            0.529891475853589
          ]
        },
        "f1_score": {
          "mean": 0.13276991742268632,
          "std": 0.0,
          "min": 0.13276991742268632,
          "max": 0.13276991742268632,
          "median": 0.13276991742268632,
          "values": [
            0.13276991742268632
          ]
        },
        "jaccard": {
          "mean": 0.07179172630138222,
          "std": 0.0,
          "min": 0.07179172630138222,
          "max": 0.07179172630138222,
          "median": 0.07179172630138222,
          "values": [
            0.07179172630138222
          ]
        },
        "semantic_similarity": {
          "mean": 0.45134885251522067,
          "std": 0.0,
          "min": 0.45134885251522067,
          "max": 0.45134885251522067,
          "median": 0.45134885251522067,
          "values": [
            0.45134885251522067
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "format_cleanliness": {
          "mean": 0.985,
          "std": 0.0,
          "min": 0.985,
          "max": 0.985,
          "median": 0.985,
          "values": [
            0.985
          ]
        }
      }
    },
    "cose_explanation_with_answer_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__u4b-llama3-8b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.21116309727705734,
          "std": 0.0,
          "min": 0.21116309727705734,
          "max": 0.21116309727705734,
          "median": 0.21116309727705734,
          "values": [
            0.21116309727705734
          ]
        },
        "recall": {
          "mean": 0.29967265965370204,
          "std": 0.0,
          "min": 0.29967265965370204,
          "max": 0.29967265965370204,
          "median": 0.29967265965370204,
          "values": [
            0.29967265965370204
          ]
        },
        "f1_score": {
          "mean": 0.23781148459618684,
          "std": 0.0,
          "min": 0.23781148459618684,
          "max": 0.23781148459618684,
          "median": 0.23781148459618684,
          "values": [
            0.23781148459618684
          ]
        },
        "jaccard": {
          "mean": 0.1381444301898855,
          "std": 0.0,
          "min": 0.1381444301898855,
          "max": 0.1381444301898855,
          "median": 0.1381444301898855,
          "values": [
            0.1381444301898855
          ]
        },
        "semantic_similarity": {
          "mean": 0.6308293950557708,
          "std": 0.0,
          "min": 0.6308293950557708,
          "max": 0.6308293950557708,
          "median": 0.6308293950557708,
          "values": [
            0.6308293950557708
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.8980000000000001,
          "std": 0.0,
          "min": 0.8980000000000001,
          "max": 0.8980000000000001,
          "median": 0.8980000000000001,
          "values": [
            0.8980000000000001
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__u4b-llama3.2-1b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.21263816083201761,
          "std": 0.0,
          "min": 0.21263816083201761,
          "max": 0.21263816083201761,
          "median": 0.21263816083201761,
          "values": [
            0.21263816083201761
          ]
        },
        "recall": {
          "mean": 0.258322573271235,
          "std": 0.0,
          "min": 0.258322573271235,
          "max": 0.258322573271235,
          "median": 0.258322573271235,
          "values": [
            0.258322573271235
          ]
        },
        "f1_score": {
          "mean": 0.22402495042812845,
          "std": 0.0,
          "min": 0.22402495042812845,
          "max": 0.22402495042812845,
          "median": 0.22402495042812845,
          "values": [
            0.22402495042812845
          ]
        },
        "jaccard": {
          "mean": 0.12987108632341812,
          "std": 0.0,
          "min": 0.12987108632341812,
          "max": 0.12987108632341812,
          "median": 0.12987108632341812,
          "values": [
            0.12987108632341812
          ]
        },
        "semantic_similarity": {
          "mean": 0.5673414608836174,
          "std": 0.0,
          "min": 0.5673414608836174,
          "max": 0.5673414608836174,
          "median": 0.5673414608836174,
          "values": [
            0.5673414608836174
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.9440000000000001,
          "std": 0.0,
          "min": 0.9440000000000001,
          "max": 0.9440000000000001,
          "median": 0.9440000000000001,
          "values": [
            0.9440000000000001
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_explanation_with_answer_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__u4b-llama3.3-70b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.205745266140876,
          "std": 0.0,
          "min": 0.205745266140876,
          "max": 0.205745266140876,
          "median": 0.205745266140876,
          "values": [
            0.205745266140876
          ]
        },
        "recall": {
          "mean": 0.29189238677750157,
          "std": 0.0,
          "min": 0.29189238677750157,
          "max": 0.29189238677750157,
          "median": 0.29189238677750157,
          "values": [
            0.29189238677750157
          ]
        },
        "f1_score": {
          "mean": 0.21782890538768335,
          "std": 0.0,
          "min": 0.21782890538768335,
          "max": 0.21782890538768335,
          "median": 0.21782890538768335,
          "values": [
            0.21782890538768335
          ]
        },
        "jaccard": {
          "mean": 0.12570849113518245,
          "std": 0.0,
          "min": 0.12570849113518245,
          "max": 0.12570849113518245,
          "median": 0.12570849113518245,
          "values": [
            0.12570849113518245
          ]
        },
        "semantic_similarity": {
          "mean": 0.5475268509984016,
          "std": 0.0,
          "min": 0.5475268509984016,
          "max": 0.5475268509984016,
          "median": 0.5475268509984016,
          "values": [
            0.5475268509984016
          ]
        },
        "explanation_correctness": {
          "mean": 0.977,
          "std": 0.0,
          "min": 0.977,
          "max": 0.977,
          "median": 0.977,
          "values": [
            0.977
          ]
        },
        "brevity_compliance": {
          "mean": 0.757,
          "std": 0.0,
          "min": 0.757,
          "max": 0.757,
          "median": 0.757,
          "values": [
            0.757
          ]
        },
        "format_cleanliness": {
          "mean": 0.929,
          "std": 0.0,
          "min": 0.929,
          "max": 0.929,
          "median": 0.929,
          "values": [
            0.929
          ]
        }
      }
    },
    "cose_explanation_with_answer_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer__u4b-mistral-7b__zero-shot__cose_explanation_with_answer__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.25302236360721825,
          "std": 0.0,
          "min": 0.25302236360721825,
          "max": 0.25302236360721825,
          "median": 0.25302236360721825,
          "values": [
            0.25302236360721825
          ]
        },
        "recall": {
          "mean": 0.28141934071330515,
          "std": 0.0,
          "min": 0.28141934071330515,
          "max": 0.28141934071330515,
          "median": 0.28141934071330515,
          "values": [
            0.28141934071330515
          ]
        },
        "f1_score": {
          "mean": 0.25214153009130524,
          "std": 0.0,
          "min": 0.25214153009130524,
          "max": 0.25214153009130524,
          "median": 0.25214153009130524,
          "values": [
            0.25214153009130524
          ]
        },
        "jaccard": {
          "mean": 0.14976971623070223,
          "std": 0.0,
          "min": 0.14976971623070223,
          "max": 0.14976971623070223,
          "median": 0.14976971623070223,
          "values": [
            0.14976971623070223
          ]
        },
        "semantic_similarity": {
          "mean": 0.6856044900417327,
          "std": 0.0,
          "min": 0.6856044900417327,
          "max": 0.6856044900417327,
          "median": 0.6856044900417327,
          "values": [
            0.6856044900417327
          ]
        },
        "explanation_correctness": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "brevity_compliance": {
          "mean": 0.972,
          "std": 0.0,
          "min": 0.972,
          "max": 0.972,
          "median": 0.972,
          "values": [
            0.972
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__hf-mistral-nemo-12b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.14020504346682058,
          "std": 0.0,
          "min": 0.14020504346682058,
          "max": 0.14020504346682058,
          "median": 0.14020504346682058,
          "values": [
            0.14020504346682058
          ]
        },
        "recall": {
          "mean": 0.3745891674788984,
          "std": 0.0,
          "min": 0.3745891674788984,
          "max": 0.3745891674788984,
          "median": 0.3745891674788984,
          "values": [
            0.3745891674788984
          ]
        },
        "f1_score": {
          "mean": 0.1953615087241507,
          "std": 0.0,
          "min": 0.1953615087241507,
          "max": 0.1953615087241507,
          "median": 0.1953615087241507,
          "values": [
            0.1953615087241507
          ]
        },
        "jaccard": {
          "mean": 0.11013022879812855,
          "std": 0.0,
          "min": 0.11013022879812855,
          "max": 0.11013022879812855,
          "median": 0.11013022879812855,
          "values": [
            0.11013022879812855
          ]
        },
        "semantic_similarity": {
          "mean": 0.6157916417717934,
          "std": 0.0,
          "min": 0.6157916417717934,
          "max": 0.6157916417717934,
          "median": 0.6157916417717934,
          "values": [
            0.6157916417717934
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.5809999999999998,
          "std": 0.0,
          "min": 0.5809999999999998,
          "max": 0.5809999999999998,
          "median": 0.5809999999999998,
          "values": [
            0.5809999999999998
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen2.5-3b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1300239358349212,
          "std": 0.0,
          "min": 0.1300239358349212,
          "max": 0.1300239358349212,
          "median": 0.1300239358349212,
          "values": [
            0.1300239358349212
          ]
        },
        "recall": {
          "mean": 0.43467512639630457,
          "std": 0.0,
          "min": 0.43467512639630457,
          "max": 0.43467512639630457,
          "median": 0.43467512639630457,
          "values": [
            0.43467512639630457
          ]
        },
        "f1_score": {
          "mean": 0.19304835868081727,
          "std": 0.0,
          "min": 0.19304835868081727,
          "max": 0.19304835868081727,
          "median": 0.19304835868081727,
          "values": [
            0.19304835868081727
          ]
        },
        "jaccard": {
          "mean": 0.10845905009479939,
          "std": 0.0,
          "min": 0.10845905009479939,
          "max": 0.10845905009479939,
          "median": 0.10845905009479939,
          "values": [
            0.10845905009479939
          ]
        },
        "semantic_similarity": {
          "mean": 0.6268906626105308,
          "std": 0.0,
          "min": 0.6268906626105308,
          "max": 0.6268906626105308,
          "median": 0.6268906626105308,
          "values": [
            0.6268906626105308
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.37200000000000005,
          "std": 0.0,
          "min": 0.37200000000000005,
          "max": 0.37200000000000005,
          "median": 0.37200000000000005,
          "values": [
            0.37200000000000005
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization_hf-qwen3-30b-thinking": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__hf-qwen3-30b-thinking__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.07156681635132248,
          "std": 0.0,
          "min": 0.07156681635132248,
          "max": 0.07156681635132248,
          "median": 0.07156681635132248,
          "values": [
            0.07156681635132248
          ]
        },
        "recall": {
          "mean": 0.524159463753342,
          "std": 0.0,
          "min": 0.524159463753342,
          "max": 0.524159463753342,
          "median": 0.524159463753342,
          "values": [
            0.524159463753342
          ]
        },
        "f1_score": {
          "mean": 0.12378835508799688,
          "std": 0.0,
          "min": 0.12378835508799688,
          "max": 0.12378835508799688,
          "median": 0.12378835508799688,
          "values": [
            0.12378835508799688
          ]
        },
        "jaccard": {
          "mean": 0.0665978537758392,
          "std": 0.0,
          "min": 0.0665978537758392,
          "max": 0.0665978537758392,
          "median": 0.0665978537758392,
          "values": [
            0.0665978537758392
          ]
        },
        "semantic_similarity": {
          "mean": 0.34287726983428,
          "std": 0.0,
          "min": 0.34287726983428,
          "max": 0.34287726983428,
          "median": 0.34287726983428,
          "values": [
            0.34287726983428
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "format_cleanliness": {
          "mean": 0.98,
          "std": 0.0,
          "min": 0.98,
          "max": 0.98,
          "median": 0.98,
          "values": [
            0.98
          ]
        }
      }
    },
    "cose_self_rationalization_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3-8b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.14095966309739427,
          "std": 0.0,
          "min": 0.14095966309739427,
          "max": 0.14095966309739427,
          "median": 0.14095966309739427,
          "values": [
            0.14095966309739427
          ]
        },
        "recall": {
          "mean": 0.4189656730716152,
          "std": 0.0,
          "min": 0.4189656730716152,
          "max": 0.4189656730716152,
          "median": 0.4189656730716152,
          "values": [
            0.4189656730716152
          ]
        },
        "f1_score": {
          "mean": 0.20248926343199922,
          "std": 0.0,
          "min": 0.20248926343199922,
          "max": 0.20248926343199922,
          "median": 0.20248926343199922,
          "values": [
            0.20248926343199922
          ]
        },
        "jaccard": {
          "mean": 0.11485640211464439,
          "std": 0.0,
          "min": 0.11485640211464439,
          "max": 0.11485640211464439,
          "median": 0.11485640211464439,
          "values": [
            0.11485640211464439
          ]
        },
        "semantic_similarity": {
          "mean": 0.6239275026321411,
          "std": 0.0,
          "min": 0.6239275026321411,
          "max": 0.6239275026321411,
          "median": 0.6239275026321411,
          "values": [
            0.6239275026321411
          ]
        },
        "explanation_correctness": {
          "mean": 0.97,
          "std": 0.0,
          "min": 0.97,
          "max": 0.97,
          "median": 0.97,
          "values": [
            0.97
          ]
        },
        "brevity_compliance": {
          "mean": 0.44000000000000006,
          "std": 0.0,
          "min": 0.44000000000000006,
          "max": 0.44000000000000006,
          "median": 0.44000000000000006,
          "values": [
            0.44000000000000006
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.2-1b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.14005320774344646,
          "std": 0.0,
          "min": 0.14005320774344646,
          "max": 0.14005320774344646,
          "median": 0.14005320774344646,
          "values": [
            0.14005320774344646
          ]
        },
        "recall": {
          "mean": 0.3776197880578145,
          "std": 0.0,
          "min": 0.3776197880578145,
          "max": 0.3776197880578145,
          "median": 0.3776197880578145,
          "values": [
            0.3776197880578145
          ]
        },
        "f1_score": {
          "mean": 0.19510097900672346,
          "std": 0.0,
          "min": 0.19510097900672346,
          "max": 0.19510097900672346,
          "median": 0.19510097900672346,
          "values": [
            0.19510097900672346
          ]
        },
        "jaccard": {
          "mean": 0.11014193094810602,
          "std": 0.0,
          "min": 0.11014193094810602,
          "max": 0.11014193094810602,
          "median": 0.11014193094810602,
          "values": [
            0.11014193094810602
          ]
        },
        "semantic_similarity": {
          "mean": 0.6150726193189621,
          "std": 0.0,
          "min": 0.6150726193189621,
          "max": 0.6150726193189621,
          "median": 0.6150726193189621,
          "values": [
            0.6150726193189621
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.433,
          "std": 0.0,
          "min": 0.433,
          "max": 0.433,
          "median": 0.433,
          "values": [
            0.433
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "cose_self_rationalization_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__u4b-llama3.3-70b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11423420907346031,
          "std": 0.0,
          "min": 0.11423420907346031,
          "max": 0.11423420907346031,
          "median": 0.11423420907346031,
          "values": [
            0.11423420907346031
          ]
        },
        "recall": {
          "mean": 0.4325517097370235,
          "std": 0.0,
          "min": 0.4325517097370235,
          "max": 0.4325517097370235,
          "median": 0.4325517097370235,
          "values": [
            0.4325517097370235
          ]
        },
        "f1_score": {
          "mean": 0.17254248915981138,
          "std": 0.0,
          "min": 0.17254248915981138,
          "max": 0.17254248915981138,
          "median": 0.17254248915981138,
          "values": [
            0.17254248915981138
          ]
        },
        "jaccard": {
          "mean": 0.09609102135918876,
          "std": 0.0,
          "min": 0.09609102135918876,
          "max": 0.09609102135918876,
          "median": 0.09609102135918876,
          "values": [
            0.09609102135918876
          ]
        },
        "semantic_similarity": {
          "mean": 0.557304115742445,
          "std": 0.0,
          "min": 0.557304115742445,
          "max": 0.557304115742445,
          "median": 0.557304115742445,
          "values": [
            0.557304115742445
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.195,
          "std": 0.0,
          "min": 0.195,
          "max": 0.195,
          "median": 0.195,
          "values": [
            0.195
          ]
        },
        "format_cleanliness": {
          "mean": 0.7959999999999999,
          "std": 0.0,
          "min": 0.7959999999999999,
          "max": 0.7959999999999999,
          "median": 0.7959999999999999,
          "values": [
            0.7959999999999999
          ]
        }
      }
    },
    "cose_self_rationalization_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "RHAI_cose_explanation_with_answer_and_questions__u4b-mistral-7b__zero-shot__cose_self_rationalization__100__0p100"
      ],
      "setups": [
        "RHAI_cose_explanation_with_answer_and_questions"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1445024106698146,
          "std": 0.0,
          "min": 0.1445024106698146,
          "max": 0.1445024106698146,
          "median": 0.1445024106698146,
          "values": [
            0.1445024106698146
          ]
        },
        "recall": {
          "mean": 0.4267943266174317,
          "std": 0.0,
          "min": 0.4267943266174317,
          "max": 0.4267943266174317,
          "median": 0.4267943266174317,
          "values": [
            0.4267943266174317
          ]
        },
        "f1_score": {
          "mean": 0.2079784619571892,
          "std": 0.0,
          "min": 0.2079784619571892,
          "max": 0.2079784619571892,
          "median": 0.2079784619571892,
          "values": [
            0.2079784619571892
          ]
        },
        "jaccard": {
          "mean": 0.11794179670312226,
          "std": 0.0,
          "min": 0.11794179670312226,
          "max": 0.11794179670312226,
          "median": 0.11794179670312226,
          "values": [
            0.11794179670312226
          ]
        },
        "semantic_similarity": {
          "mean": 0.6387710550427437,
          "std": 0.0,
          "min": 0.6387710550427437,
          "max": 0.6387710550427437,
          "median": 0.6387710550427437,
          "values": [
            0.6387710550427437
          ]
        },
        "explanation_correctness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "brevity_compliance": {
          "mean": 0.509,
          "std": 0.0,
          "min": 0.509,
          "max": 0.509,
          "median": 0.509,
          "values": [
            0.509
          ]
        },
        "format_cleanliness": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecare_choice_selection_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.020452365268034355,
          "std": 0.0,
          "min": 0.020452365268034355,
          "max": 0.020452365268034355,
          "median": 0.020452365268034355,
          "values": [
            0.020452365268034355
          ]
        },
        "recall": {
          "mean": 0.82,
          "std": 0.0,
          "min": 0.82,
          "max": 0.82,
          "median": 0.82,
          "values": [
            0.82
          ]
        },
        "f1_score": {
          "mean": 0.03984708344301362,
          "std": 0.0,
          "min": 0.03984708344301362,
          "max": 0.03984708344301362,
          "median": 0.03984708344301362,
          "values": [
            0.03984708344301362
          ]
        },
        "jaccard": {
          "mean": 0.020452365268034355,
          "std": 0.0,
          "min": 0.020452365268034355,
          "max": 0.020452365268034355,
          "median": 0.020452365268034355,
          "values": [
            0.020452365268034355
          ]
        },
        "semantic_similarity": {
          "mean": 0.10024355170316994,
          "std": 0.0,
          "min": 0.10024355170316994,
          "max": 0.10024355170316994,
          "median": 0.10024355170316994,
          "values": [
            0.10024355170316994
          ]
        },
        "answer_correctness": {
          "mean": 0.59,
          "std": 0.0,
          "min": 0.59,
          "max": 0.59,
          "median": 0.59,
          "values": [
            0.59
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19799999999999998,
          "std": 0.0,
          "min": 0.19799999999999998,
          "max": 0.19799999999999998,
          "median": 0.19799999999999998,
          "values": [
            0.19799999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8039999999999998,
          "std": 0.0,
          "min": 0.8039999999999998,
          "max": 0.8039999999999998,
          "median": 0.8039999999999998,
          "values": [
            0.8039999999999998
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.005985092345049565,
          "std": 0.0,
          "min": 0.005985092345049565,
          "max": 0.005985092345049565,
          "median": 0.005985092345049565,
          "values": [
            0.005985092345049565
          ]
        },
        "recall": {
          "mean": 0.41,
          "std": 0.0,
          "min": 0.41,
          "max": 0.41,
          "median": 0.41,
          "values": [
            0.41
          ]
        },
        "f1_score": {
          "mean": 0.011789023708306517,
          "std": 0.0,
          "min": 0.011789023708306517,
          "max": 0.011789023708306517,
          "median": 0.011789023708306517,
          "values": [
            0.011789023708306517
          ]
        },
        "jaccard": {
          "mean": 0.005985092345049565,
          "std": 0.0,
          "min": 0.005985092345049565,
          "max": 0.005985092345049565,
          "median": 0.005985092345049565,
          "values": [
            0.005985092345049565
          ]
        },
        "semantic_similarity": {
          "mean": 0.08034662605263293,
          "std": 0.0,
          "min": 0.08034662605263293,
          "max": 0.08034662605263293,
          "median": 0.08034662605263293,
          "values": [
            0.08034662605263293
          ]
        },
        "answer_correctness": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19599999999999995,
          "std": 0.0,
          "min": 0.19599999999999995,
          "max": 0.19599999999999995,
          "median": 0.19599999999999995,
          "values": [
            0.19599999999999995
          ]
        },
        "answer_extractability": {
          "mean": 0.7439999999999998,
          "std": 0.0,
          "min": 0.7439999999999998,
          "max": 0.7439999999999998,
          "median": 0.7439999999999998,
          "values": [
            0.7439999999999998
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008865008079372771,
          "std": 0.0,
          "min": 0.008865008079372771,
          "max": 0.008865008079372771,
          "median": 0.008865008079372771,
          "values": [
            0.008865008079372771
          ]
        },
        "recall": {
          "mean": 0.57,
          "std": 0.0,
          "min": 0.57,
          "max": 0.57,
          "median": 0.57,
          "values": [
            0.57
          ]
        },
        "f1_score": {
          "mean": 0.017433196194303512,
          "std": 0.0,
          "min": 0.017433196194303512,
          "max": 0.017433196194303512,
          "median": 0.017433196194303512,
          "values": [
            0.017433196194303512
          ]
        },
        "jaccard": {
          "mean": 0.008865008079372771,
          "std": 0.0,
          "min": 0.008865008079372771,
          "max": 0.008865008079372771,
          "median": 0.008865008079372771,
          "values": [
            0.008865008079372771
          ]
        },
        "semantic_similarity": {
          "mean": 0.08063821282004938,
          "std": 0.0,
          "min": 0.08063821282004938,
          "max": 0.08063821282004938,
          "median": 0.08063821282004938,
          "values": [
            0.08063821282004938
          ]
        },
        "answer_correctness": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19199999999999995,
          "std": 0.0,
          "min": 0.19199999999999995,
          "max": 0.19199999999999995,
          "median": 0.19199999999999995,
          "values": [
            0.19199999999999995
          ]
        },
        "answer_extractability": {
          "mean": 0.7339999999999999,
          "std": 0.0,
          "min": 0.7339999999999999,
          "max": 0.7339999999999999,
          "median": 0.7339999999999999,
          "values": [
            0.7339999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.004004692475962245,
          "std": 0.0,
          "min": 0.004004692475962245,
          "max": 0.004004692475962245,
          "median": 0.004004692475962245,
          "values": [
            0.004004692475962245
          ]
        },
        "recall": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "f1_score": {
          "mean": 0.00790448794836974,
          "std": 0.0,
          "min": 0.00790448794836974,
          "max": 0.00790448794836974,
          "median": 0.00790448794836974,
          "values": [
            0.00790448794836974
          ]
        },
        "jaccard": {
          "mean": 0.004004692475962245,
          "std": 0.0,
          "min": 0.004004692475962245,
          "max": 0.004004692475962245,
          "median": 0.004004692475962245,
          "values": [
            0.004004692475962245
          ]
        },
        "semantic_similarity": {
          "mean": 0.07979790750890971,
          "std": 0.0,
          "min": 0.07979790750890971,
          "max": 0.07979790750890971,
          "median": 0.07979790750890971,
          "values": [
            0.07979790750890971
          ]
        },
        "answer_correctness": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19599999999999995,
          "std": 0.0,
          "min": 0.19599999999999995,
          "max": 0.19599999999999995,
          "median": 0.19599999999999995,
          "values": [
            0.19599999999999995
          ]
        },
        "answer_extractability": {
          "mean": 0.8159999999999998,
          "std": 0.0,
          "min": 0.8159999999999998,
          "max": 0.8159999999999998,
          "median": 0.8159999999999998,
          "values": [
            0.8159999999999998
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-mistral-nemo-12b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.004203346237664929,
          "std": 0.0,
          "min": 0.004203346237664929,
          "max": 0.004203346237664929,
          "median": 0.004203346237664929,
          "values": [
            0.004203346237664929
          ]
        },
        "recall": {
          "mean": 0.4,
          "std": 0.0,
          "min": 0.4,
          "max": 0.4,
          "median": 0.4,
          "values": [
            0.4
          ]
        },
        "f1_score": {
          "mean": 0.008314979305548883,
          "std": 0.0,
          "min": 0.008314979305548883,
          "max": 0.008314979305548883,
          "median": 0.008314979305548883,
          "values": [
            0.008314979305548883
          ]
        },
        "jaccard": {
          "mean": 0.004203346237664929,
          "std": 0.0,
          "min": 0.004203346237664929,
          "max": 0.004203346237664929,
          "median": 0.004203346237664929,
          "values": [
            0.004203346237664929
          ]
        },
        "semantic_similarity": {
          "mean": 0.06692904483061284,
          "std": 0.0,
          "min": 0.06692904483061284,
          "max": 0.06692904483061284,
          "median": 0.06692904483061284,
          "values": [
            0.06692904483061284
          ]
        },
        "answer_correctness": {
          "mean": 0.49,
          "std": 0.0,
          "min": 0.49,
          "max": 0.49,
          "median": 0.49,
          "values": [
            0.49
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19599999999999995,
          "std": 0.0,
          "min": 0.19599999999999995,
          "max": 0.19599999999999995,
          "median": 0.19599999999999995,
          "values": [
            0.19599999999999995
          ]
        },
        "answer_extractability": {
          "mean": 0.8059999999999999,
          "std": 0.0,
          "min": 0.8059999999999999,
          "max": 0.8059999999999999,
          "median": 0.8059999999999999,
          "values": [
            0.8059999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.05504448497041228,
          "std": 0.0,
          "min": 0.05504448497041228,
          "max": 0.05504448497041228,
          "median": 0.05504448497041228,
          "values": [
            0.05504448497041228
          ]
        },
        "recall": {
          "mean": 0.87,
          "std": 0.0,
          "min": 0.87,
          "max": 0.87,
          "median": 0.87,
          "values": [
            0.87
          ]
        },
        "f1_score": {
          "mean": 0.06951497669012922,
          "std": 0.0,
          "min": 0.06951497669012922,
          "max": 0.06951497669012922,
          "median": 0.06951497669012922,
          "values": [
            0.06951497669012922
          ]
        },
        "jaccard": {
          "mean": 0.05504448497041228,
          "std": 0.0,
          "min": 0.05504448497041228,
          "max": 0.05504448497041228,
          "median": 0.05504448497041228,
          "values": [
            0.05504448497041228
          ]
        },
        "semantic_similarity": {
          "mean": 0.20189217027276754,
          "std": 0.0,
          "min": 0.20189217027276754,
          "max": 0.20189217027276754,
          "median": 0.20189217027276754,
          "values": [
            0.20189217027276754
          ]
        },
        "answer_correctness": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "follows_format_instruction": {
          "mean": 0.256,
          "std": 0.0,
          "min": 0.256,
          "max": 0.256,
          "median": 0.256,
          "values": [
            0.256
          ]
        },
        "answer_extractability": {
          "mean": 0.8044,
          "std": 0.0,
          "min": 0.8044,
          "max": 0.8044,
          "median": 0.8044,
          "values": [
            0.8044
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.004097150807079523,
          "std": 0.0,
          "min": 0.004097150807079523,
          "max": 0.004097150807079523,
          "median": 0.004097150807079523,
          "values": [
            0.004097150807079523
          ]
        },
        "recall": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "f1_score": {
          "mean": 0.008099777960585737,
          "std": 0.0,
          "min": 0.008099777960585737,
          "max": 0.008099777960585737,
          "median": 0.008099777960585737,
          "values": [
            0.008099777960585737
          ]
        },
        "jaccard": {
          "mean": 0.004097150807079523,
          "std": 0.0,
          "min": 0.004097150807079523,
          "max": 0.004097150807079523,
          "median": 0.004097150807079523,
          "values": [
            0.004097150807079523
          ]
        },
        "semantic_similarity": {
          "mean": 0.08523711950518191,
          "std": 0.0,
          "min": 0.08523711950518191,
          "max": 0.08523711950518191,
          "median": 0.08523711950518191,
          "values": [
            0.08523711950518191
          ]
        },
        "answer_correctness": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.8271999999999999,
          "std": 0.0,
          "min": 0.8271999999999999,
          "max": 0.8271999999999999,
          "median": 0.8271999999999999,
          "values": [
            0.8271999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008009306615740935,
          "std": 0.0,
          "min": 0.008009306615740935,
          "max": 0.008009306615740935,
          "median": 0.008009306615740935,
          "values": [
            0.008009306615740935
          ]
        },
        "recall": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.015820790405372404,
          "std": 0.0,
          "min": 0.015820790405372404,
          "max": 0.015820790405372404,
          "median": 0.015820790405372404,
          "values": [
            0.015820790405372404
          ]
        },
        "jaccard": {
          "mean": 0.008009306615740935,
          "std": 0.0,
          "min": 0.008009306615740935,
          "max": 0.008009306615740935,
          "median": 0.008009306615740935,
          "values": [
            0.008009306615740935
          ]
        },
        "semantic_similarity": {
          "mean": 0.0891882423334755,
          "std": 0.0,
          "min": 0.0891882423334755,
          "max": 0.0891882423334755,
          "median": 0.0891882423334755,
          "values": [
            0.0891882423334755
          ]
        },
        "answer_correctness": {
          "mean": 0.46,
          "std": 0.0,
          "min": 0.46,
          "max": 0.46,
          "median": 0.46,
          "values": [
            0.46
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.8079999999999999,
          "std": 0.0,
          "min": 0.8079999999999999,
          "max": 0.8079999999999999,
          "median": 0.8079999999999999,
          "values": [
            0.8079999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0035154361016346003,
          "std": 0.0,
          "min": 0.0035154361016346003,
          "max": 0.0035154361016346003,
          "median": 0.0035154361016346003,
          "values": [
            0.0035154361016346003
          ]
        },
        "recall": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "f1_score": {
          "mean": 0.0069559761868289,
          "std": 0.0,
          "min": 0.0069559761868289,
          "max": 0.0069559761868289,
          "median": 0.0069559761868289,
          "values": [
            0.0069559761868289
          ]
        },
        "jaccard": {
          "mean": 0.0035154361016346003,
          "std": 0.0,
          "min": 0.0035154361016346003,
          "max": 0.0035154361016346003,
          "median": 0.0035154361016346003,
          "values": [
            0.0035154361016346003
          ]
        },
        "semantic_similarity": {
          "mean": 0.08658209849148989,
          "std": 0.0,
          "min": 0.08658209849148989,
          "max": 0.08658209849148989,
          "median": 0.08658209849148989,
          "values": [
            0.08658209849148989
          ]
        },
        "answer_correctness": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.9076000000000001,
          "std": 0.0,
          "min": 0.9076000000000001,
          "max": 0.9076000000000001,
          "median": 0.9076000000000001,
          "values": [
            0.9076000000000001
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__hf-qwen2.5-3b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.003621677298628642,
          "std": 0.0,
          "min": 0.003621677298628642,
          "max": 0.003621677298628642,
          "median": 0.003621677298628642,
          "values": [
            0.003621677298628642
          ]
        },
        "recall": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        },
        "f1_score": {
          "mean": 0.007172080855749655,
          "std": 0.0,
          "min": 0.007172080855749655,
          "max": 0.007172080855749655,
          "median": 0.007172080855749655,
          "values": [
            0.007172080855749655
          ]
        },
        "jaccard": {
          "mean": 0.003621677298628642,
          "std": 0.0,
          "min": 0.003621677298628642,
          "max": 0.003621677298628642,
          "median": 0.003621677298628642,
          "values": [
            0.003621677298628642
          ]
        },
        "semantic_similarity": {
          "mean": 0.0709951462643221,
          "std": 0.0,
          "min": 0.0709951462643221,
          "max": 0.0709951462643221,
          "median": 0.0709951462643221,
          "values": [
            0.0709951462643221
          ]
        },
        "answer_correctness": {
          "mean": 0.46,
          "std": 0.0,
          "min": 0.46,
          "max": 0.46,
          "median": 0.46,
          "values": [
            0.46
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.862,
          "std": 0.0,
          "min": 0.862,
          "max": 0.862,
          "median": 0.862,
          "values": [
            0.862
          ]
        }
      }
    },
    "ecare_choice_selection_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.004871921526766584,
          "std": 0.0,
          "min": 0.004871921526766584,
          "max": 0.004871921526766584,
          "median": 0.004871921526766584,
          "values": [
            0.004871921526766584
          ]
        },
        "recall": {
          "mean": 0.21,
          "std": 0.0,
          "min": 0.21,
          "max": 0.21,
          "median": 0.21,
          "values": [
            0.21
          ]
        },
        "f1_score": {
          "mean": 0.009484041067423092,
          "std": 0.0,
          "min": 0.009484041067423092,
          "max": 0.009484041067423092,
          "median": 0.009484041067423092,
          "values": [
            0.009484041067423092
          ]
        },
        "jaccard": {
          "mean": 0.004871921526766584,
          "std": 0.0,
          "min": 0.004871921526766584,
          "max": 0.004871921526766584,
          "median": 0.004871921526766584,
          "values": [
            0.004871921526766584
          ]
        },
        "semantic_similarity": {
          "mean": 0.12052421312779188,
          "std": 0.0,
          "min": 0.12052421312779188,
          "max": 0.12052421312779188,
          "median": 0.12052421312779188,
          "values": [
            0.12052421312779188
          ]
        },
        "answer_correctness": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "follows_format_instruction": {
          "mean": 0.17999999999999994,
          "std": 0.0,
          "min": 0.17999999999999994,
          "max": 0.17999999999999994,
          "median": 0.17999999999999994,
          "values": [
            0.17999999999999994
          ]
        },
        "answer_extractability": {
          "mean": 0.7364,
          "std": 0.0,
          "min": 0.7364,
          "max": 0.7364,
          "median": 0.7364,
          "values": [
            0.7364
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0006477361291654041,
          "std": 0.0,
          "min": 0.0006477361291654041,
          "max": 0.0006477361291654041,
          "median": 0.0006477361291654041,
          "values": [
            0.0006477361291654041
          ]
        },
        "recall": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "f1_score": {
          "mean": 0.0012807210728379142,
          "std": 0.0,
          "min": 0.0012807210728379142,
          "max": 0.0012807210728379142,
          "median": 0.0012807210728379142,
          "values": [
            0.0012807210728379142
          ]
        },
        "jaccard": {
          "mean": 0.0006477361291654041,
          "std": 0.0,
          "min": 0.0006477361291654041,
          "max": 0.0006477361291654041,
          "median": 0.0006477361291654041,
          "values": [
            0.0006477361291654041
          ]
        },
        "semantic_similarity": {
          "mean": 0.06372932297177612,
          "std": 0.0,
          "min": 0.06372932297177612,
          "max": 0.06372932297177612,
          "median": 0.06372932297177612,
          "values": [
            0.06372932297177612
          ]
        },
        "answer_correctness": {
          "mean": 0.27,
          "std": 0.0,
          "min": 0.27,
          "max": 0.27,
          "median": 0.27,
          "values": [
            0.27
          ]
        },
        "follows_format_instruction": {
          "mean": 0.13599999999999998,
          "std": 0.0,
          "min": 0.13599999999999998,
          "max": 0.13599999999999998,
          "median": 0.13599999999999998,
          "values": [
            0.13599999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.5259999999999999,
          "std": 0.0,
          "min": 0.5259999999999999,
          "max": 0.5259999999999999,
          "median": 0.5259999999999999,
          "values": [
            0.5259999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0016166789530344982,
          "std": 0.0,
          "min": 0.0016166789530344982,
          "max": 0.0016166789530344982,
          "median": 0.0016166789530344982,
          "values": [
            0.0016166789530344982
          ]
        },
        "recall": {
          "mean": 0.14,
          "std": 0.0,
          "min": 0.14,
          "max": 0.14,
          "median": 0.14,
          "values": [
            0.14
          ]
        },
        "f1_score": {
          "mean": 0.0031952622714167545,
          "std": 0.0,
          "min": 0.0031952622714167545,
          "max": 0.0031952622714167545,
          "median": 0.0031952622714167545,
          "values": [
            0.0031952622714167545
          ]
        },
        "jaccard": {
          "mean": 0.0016166789530344982,
          "std": 0.0,
          "min": 0.0016166789530344982,
          "max": 0.0016166789530344982,
          "median": 0.0016166789530344982,
          "values": [
            0.0016166789530344982
          ]
        },
        "semantic_similarity": {
          "mean": 0.04626693273894489,
          "std": 0.0,
          "min": 0.04626693273894489,
          "max": 0.04626693273894489,
          "median": 0.04626693273894489,
          "values": [
            0.04626693273894489
          ]
        },
        "answer_correctness": {
          "mean": 0.46,
          "std": 0.0,
          "min": 0.46,
          "max": 0.46,
          "median": 0.46,
          "values": [
            0.46
          ]
        },
        "follows_format_instruction": {
          "mean": 0.17999999999999997,
          "std": 0.0,
          "min": 0.17999999999999997,
          "max": 0.17999999999999997,
          "median": 0.17999999999999997,
          "values": [
            0.17999999999999997
          ]
        },
        "answer_extractability": {
          "mean": 0.7467999999999999,
          "std": 0.0,
          "min": 0.7467999999999999,
          "max": 0.7467999999999999,
          "median": 0.7467999999999999,
          "values": [
            0.7467999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0004219207887564308,
          "std": 0.0,
          "min": 0.0004219207887564308,
          "max": 0.0004219207887564308,
          "median": 0.0004219207887564308,
          "values": [
            0.0004219207887564308
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.0008367523270071996,
          "std": 0.0,
          "min": 0.0008367523270071996,
          "max": 0.0008367523270071996,
          "median": 0.0008367523270071996,
          "values": [
            0.0008367523270071996
          ]
        },
        "jaccard": {
          "mean": 0.0004219207887564308,
          "std": 0.0,
          "min": 0.0004219207887564308,
          "max": 0.0004219207887564308,
          "median": 0.0004219207887564308,
          "values": [
            0.0004219207887564308
          ]
        },
        "semantic_similarity": {
          "mean": 0.05541439942084253,
          "std": 0.0,
          "min": 0.05541439942084253,
          "max": 0.05541439942084253,
          "median": 0.05541439942084253,
          "values": [
            0.05541439942084253
          ]
        },
        "answer_correctness": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19599999999999995,
          "std": 0.0,
          "min": 0.19599999999999995,
          "max": 0.19599999999999995,
          "median": 0.19599999999999995,
          "values": [
            0.19599999999999995
          ]
        },
        "answer_extractability": {
          "mean": 0.7900000000000001,
          "std": 0.0,
          "min": 0.7900000000000001,
          "max": 0.7900000000000001,
          "median": 0.7900000000000001,
          "values": [
            0.7900000000000001
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3-8b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0005192621365700335,
          "std": 0.0,
          "min": 0.0005192621365700335,
          "max": 0.0005192621365700335,
          "median": 0.0005192621365700335,
          "values": [
            0.0005192621365700335
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.0010277187254434798,
          "std": 0.0,
          "min": 0.0010277187254434798,
          "max": 0.0010277187254434798,
          "median": 0.0010277187254434798,
          "values": [
            0.0010277187254434798
          ]
        },
        "jaccard": {
          "mean": 0.0005192621365700335,
          "std": 0.0,
          "min": 0.0005192621365700335,
          "max": 0.0005192621365700335,
          "median": 0.0005192621365700335,
          "values": [
            0.0005192621365700335
          ]
        },
        "semantic_similarity": {
          "mean": 0.059290827624499796,
          "std": 0.0,
          "min": 0.059290827624499796,
          "max": 0.059290827624499796,
          "median": 0.059290827624499796,
          "values": [
            0.059290827624499796
          ]
        },
        "answer_correctness": {
          "mean": 0.41,
          "std": 0.0,
          "min": 0.41,
          "max": 0.41,
          "median": 0.41,
          "values": [
            0.41
          ]
        },
        "follows_format_instruction": {
          "mean": 0.17399999999999996,
          "std": 0.0,
          "min": 0.17399999999999996,
          "max": 0.17399999999999996,
          "median": 0.17399999999999996,
          "values": [
            0.17399999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.5954,
          "std": 0.0,
          "min": 0.5954,
          "max": 0.5954,
          "median": 0.5954,
          "values": [
            0.5954
          ]
        }
      }
    },
    "ecare_choice_selection_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.009739789776005309,
          "std": 0.0,
          "min": 0.009739789776005309,
          "max": 0.009739789776005309,
          "median": 0.009739789776005309,
          "values": [
            0.009739789776005309
          ]
        },
        "recall": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "f1_score": {
          "mean": 0.019050075906730475,
          "std": 0.0,
          "min": 0.019050075906730475,
          "max": 0.019050075906730475,
          "median": 0.019050075906730475,
          "values": [
            0.019050075906730475
          ]
        },
        "jaccard": {
          "mean": 0.009739789776005309,
          "std": 0.0,
          "min": 0.009739789776005309,
          "max": 0.009739789776005309,
          "median": 0.009739789776005309,
          "values": [
            0.009739789776005309
          ]
        },
        "semantic_similarity": {
          "mean": 0.09544986858265475,
          "std": 0.0,
          "min": 0.09544986858265475,
          "max": 0.09544986858265475,
          "median": 0.09544986858265475,
          "values": [
            0.09544986858265475
          ]
        },
        "answer_correctness": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "follows_format_instruction": {
          "mean": 0.16999999999999996,
          "std": 0.0,
          "min": 0.16999999999999996,
          "max": 0.16999999999999996,
          "median": 0.16999999999999996,
          "values": [
            0.16999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.836,
          "std": 0.0,
          "min": 0.836,
          "max": 0.836,
          "median": 0.836,
          "values": [
            0.836
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0031231891411951083,
          "std": 0.0,
          "min": 0.0031231891411951083,
          "max": 0.0031231891411951083,
          "median": 0.0031231891411951083,
          "values": [
            0.0031231891411951083
          ]
        },
        "recall": {
          "mean": 0.23,
          "std": 0.0,
          "min": 0.23,
          "max": 0.23,
          "median": 0.23,
          "values": [
            0.23
          ]
        },
        "f1_score": {
          "mean": 0.006159422614456698,
          "std": 0.0,
          "min": 0.006159422614456698,
          "max": 0.006159422614456698,
          "median": 0.006159422614456698,
          "values": [
            0.006159422614456698
          ]
        },
        "jaccard": {
          "mean": 0.0031231891411951083,
          "std": 0.0,
          "min": 0.0031231891411951083,
          "max": 0.0031231891411951083,
          "median": 0.0031231891411951083,
          "values": [
            0.0031231891411951083
          ]
        },
        "semantic_similarity": {
          "mean": 0.07076871836092323,
          "std": 0.0,
          "min": 0.07076871836092323,
          "max": 0.07076871836092323,
          "median": 0.07076871836092323,
          "values": [
            0.07076871836092323
          ]
        },
        "answer_correctness": {
          "mean": 0.29,
          "std": 0.0,
          "min": 0.29,
          "max": 0.29,
          "median": 0.29,
          "values": [
            0.29
          ]
        },
        "follows_format_instruction": {
          "mean": 0.11299999999999998,
          "std": 0.0,
          "min": 0.11299999999999998,
          "max": 0.11299999999999998,
          "median": 0.11299999999999998,
          "values": [
            0.11299999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.48800000000000004,
          "std": 0.0,
          "min": 0.48800000000000004,
          "max": 0.48800000000000004,
          "median": 0.48800000000000004,
          "values": [
            0.48800000000000004
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0039019365517186364,
          "std": 0.0,
          "min": 0.0039019365517186364,
          "max": 0.0039019365517186364,
          "median": 0.0039019365517186364,
          "values": [
            0.0039019365517186364
          ]
        },
        "recall": {
          "mean": 0.22,
          "std": 0.0,
          "min": 0.22,
          "max": 0.22,
          "median": 0.22,
          "values": [
            0.22
          ]
        },
        "f1_score": {
          "mean": 0.007605957379241352,
          "std": 0.0,
          "min": 0.007605957379241352,
          "max": 0.007605957379241352,
          "median": 0.007605957379241352,
          "values": [
            0.007605957379241352
          ]
        },
        "jaccard": {
          "mean": 0.0039019365517186364,
          "std": 0.0,
          "min": 0.0039019365517186364,
          "max": 0.0039019365517186364,
          "median": 0.0039019365517186364,
          "values": [
            0.0039019365517186364
          ]
        },
        "semantic_similarity": {
          "mean": 0.06017413009889424,
          "std": 0.0,
          "min": 0.06017413009889424,
          "max": 0.06017413009889424,
          "median": 0.06017413009889424,
          "values": [
            0.06017413009889424
          ]
        },
        "answer_correctness": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "follows_format_instruction": {
          "mean": 0.16599999999999998,
          "std": 0.0,
          "min": 0.16599999999999998,
          "max": 0.16599999999999998,
          "median": 0.16599999999999998,
          "values": [
            0.16599999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.758,
          "std": 0.0,
          "min": 0.758,
          "max": 0.758,
          "median": 0.758,
          "values": [
            0.758
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00569389276392284,
          "std": 0.0,
          "min": 0.00569389276392284,
          "max": 0.00569389276392284,
          "median": 0.00569389276392284,
          "values": [
            0.00569389276392284
          ]
        },
        "recall": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "f1_score": {
          "mean": 0.011139450081063778,
          "std": 0.0,
          "min": 0.011139450081063778,
          "max": 0.011139450081063778,
          "median": 0.011139450081063778,
          "values": [
            0.011139450081063778
          ]
        },
        "jaccard": {
          "mean": 0.00569389276392284,
          "std": 0.0,
          "min": 0.00569389276392284,
          "max": 0.00569389276392284,
          "median": 0.00569389276392284,
          "values": [
            0.00569389276392284
          ]
        },
        "semantic_similarity": {
          "mean": 0.1094636170216836,
          "std": 0.0,
          "min": 0.1094636170216836,
          "max": 0.1094636170216836,
          "median": 0.1094636170216836,
          "values": [
            0.1094636170216836
          ]
        },
        "answer_correctness": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "follows_format_instruction": {
          "mean": 0.18699999999999994,
          "std": 0.0,
          "min": 0.18699999999999994,
          "max": 0.18699999999999994,
          "median": 0.18699999999999994,
          "values": [
            0.18699999999999994
          ]
        },
        "answer_extractability": {
          "mean": 0.7447999999999999,
          "std": 0.0,
          "min": 0.7447999999999999,
          "max": 0.7447999999999999,
          "median": 0.7447999999999999,
          "values": [
            0.7447999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.2-1b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0055592669872044235,
          "std": 0.0,
          "min": 0.0055592669872044235,
          "max": 0.0055592669872044235,
          "median": 0.0055592669872044235,
          "values": [
            0.0055592669872044235
          ]
        },
        "recall": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "f1_score": {
          "mean": 0.01094425530777698,
          "std": 0.0,
          "min": 0.01094425530777698,
          "max": 0.01094425530777698,
          "median": 0.01094425530777698,
          "values": [
            0.01094425530777698
          ]
        },
        "jaccard": {
          "mean": 0.0055592669872044235,
          "std": 0.0,
          "min": 0.0055592669872044235,
          "max": 0.0055592669872044235,
          "median": 0.0055592669872044235,
          "values": [
            0.0055592669872044235
          ]
        },
        "semantic_similarity": {
          "mean": 0.10359871821478009,
          "std": 0.0,
          "min": 0.10359871821478009,
          "max": 0.10359871821478009,
          "median": 0.10359871821478009,
          "values": [
            0.10359871821478009
          ]
        },
        "answer_correctness": {
          "mean": 0.3,
          "std": 0.0,
          "min": 0.3,
          "max": 0.3,
          "median": 0.3,
          "values": [
            0.3
          ]
        },
        "follows_format_instruction": {
          "mean": 0.13799999999999998,
          "std": 0.0,
          "min": 0.13799999999999998,
          "max": 0.13799999999999998,
          "median": 0.13799999999999998,
          "values": [
            0.13799999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.637,
          "std": 0.0,
          "min": 0.637,
          "max": 0.637,
          "median": 0.637,
          "values": [
            0.637
          ]
        }
      }
    },
    "ecare_choice_selection_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.021432659648304653,
          "std": 0.0,
          "min": 0.021432659648304653,
          "max": 0.021432659648304653,
          "median": 0.021432659648304653,
          "values": [
            0.021432659648304653
          ]
        },
        "recall": {
          "mean": 0.92,
          "std": 0.0,
          "min": 0.92,
          "max": 0.92,
          "median": 0.92,
          "values": [
            0.92
          ]
        },
        "f1_score": {
          "mean": 0.03951849233226848,
          "std": 0.0,
          "min": 0.03951849233226848,
          "max": 0.03951849233226848,
          "median": 0.03951849233226848,
          "values": [
            0.03951849233226848
          ]
        },
        "jaccard": {
          "mean": 0.021432659648304653,
          "std": 0.0,
          "min": 0.021432659648304653,
          "max": 0.021432659648304653,
          "median": 0.021432659648304653,
          "values": [
            0.021432659648304653
          ]
        },
        "semantic_similarity": {
          "mean": 0.18492254968732597,
          "std": 0.0,
          "min": 0.18492254968732597,
          "max": 0.18492254968732597,
          "median": 0.18492254968732597,
          "values": [
            0.18492254968732597
          ]
        },
        "answer_correctness": {
          "mean": 0.58,
          "std": 0.0,
          "min": 0.58,
          "max": 0.58,
          "median": 0.58,
          "values": [
            0.58
          ]
        },
        "follows_format_instruction": {
          "mean": 0.22099999999999997,
          "std": 0.0,
          "min": 0.22099999999999997,
          "max": 0.22099999999999997,
          "median": 0.22099999999999997,
          "values": [
            0.22099999999999997
          ]
        },
        "answer_extractability": {
          "mean": 0.8719999999999999,
          "std": 0.0,
          "min": 0.8719999999999999,
          "max": 0.8719999999999999,
          "median": 0.8719999999999999,
          "values": [
            0.8719999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008092042376722132,
          "std": 0.0,
          "min": 0.008092042376722132,
          "max": 0.008092042376722132,
          "median": 0.008092042376722132,
          "values": [
            0.008092042376722132
          ]
        },
        "recall": {
          "mean": 0.81,
          "std": 0.0,
          "min": 0.81,
          "max": 0.81,
          "median": 0.81,
          "values": [
            0.81
          ]
        },
        "f1_score": {
          "mean": 0.01601538833777067,
          "std": 0.0,
          "min": 0.01601538833777067,
          "max": 0.01601538833777067,
          "median": 0.01601538833777067,
          "values": [
            0.01601538833777067
          ]
        },
        "jaccard": {
          "mean": 0.008092042376722132,
          "std": 0.0,
          "min": 0.008092042376722132,
          "max": 0.008092042376722132,
          "median": 0.008092042376722132,
          "values": [
            0.008092042376722132
          ]
        },
        "semantic_similarity": {
          "mean": 0.11910263258963823,
          "std": 0.0,
          "min": 0.11910263258963823,
          "max": 0.11910263258963823,
          "median": 0.11910263258963823,
          "values": [
            0.11910263258963823
          ]
        },
        "answer_correctness": {
          "mean": 0.53,
          "std": 0.0,
          "min": 0.53,
          "max": 0.53,
          "median": 0.53,
          "values": [
            0.53
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.8539999999999999,
          "std": 0.0,
          "min": 0.8539999999999999,
          "max": 0.8539999999999999,
          "median": 0.8539999999999999,
          "values": [
            0.8539999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "precision": {
          "mean": 0.02470466625104554,
          "std": 0.0,
          "min": 0.02470466625104554,
          "max": 0.02470466625104554,
          "median": 0.02470466625104554,
          "values": [
            0.02470466625104554
          ]
        },
        "recall": {
          "mean": 0.49,
          "std": 0.0,
          "min": 0.49,
          "max": 0.49,
          "median": 0.49,
          "values": [
            0.49
          ]
        },
        "f1_score": {
          "mean": 0.029313988628386235,
          "std": 0.0,
          "min": 0.029313988628386235,
          "max": 0.029313988628386235,
          "median": 0.029313988628386235,
          "values": [
            0.029313988628386235
          ]
        },
        "jaccard": {
          "mean": 0.02470466625104554,
          "std": 0.0,
          "min": 0.02470466625104554,
          "max": 0.02470466625104554,
          "median": 0.02470466625104554,
          "values": [
            0.02470466625104554
          ]
        },
        "semantic_similarity": {
          "mean": 0.10996505566872657,
          "std": 0.0,
          "min": 0.10996505566872657,
          "max": 0.10996505566872657,
          "median": 0.10996505566872657,
          "values": [
            0.10996505566872657
          ]
        },
        "answer_correctness": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.21599999999999994,
          "std": 0.0,
          "min": 0.21599999999999994,
          "max": 0.21599999999999994,
          "median": 0.21599999999999994,
          "values": [
            0.21599999999999994
          ]
        },
        "answer_extractability": {
          "mean": 0.8259999999999998,
          "std": 0.0,
          "min": 0.8259999999999998,
          "max": 0.8259999999999998,
          "median": 0.8259999999999998,
          "values": [
            0.8259999999999998
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.016072413841084866,
          "std": 0.0,
          "min": 0.016072413841084866,
          "max": 0.016072413841084866,
          "median": 0.016072413841084866,
          "values": [
            0.016072413841084866
          ]
        },
        "recall": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "f1_score": {
          "mean": 0.022026359114911825,
          "std": 0.0,
          "min": 0.022026359114911825,
          "max": 0.022026359114911825,
          "median": 0.022026359114911825,
          "values": [
            0.022026359114911825
          ]
        },
        "jaccard": {
          "mean": 0.016072413841084866,
          "std": 0.0,
          "min": 0.016072413841084866,
          "max": 0.016072413841084866,
          "median": 0.016072413841084866,
          "values": [
            0.016072413841084866
          ]
        },
        "semantic_similarity": {
          "mean": 0.10301524182315916,
          "std": 0.0,
          "min": 0.10301524182315916,
          "max": 0.10301524182315916,
          "median": 0.10301524182315916,
          "values": [
            0.10301524182315916
          ]
        },
        "answer_correctness": {
          "mean": 0.46,
          "std": 0.0,
          "min": 0.46,
          "max": 0.46,
          "median": 0.46,
          "values": [
            0.46
          ]
        },
        "follows_format_instruction": {
          "mean": 0.20799999999999996,
          "std": 0.0,
          "min": 0.20799999999999996,
          "max": 0.20799999999999996,
          "median": 0.20799999999999996,
          "values": [
            0.20799999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.7899999999999998,
          "std": 0.0,
          "min": 0.7899999999999998,
          "max": 0.7899999999999998,
          "median": 0.7899999999999998,
          "values": [
            0.7899999999999998
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "precision": {
          "mean": 0.029661899198626177,
          "std": 0.0,
          "min": 0.029661899198626177,
          "max": 0.029661899198626177,
          "median": 0.029661899198626177,
          "values": [
            0.029661899198626177
          ]
        },
        "recall": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "f1_score": {
          "mean": 0.03762034093377349,
          "std": 0.0,
          "min": 0.03762034093377349,
          "max": 0.03762034093377349,
          "median": 0.03762034093377349,
          "values": [
            0.03762034093377349
          ]
        },
        "jaccard": {
          "mean": 0.029661899198626177,
          "std": 0.0,
          "min": 0.029661899198626177,
          "max": 0.029661899198626177,
          "median": 0.029661899198626177,
          "values": [
            0.029661899198626177
          ]
        },
        "semantic_similarity": {
          "mean": 0.11566578415920958,
          "std": 0.0,
          "min": 0.11566578415920958,
          "max": 0.11566578415920958,
          "median": 0.11566578415920958,
          "values": [
            0.11566578415920958
          ]
        },
        "answer_correctness": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.23299999999999998,
          "std": 0.0,
          "min": 0.23299999999999998,
          "max": 0.23299999999999998,
          "median": 0.23299999999999998,
          "values": [
            0.23299999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8439999999999999,
          "std": 0.0,
          "min": 0.8439999999999999,
          "max": 0.8439999999999999,
          "median": 0.8439999999999999,
          "values": [
            0.8439999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.11,
          "std": 0.0,
          "min": 0.11,
          "max": 0.11,
          "median": 0.11,
          "values": [
            0.11
          ]
        },
        "precision": {
          "mean": 0.13377257475840934,
          "std": 0.0,
          "min": 0.13377257475840934,
          "max": 0.13377257475840934,
          "median": 0.13377257475840934,
          "values": [
            0.13377257475840934
          ]
        },
        "recall": {
          "mean": 0.75,
          "std": 0.0,
          "min": 0.75,
          "max": 0.75,
          "median": 0.75,
          "values": [
            0.75
          ]
        },
        "f1_score": {
          "mean": 0.1530097313245453,
          "std": 0.0,
          "min": 0.1530097313245453,
          "max": 0.1530097313245453,
          "median": 0.1530097313245453,
          "values": [
            0.1530097313245453
          ]
        },
        "jaccard": {
          "mean": 0.13377257475840934,
          "std": 0.0,
          "min": 0.13377257475840934,
          "max": 0.13377257475840934,
          "median": 0.13377257475840934,
          "values": [
            0.13377257475840934
          ]
        },
        "semantic_similarity": {
          "mean": 0.25062844025669617,
          "std": 0.0,
          "min": 0.25062844025669617,
          "max": 0.25062844025669617,
          "median": 0.25062844025669617,
          "values": [
            0.25062844025669617
          ]
        },
        "answer_correctness": {
          "mean": 0.58,
          "std": 0.0,
          "min": 0.58,
          "max": 0.58,
          "median": 0.58,
          "values": [
            0.58
          ]
        },
        "follows_format_instruction": {
          "mean": 0.36500000000000016,
          "std": 0.0,
          "min": 0.36500000000000016,
          "max": 0.36500000000000016,
          "median": 0.36500000000000016,
          "values": [
            0.36500000000000016
          ]
        },
        "answer_extractability": {
          "mean": 0.9619999999999999,
          "std": 0.0,
          "min": 0.9619999999999999,
          "max": 0.9619999999999999,
          "median": 0.9619999999999999,
          "values": [
            0.9619999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_demographic_young_american_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "precision": {
          "mean": 0.08278881113019149,
          "std": 0.0,
          "min": 0.08278881113019149,
          "max": 0.08278881113019149,
          "median": 0.08278881113019149,
          "values": [
            0.08278881113019149
          ]
        },
        "recall": {
          "mean": 0.76,
          "std": 0.0,
          "min": 0.76,
          "max": 0.76,
          "median": 0.76,
          "values": [
            0.76
          ]
        },
        "f1_score": {
          "mean": 0.10109790316320959,
          "std": 0.0,
          "min": 0.10109790316320959,
          "max": 0.10109790316320959,
          "median": 0.10109790316320959,
          "values": [
            0.10109790316320959
          ]
        },
        "jaccard": {
          "mean": 0.08278881113019149,
          "std": 0.0,
          "min": 0.08278881113019149,
          "max": 0.08278881113019149,
          "median": 0.08278881113019149,
          "values": [
            0.08278881113019149
          ]
        },
        "semantic_similarity": {
          "mean": 0.15466225712327286,
          "std": 0.0,
          "min": 0.15466225712327286,
          "max": 0.15466225712327286,
          "median": 0.15466225712327286,
          "values": [
            0.15466225712327286
          ]
        },
        "answer_correctness": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "follows_format_instruction": {
          "mean": 0.274,
          "std": 0.0,
          "min": 0.274,
          "max": 0.274,
          "median": 0.274,
          "values": [
            0.274
          ]
        },
        "answer_extractability": {
          "mean": 0.9460000000000001,
          "std": 0.0,
          "min": 0.9460000000000001,
          "max": 0.9460000000000001,
          "median": 0.9460000000000001,
          "values": [
            0.9460000000000001
          ]
        }
      }
    },
    "ecare_choice_selection_expert_medical_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02402122648029591,
          "std": 0.0,
          "min": 0.02402122648029591,
          "max": 0.02402122648029591,
          "median": 0.02402122648029591,
          "values": [
            0.02402122648029591
          ]
        },
        "recall": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "f1_score": {
          "mean": 0.042532834690145994,
          "std": 0.0,
          "min": 0.042532834690145994,
          "max": 0.042532834690145994,
          "median": 0.042532834690145994,
          "values": [
            0.042532834690145994
          ]
        },
        "jaccard": {
          "mean": 0.02402122648029591,
          "std": 0.0,
          "min": 0.02402122648029591,
          "max": 0.02402122648029591,
          "median": 0.02402122648029591,
          "values": [
            0.02402122648029591
          ]
        },
        "semantic_similarity": {
          "mean": 0.07868204438360409,
          "std": 0.0,
          "min": 0.07868204438360409,
          "max": 0.07868204438360409,
          "median": 0.07868204438360409,
          "values": [
            0.07868204438360409
          ]
        },
        "answer_correctness": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.206,
          "std": 0.0,
          "min": 0.206,
          "max": 0.206,
          "median": 0.206,
          "values": [
            0.206
          ]
        },
        "answer_extractability": {
          "mean": 0.9359999999999999,
          "std": 0.0,
          "min": 0.9359999999999999,
          "max": 0.9359999999999999,
          "median": 0.9359999999999999,
          "values": [
            0.9359999999999999
          ]
        }
      }
    },
    "ecare_choice_selection_expert_psychologist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.02139867556280669,
          "std": 0.0,
          "min": 0.02139867556280669,
          "max": 0.02139867556280669,
          "median": 0.02139867556280669,
          "values": [
            0.02139867556280669
          ]
        },
        "recall": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "f1_score": {
          "mean": 0.0324165497585378,
          "std": 0.0,
          "min": 0.0324165497585378,
          "max": 0.0324165497585378,
          "median": 0.0324165497585378,
          "values": [
            0.0324165497585378
          ]
        },
        "jaccard": {
          "mean": 0.02139867556280669,
          "std": 0.0,
          "min": 0.02139867556280669,
          "max": 0.02139867556280669,
          "median": 0.02139867556280669,
          "values": [
            0.02139867556280669
          ]
        },
        "semantic_similarity": {
          "mean": 0.08075127707328647,
          "std": 0.0,
          "min": 0.08075127707328647,
          "max": 0.08075127707328647,
          "median": 0.08075127707328647,
          "values": [
            0.08075127707328647
          ]
        },
        "answer_correctness": {
          "mean": 0.49,
          "std": 0.0,
          "min": 0.49,
          "max": 0.49,
          "median": 0.49,
          "values": [
            0.49
          ]
        },
        "follows_format_instruction": {
          "mean": 0.19999999999999996,
          "std": 0.0,
          "min": 0.19999999999999996,
          "max": 0.19999999999999996,
          "median": 0.19999999999999996,
          "values": [
            0.19999999999999996
          ]
        },
        "answer_extractability": {
          "mean": 0.88,
          "std": 0.0,
          "min": 0.88,
          "max": 0.88,
          "median": 0.88,
          "values": [
            0.88
          ]
        }
      }
    },
    "ecare_choice_selection_stakeholder_parent_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_choice_selection__u4b-mistral-7b__zero-shot__ecare_choice_selection_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_choice_selection"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "precision": {
          "mean": 0.04214882710953342,
          "std": 0.0,
          "min": 0.04214882710953342,
          "max": 0.04214882710953342,
          "median": 0.04214882710953342,
          "values": [
            0.04214882710953342
          ]
        },
        "recall": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.05364486570022528,
          "std": 0.0,
          "min": 0.05364486570022528,
          "max": 0.05364486570022528,
          "median": 0.05364486570022528,
          "values": [
            0.05364486570022528
          ]
        },
        "jaccard": {
          "mean": 0.04214882710953342,
          "std": 0.0,
          "min": 0.04214882710953342,
          "max": 0.04214882710953342,
          "median": 0.04214882710953342,
          "values": [
            0.04214882710953342
          ]
        },
        "semantic_similarity": {
          "mean": 0.10947090022265911,
          "std": 0.0,
          "min": 0.10947090022265911,
          "max": 0.10947090022265911,
          "median": 0.10947090022265911,
          "values": [
            0.10947090022265911
          ]
        },
        "answer_correctness": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.22699999999999998,
          "std": 0.0,
          "min": 0.22699999999999998,
          "max": 0.22699999999999998,
          "median": 0.22699999999999998,
          "values": [
            0.22699999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.9640000000000001,
          "std": 0.0,
          "min": 0.9640000000000001,
          "max": 0.9640000000000001,
          "median": 0.9640000000000001,
          "values": [
            0.9640000000000001
          ]
        }
      }
    },
    "ecare_explanation_generation_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.032449076719028774,
          "std": 0.0,
          "min": 0.032449076719028774,
          "max": 0.032449076719028774,
          "median": 0.032449076719028774,
          "values": [
            0.032449076719028774
          ]
        },
        "recall": {
          "mean": 0.5604792103321515,
          "std": 0.0,
          "min": 0.5604792103321515,
          "max": 0.5604792103321515,
          "median": 0.5604792103321515,
          "values": [
            0.5604792103321515
          ]
        },
        "f1_score": {
          "mean": 0.06052878394901255,
          "std": 0.0,
          "min": 0.06052878394901255,
          "max": 0.06052878394901255,
          "median": 0.06052878394901255,
          "values": [
            0.06052878394901255
          ]
        },
        "jaccard": {
          "mean": 0.031707883672439735,
          "std": 0.0,
          "min": 0.031707883672439735,
          "max": 0.031707883672439735,
          "median": 0.031707883672439735,
          "values": [
            0.031707883672439735
          ]
        },
        "semantic_similarity": {
          "mean": 0.5460975639522075,
          "std": 0.0,
          "min": 0.5460975639522075,
          "max": 0.5460975639522075,
          "median": 0.5460975639522075,
          "values": [
            0.5460975639522075
          ]
        },
        "response_brevity": {
          "mean": 0.20400000000000007,
          "std": 0.0,
          "min": 0.20400000000000007,
          "max": 0.20400000000000007,
          "median": 0.20400000000000007,
          "values": [
            0.20400000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.6245,
          "std": 0.0,
          "min": 0.6245,
          "max": 0.6245,
          "median": 0.6245,
          "values": [
            0.6245
          ]
        },
        "single_sentence_format": {
          "mean": 0.052000000000000005,
          "std": 0.0,
          "min": 0.052000000000000005,
          "max": 0.052000000000000005,
          "median": 0.052000000000000005,
          "values": [
            0.052000000000000005
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02539934680838183,
          "std": 0.0,
          "min": 0.02539934680838183,
          "max": 0.02539934680838183,
          "median": 0.02539934680838183,
          "values": [
            0.02539934680838183
          ]
        },
        "recall": {
          "mean": 0.5344611466964408,
          "std": 0.0,
          "min": 0.5344611466964408,
          "max": 0.5344611466964408,
          "median": 0.5344611466964408,
          "values": [
            0.5344611466964408
          ]
        },
        "f1_score": {
          "mean": 0.047977696170655594,
          "std": 0.0,
          "min": 0.047977696170655594,
          "max": 0.047977696170655594,
          "median": 0.047977696170655594,
          "values": [
            0.047977696170655594
          ]
        },
        "jaccard": {
          "mean": 0.02488708714681504,
          "std": 0.0,
          "min": 0.02488708714681504,
          "max": 0.02488708714681504,
          "median": 0.02488708714681504,
          "values": [
            0.02488708714681504
          ]
        },
        "semantic_similarity": {
          "mean": 0.5280703355371952,
          "std": 0.0,
          "min": 0.5280703355371952,
          "max": 0.5280703355371952,
          "median": 0.5280703355371952,
          "values": [
            0.5280703355371952
          ]
        },
        "response_brevity": {
          "mean": 0.14300000000000002,
          "std": 0.0,
          "min": 0.14300000000000002,
          "max": 0.14300000000000002,
          "median": 0.14300000000000002,
          "values": [
            0.14300000000000002
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.544,
          "std": 0.0,
          "min": 0.544,
          "max": 0.544,
          "median": 0.544,
          "values": [
            0.544
          ]
        },
        "single_sentence_format": {
          "mean": 0.013999999999999999,
          "std": 0.0,
          "min": 0.013999999999999999,
          "max": 0.013999999999999999,
          "median": 0.013999999999999999,
          "values": [
            0.013999999999999999
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02266541758495709,
          "std": 0.0,
          "min": 0.02266541758495709,
          "max": 0.02266541758495709,
          "median": 0.02266541758495709,
          "values": [
            0.02266541758495709
          ]
        },
        "recall": {
          "mean": 0.5440752074722662,
          "std": 0.0,
          "min": 0.5440752074722662,
          "max": 0.5440752074722662,
          "median": 0.5440752074722662,
          "values": [
            0.5440752074722662
          ]
        },
        "f1_score": {
          "mean": 0.04316994111214483,
          "std": 0.0,
          "min": 0.04316994111214483,
          "max": 0.04316994111214483,
          "median": 0.04316994111214483,
          "values": [
            0.04316994111214483
          ]
        },
        "jaccard": {
          "mean": 0.02232481265604236,
          "std": 0.0,
          "min": 0.02232481265604236,
          "max": 0.02232481265604236,
          "median": 0.02232481265604236,
          "values": [
            0.02232481265604236
          ]
        },
        "semantic_similarity": {
          "mean": 0.5488447332382203,
          "std": 0.0,
          "min": 0.5488447332382203,
          "max": 0.5488447332382203,
          "median": 0.5488447332382203,
          "values": [
            0.5488447332382203
          ]
        },
        "response_brevity": {
          "mean": 0.10199999999999998,
          "std": 0.0,
          "min": 0.10199999999999998,
          "max": 0.10199999999999998,
          "median": 0.10199999999999998,
          "values": [
            0.10199999999999998
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.607,
          "std": 0.0,
          "min": 0.607,
          "max": 0.607,
          "median": 0.607,
          "values": [
            0.607
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.01935852294089931,
          "std": 0.0,
          "min": 0.01935852294089931,
          "max": 0.01935852294089931,
          "median": 0.01935852294089931,
          "values": [
            0.01935852294089931
          ]
        },
        "recall": {
          "mean": 0.521779503176562,
          "std": 0.0,
          "min": 0.521779503176562,
          "max": 0.521779503176562,
          "median": 0.521779503176562,
          "values": [
            0.521779503176562
          ]
        },
        "f1_score": {
          "mean": 0.037057229303371096,
          "std": 0.0,
          "min": 0.037057229303371096,
          "max": 0.037057229303371096,
          "median": 0.037057229303371096,
          "values": [
            0.037057229303371096
          ]
        },
        "jaccard": {
          "mean": 0.019046013172166105,
          "std": 0.0,
          "min": 0.019046013172166105,
          "max": 0.019046013172166105,
          "median": 0.019046013172166105,
          "values": [
            0.019046013172166105
          ]
        },
        "semantic_similarity": {
          "mean": 0.4713038282096386,
          "std": 0.0,
          "min": 0.4713038282096386,
          "max": 0.4713038282096386,
          "median": 0.4713038282096386,
          "values": [
            0.4713038282096386
          ]
        },
        "response_brevity": {
          "mean": 0.09799999999999998,
          "std": 0.0,
          "min": 0.09799999999999998,
          "max": 0.09799999999999998,
          "median": 0.09799999999999998,
          "values": [
            0.09799999999999998
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.4770000000000001,
          "std": 0.0,
          "min": 0.4770000000000001,
          "max": 0.4770000000000001,
          "median": 0.4770000000000001,
          "values": [
            0.4770000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-mistral-nemo-12b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.020226702046395123,
          "std": 0.0,
          "min": 0.020226702046395123,
          "max": 0.020226702046395123,
          "median": 0.020226702046395123,
          "values": [
            0.020226702046395123
          ]
        },
        "recall": {
          "mean": 0.4908949808361573,
          "std": 0.0,
          "min": 0.4908949808361573,
          "max": 0.4908949808361573,
          "median": 0.4908949808361573,
          "values": [
            0.4908949808361573
          ]
        },
        "f1_score": {
          "mean": 0.03854518039754913,
          "std": 0.0,
          "min": 0.03854518039754913,
          "max": 0.03854518039754913,
          "median": 0.03854518039754913,
          "values": [
            0.03854518039754913
          ]
        },
        "jaccard": {
          "mean": 0.019853574130229187,
          "std": 0.0,
          "min": 0.019853574130229187,
          "max": 0.019853574130229187,
          "median": 0.019853574130229187,
          "values": [
            0.019853574130229187
          ]
        },
        "semantic_similarity": {
          "mean": 0.4902907335758209,
          "std": 0.0,
          "min": 0.4902907335758209,
          "max": 0.4902907335758209,
          "median": 0.4902907335758209,
          "values": [
            0.4902907335758209
          ]
        },
        "response_brevity": {
          "mean": 0.10899999999999999,
          "std": 0.0,
          "min": 0.10899999999999999,
          "max": 0.10899999999999999,
          "median": 0.10899999999999999,
          "values": [
            0.10899999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.6744999999999999,
          "std": 0.0,
          "min": 0.6744999999999999,
          "max": 0.6744999999999999,
          "median": 0.6744999999999999,
          "values": [
            0.6744999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.04557084842706161,
          "std": 0.0,
          "min": 0.04557084842706161,
          "max": 0.04557084842706161,
          "median": 0.04557084842706161,
          "values": [
            0.04557084842706161
          ]
        },
        "recall": {
          "mean": 0.5241906949259891,
          "std": 0.0,
          "min": 0.5241906949259891,
          "max": 0.5241906949259891,
          "median": 0.5241906949259891,
          "values": [
            0.5241906949259891
          ]
        },
        "f1_score": {
          "mean": 0.08233170180758277,
          "std": 0.0,
          "min": 0.08233170180758277,
          "max": 0.08233170180758277,
          "median": 0.08233170180758277,
          "values": [
            0.08233170180758277
          ]
        },
        "jaccard": {
          "mean": 0.04393898211276162,
          "std": 0.0,
          "min": 0.04393898211276162,
          "max": 0.04393898211276162,
          "median": 0.04393898211276162,
          "values": [
            0.04393898211276162
          ]
        },
        "semantic_similarity": {
          "mean": 0.5832784813642502,
          "std": 0.0,
          "min": 0.5832784813642502,
          "max": 0.5832784813642502,
          "median": 0.5832784813642502,
          "values": [
            0.5832784813642502
          ]
        },
        "response_brevity": {
          "mean": 0.32400000000000007,
          "std": 0.0,
          "min": 0.32400000000000007,
          "max": 0.32400000000000007,
          "median": 0.32400000000000007,
          "values": [
            0.32400000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8715000000000002,
          "std": 0.0,
          "min": 0.8715000000000002,
          "max": 0.8715000000000002,
          "median": 0.8715000000000002,
          "values": [
            0.8715000000000002
          ]
        },
        "single_sentence_format": {
          "mean": 0.284,
          "std": 0.0,
          "min": 0.284,
          "max": 0.284,
          "median": 0.284,
          "values": [
            0.284
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.024600590783897664,
          "std": 0.0,
          "min": 0.024600590783897664,
          "max": 0.024600590783897664,
          "median": 0.024600590783897664,
          "values": [
            0.024600590783897664
          ]
        },
        "recall": {
          "mean": 0.5406409015820781,
          "std": 0.0,
          "min": 0.5406409015820781,
          "max": 0.5406409015820781,
          "median": 0.5406409015820781,
          "values": [
            0.5406409015820781
          ]
        },
        "f1_score": {
          "mean": 0.04658093408689554,
          "std": 0.0,
          "min": 0.04658093408689554,
          "max": 0.04658093408689554,
          "median": 0.04658093408689554,
          "values": [
            0.04658093408689554
          ]
        },
        "jaccard": {
          "mean": 0.024130243659895724,
          "std": 0.0,
          "min": 0.024130243659895724,
          "max": 0.024130243659895724,
          "median": 0.024130243659895724,
          "values": [
            0.024130243659895724
          ]
        },
        "semantic_similarity": {
          "mean": 0.5587588514387608,
          "std": 0.0,
          "min": 0.5587588514387608,
          "max": 0.5587588514387608,
          "median": 0.5587588514387608,
          "values": [
            0.5587588514387608
          ]
        },
        "response_brevity": {
          "mean": 0.106,
          "std": 0.0,
          "min": 0.106,
          "max": 0.106,
          "median": 0.106,
          "values": [
            0.106
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7765,
          "std": 0.0,
          "min": 0.7765,
          "max": 0.7765,
          "median": 0.7765,
          "values": [
            0.7765
          ]
        },
        "single_sentence_format": {
          "mean": 0.002,
          "std": 0.0,
          "min": 0.002,
          "max": 0.002,
          "median": 0.002,
          "values": [
            0.002
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02190535877702346,
          "std": 0.0,
          "min": 0.02190535877702346,
          "max": 0.02190535877702346,
          "median": 0.02190535877702346,
          "values": [
            0.02190535877702346
          ]
        },
        "recall": {
          "mean": 0.536275461466638,
          "std": 0.0,
          "min": 0.536275461466638,
          "max": 0.536275461466638,
          "median": 0.536275461466638,
          "values": [
            0.536275461466638
          ]
        },
        "f1_score": {
          "mean": 0.041750485357798885,
          "std": 0.0,
          "min": 0.041750485357798885,
          "max": 0.041750485357798885,
          "median": 0.041750485357798885,
          "values": [
            0.041750485357798885
          ]
        },
        "jaccard": {
          "mean": 0.021547826665100175,
          "std": 0.0,
          "min": 0.021547826665100175,
          "max": 0.021547826665100175,
          "median": 0.021547826665100175,
          "values": [
            0.021547826665100175
          ]
        },
        "semantic_similarity": {
          "mean": 0.5653236883878708,
          "std": 0.0,
          "min": 0.5653236883878708,
          "max": 0.5653236883878708,
          "median": 0.5653236883878708,
          "values": [
            0.5653236883878708
          ]
        },
        "response_brevity": {
          "mean": 0.09299999999999999,
          "std": 0.0,
          "min": 0.09299999999999999,
          "max": 0.09299999999999999,
          "median": 0.09299999999999999,
          "values": [
            0.09299999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7829999999999999,
          "std": 0.0,
          "min": 0.7829999999999999,
          "max": 0.7829999999999999,
          "median": 0.7829999999999999,
          "values": [
            0.7829999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0189175577179169,
          "std": 0.0,
          "min": 0.0189175577179169,
          "max": 0.0189175577179169,
          "median": 0.0189175577179169,
          "values": [
            0.0189175577179169
          ]
        },
        "recall": {
          "mean": 0.5358647153500096,
          "std": 0.0,
          "min": 0.5358647153500096,
          "max": 0.5358647153500096,
          "median": 0.5358647153500096,
          "values": [
            0.5358647153500096
          ]
        },
        "f1_score": {
          "mean": 0.03627560367279813,
          "std": 0.0,
          "min": 0.03627560367279813,
          "max": 0.03627560367279813,
          "median": 0.03627560367279813,
          "values": [
            0.03627560367279813
          ]
        },
        "jaccard": {
          "mean": 0.01862308872016518,
          "std": 0.0,
          "min": 0.01862308872016518,
          "max": 0.01862308872016518,
          "median": 0.01862308872016518,
          "values": [
            0.01862308872016518
          ]
        },
        "semantic_similarity": {
          "mean": 0.53038289681077,
          "std": 0.0,
          "min": 0.53038289681077,
          "max": 0.53038289681077,
          "median": 0.53038289681077,
          "values": [
            0.53038289681077
          ]
        },
        "response_brevity": {
          "mean": 0.04899999999999999,
          "std": 0.0,
          "min": 0.04899999999999999,
          "max": 0.04899999999999999,
          "median": 0.04899999999999999,
          "values": [
            0.04899999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.709,
          "std": 0.0,
          "min": 0.709,
          "max": 0.709,
          "median": 0.709,
          "values": [
            0.709
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__hf-qwen2.5-3b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.019163296167254287,
          "std": 0.0,
          "min": 0.019163296167254287,
          "max": 0.019163296167254287,
          "median": 0.019163296167254287,
          "values": [
            0.019163296167254287
          ]
        },
        "recall": {
          "mean": 0.4955000603971192,
          "std": 0.0,
          "min": 0.4955000603971192,
          "max": 0.4955000603971192,
          "median": 0.4955000603971192,
          "values": [
            0.4955000603971192
          ]
        },
        "f1_score": {
          "mean": 0.036631992125233236,
          "std": 0.0,
          "min": 0.036631992125233236,
          "max": 0.036631992125233236,
          "median": 0.036631992125233236,
          "values": [
            0.036631992125233236
          ]
        },
        "jaccard": {
          "mean": 0.018844493331047055,
          "std": 0.0,
          "min": 0.018844493331047055,
          "max": 0.018844493331047055,
          "median": 0.018844493331047055,
          "values": [
            0.018844493331047055
          ]
        },
        "semantic_similarity": {
          "mean": 0.49335808366537093,
          "std": 0.0,
          "min": 0.49335808366537093,
          "max": 0.49335808366537093,
          "median": 0.49335808366537093,
          "values": [
            0.49335808366537093
          ]
        },
        "response_brevity": {
          "mean": 0.07399999999999998,
          "std": 0.0,
          "min": 0.07399999999999998,
          "max": 0.07399999999999998,
          "median": 0.07399999999999998,
          "values": [
            0.07399999999999998
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7199999999999999,
          "std": 0.0,
          "min": 0.7199999999999999,
          "max": 0.7199999999999999,
          "median": 0.7199999999999999,
          "values": [
            0.7199999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03794357996519728,
          "std": 0.0,
          "min": 0.03794357996519728,
          "max": 0.03794357996519728,
          "median": 0.03794357996519728,
          "values": [
            0.03794357996519728
          ]
        },
        "recall": {
          "mean": 0.5060214556685145,
          "std": 0.0,
          "min": 0.5060214556685145,
          "max": 0.5060214556685145,
          "median": 0.5060214556685145,
          "values": [
            0.5060214556685145
          ]
        },
        "f1_score": {
          "mean": 0.06938919762058396,
          "std": 0.0,
          "min": 0.06938919762058396,
          "max": 0.06938919762058396,
          "median": 0.06938919762058396,
          "values": [
            0.06938919762058396
          ]
        },
        "jaccard": {
          "mean": 0.03659897824736949,
          "std": 0.0,
          "min": 0.03659897824736949,
          "max": 0.03659897824736949,
          "median": 0.03659897824736949,
          "values": [
            0.03659897824736949
          ]
        },
        "semantic_similarity": {
          "mean": 0.519278349801898,
          "std": 0.0,
          "min": 0.519278349801898,
          "max": 0.519278349801898,
          "median": 0.519278349801898,
          "values": [
            0.519278349801898
          ]
        },
        "response_brevity": {
          "mean": 0.27399999999999997,
          "std": 0.0,
          "min": 0.27399999999999997,
          "max": 0.27399999999999997,
          "median": 0.27399999999999997,
          "values": [
            0.27399999999999997
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7369999999999999,
          "std": 0.0,
          "min": 0.7369999999999999,
          "max": 0.7369999999999999,
          "median": 0.7369999999999999,
          "values": [
            0.7369999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.06000000000000001,
          "std": 0.0,
          "min": 0.06000000000000001,
          "max": 0.06000000000000001,
          "median": 0.06000000000000001,
          "values": [
            0.06000000000000001
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.026672471833243463,
          "std": 0.0,
          "min": 0.026672471833243463,
          "max": 0.026672471833243463,
          "median": 0.026672471833243463,
          "values": [
            0.026672471833243463
          ]
        },
        "recall": {
          "mean": 0.4560957343310284,
          "std": 0.0,
          "min": 0.4560957343310284,
          "max": 0.4560957343310284,
          "median": 0.4560957343310284,
          "values": [
            0.4560957343310284
          ]
        },
        "f1_score": {
          "mean": 0.049912306747785413,
          "std": 0.0,
          "min": 0.049912306747785413,
          "max": 0.049912306747785413,
          "median": 0.049912306747785413,
          "values": [
            0.049912306747785413
          ]
        },
        "jaccard": {
          "mean": 0.025979021207328982,
          "std": 0.0,
          "min": 0.025979021207328982,
          "max": 0.025979021207328982,
          "median": 0.025979021207328982,
          "values": [
            0.025979021207328982
          ]
        },
        "semantic_similarity": {
          "mean": 0.5351960927993059,
          "std": 0.0,
          "min": 0.5351960927993059,
          "max": 0.5351960927993059,
          "median": 0.5351960927993059,
          "values": [
            0.5351960927993059
          ]
        },
        "response_brevity": {
          "mean": 0.22600000000000006,
          "std": 0.0,
          "min": 0.22600000000000006,
          "max": 0.22600000000000006,
          "median": 0.22600000000000006,
          "values": [
            0.22600000000000006
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.875,
          "std": 0.0,
          "min": 0.875,
          "max": 0.875,
          "median": 0.875,
          "values": [
            0.875
          ]
        },
        "single_sentence_format": {
          "mean": 0.008,
          "std": 0.0,
          "min": 0.008,
          "max": 0.008,
          "median": 0.008,
          "values": [
            0.008
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.027553456553650088,
          "std": 0.0,
          "min": 0.027553456553650088,
          "max": 0.027553456553650088,
          "median": 0.027553456553650088,
          "values": [
            0.027553456553650088
          ]
        },
        "recall": {
          "mean": 0.479000930442107,
          "std": 0.0,
          "min": 0.479000930442107,
          "max": 0.479000930442107,
          "median": 0.479000930442107,
          "values": [
            0.479000930442107
          ]
        },
        "f1_score": {
          "mean": 0.05161889758064091,
          "std": 0.0,
          "min": 0.05161889758064091,
          "max": 0.05161889758064091,
          "median": 0.05161889758064091,
          "values": [
            0.05161889758064091
          ]
        },
        "jaccard": {
          "mean": 0.02686778350922517,
          "std": 0.0,
          "min": 0.02686778350922517,
          "max": 0.02686778350922517,
          "median": 0.02686778350922517,
          "values": [
            0.02686778350922517
          ]
        },
        "semantic_similarity": {
          "mean": 0.5556939855962991,
          "std": 0.0,
          "min": 0.5556939855962991,
          "max": 0.5556939855962991,
          "median": 0.5556939855962991,
          "values": [
            0.5556939855962991
          ]
        },
        "response_brevity": {
          "mean": 0.17200000000000007,
          "std": 0.0,
          "min": 0.17200000000000007,
          "max": 0.17200000000000007,
          "median": 0.17200000000000007,
          "values": [
            0.17200000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7340000000000001,
          "std": 0.0,
          "min": 0.7340000000000001,
          "max": 0.7340000000000001,
          "median": 0.7340000000000001,
          "values": [
            0.7340000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02628224892419033,
          "std": 0.0,
          "min": 0.02628224892419033,
          "max": 0.02628224892419033,
          "median": 0.02628224892419033,
          "values": [
            0.02628224892419033
          ]
        },
        "recall": {
          "mean": 0.4550645335057101,
          "std": 0.0,
          "min": 0.4550645335057101,
          "max": 0.4550645335057101,
          "median": 0.4550645335057101,
          "values": [
            0.4550645335057101
          ]
        },
        "f1_score": {
          "mean": 0.04923023302997629,
          "std": 0.0,
          "min": 0.04923023302997629,
          "max": 0.04923023302997629,
          "median": 0.04923023302997629,
          "values": [
            0.04923023302997629
          ]
        },
        "jaccard": {
          "mean": 0.025596235273989052,
          "std": 0.0,
          "min": 0.025596235273989052,
          "max": 0.025596235273989052,
          "median": 0.025596235273989052,
          "values": [
            0.025596235273989052
          ]
        },
        "semantic_similarity": {
          "mean": 0.4980191664397717,
          "std": 0.0,
          "min": 0.4980191664397717,
          "max": 0.4980191664397717,
          "median": 0.4980191664397717,
          "values": [
            0.4980191664397717
          ]
        },
        "response_brevity": {
          "mean": 0.16000000000000003,
          "std": 0.0,
          "min": 0.16000000000000003,
          "max": 0.16000000000000003,
          "median": 0.16000000000000003,
          "values": [
            0.16000000000000003
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.7325,
          "std": 0.0,
          "min": 0.7325,
          "max": 0.7325,
          "median": 0.7325,
          "values": [
            0.7325
          ]
        },
        "single_sentence_format": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3-8b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.024979105849702088,
          "std": 0.0,
          "min": 0.024979105849702088,
          "max": 0.024979105849702088,
          "median": 0.024979105849702088,
          "values": [
            0.024979105849702088
          ]
        },
        "recall": {
          "mean": 0.40925482524011936,
          "std": 0.0,
          "min": 0.40925482524011936,
          "max": 0.40925482524011936,
          "median": 0.40925482524011936,
          "values": [
            0.40925482524011936
          ]
        },
        "f1_score": {
          "mean": 0.04625390969398709,
          "std": 0.0,
          "min": 0.04625390969398709,
          "max": 0.04625390969398709,
          "median": 0.04625390969398709,
          "values": [
            0.04625390969398709
          ]
        },
        "jaccard": {
          "mean": 0.024047286391003904,
          "std": 0.0,
          "min": 0.024047286391003904,
          "max": 0.024047286391003904,
          "median": 0.024047286391003904,
          "values": [
            0.024047286391003904
          ]
        },
        "semantic_similarity": {
          "mean": 0.4677891075611115,
          "std": 0.0,
          "min": 0.4677891075611115,
          "max": 0.4677891075611115,
          "median": 0.4677891075611115,
          "values": [
            0.4677891075611115
          ]
        },
        "response_brevity": {
          "mean": 0.138,
          "std": 0.0,
          "min": 0.138,
          "max": 0.138,
          "median": 0.138,
          "values": [
            0.138
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8345,
          "std": 0.0,
          "min": 0.8345,
          "max": 0.8345,
          "median": 0.8345,
          "values": [
            0.8345
          ]
        },
        "single_sentence_format": {
          "mean": 0.036000000000000004,
          "std": 0.0,
          "min": 0.036000000000000004,
          "max": 0.036000000000000004,
          "median": 0.036000000000000004,
          "values": [
            0.036000000000000004
          ]
        }
      }
    },
    "ecare_explanation_generation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.029972077692139765,
          "std": 0.0,
          "min": 0.029972077692139765,
          "max": 0.029972077692139765,
          "median": 0.029972077692139765,
          "values": [
            0.029972077692139765
          ]
        },
        "recall": {
          "mean": 0.20926401049930457,
          "std": 0.0,
          "min": 0.20926401049930457,
          "max": 0.20926401049930457,
          "median": 0.20926401049930457,
          "values": [
            0.20926401049930457
          ]
        },
        "f1_score": {
          "mean": 0.04631395111031909,
          "std": 0.0,
          "min": 0.04631395111031909,
          "max": 0.04631395111031909,
          "median": 0.04631395111031909,
          "values": [
            0.04631395111031909
          ]
        },
        "jaccard": {
          "mean": 0.024981592147678998,
          "std": 0.0,
          "min": 0.024981592147678998,
          "max": 0.024981592147678998,
          "median": 0.024981592147678998,
          "values": [
            0.024981592147678998
          ]
        },
        "semantic_similarity": {
          "mean": 0.2561993674002588,
          "std": 0.0,
          "min": 0.2561993674002588,
          "max": 0.2561993674002588,
          "median": 0.2561993674002588,
          "values": [
            0.2561993674002588
          ]
        },
        "response_brevity": {
          "mean": 0.6809999999999999,
          "std": 0.0,
          "min": 0.6809999999999999,
          "max": 0.6809999999999999,
          "median": 0.6809999999999999,
          "values": [
            0.6809999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.927,
          "std": 0.0,
          "min": 0.927,
          "max": 0.927,
          "median": 0.927,
          "values": [
            0.927
          ]
        },
        "single_sentence_format": {
          "mean": 0.602,
          "std": 0.0,
          "min": 0.602,
          "max": 0.602,
          "median": 0.602,
          "values": [
            0.602
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.029267218060280285,
          "std": 0.0,
          "min": 0.029267218060280285,
          "max": 0.029267218060280285,
          "median": 0.029267218060280285,
          "values": [
            0.029267218060280285
          ]
        },
        "recall": {
          "mean": 0.247400302312067,
          "std": 0.0,
          "min": 0.247400302312067,
          "max": 0.247400302312067,
          "median": 0.247400302312067,
          "values": [
            0.247400302312067
          ]
        },
        "f1_score": {
          "mean": 0.04864732050897201,
          "std": 0.0,
          "min": 0.04864732050897201,
          "max": 0.04864732050897201,
          "median": 0.04864732050897201,
          "values": [
            0.04864732050897201
          ]
        },
        "jaccard": {
          "mean": 0.025602282422562926,
          "std": 0.0,
          "min": 0.025602282422562926,
          "max": 0.025602282422562926,
          "median": 0.025602282422562926,
          "values": [
            0.025602282422562926
          ]
        },
        "semantic_similarity": {
          "mean": 0.38778922430239615,
          "std": 0.0,
          "min": 0.38778922430239615,
          "max": 0.38778922430239615,
          "median": 0.38778922430239615,
          "values": [
            0.38778922430239615
          ]
        },
        "response_brevity": {
          "mean": 0.5539999999999999,
          "std": 0.0,
          "min": 0.5539999999999999,
          "max": 0.5539999999999999,
          "median": 0.5539999999999999,
          "values": [
            0.5539999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.9294999999999999,
          "std": 0.0,
          "min": 0.9294999999999999,
          "max": 0.9294999999999999,
          "median": 0.9294999999999999,
          "values": [
            0.9294999999999999
          ]
        },
        "single_sentence_format": {
          "mean": 0.42599999999999993,
          "std": 0.0,
          "min": 0.42599999999999993,
          "max": 0.42599999999999993,
          "median": 0.42599999999999993,
          "values": [
            0.42599999999999993
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.031800801952624004,
          "std": 0.0,
          "min": 0.031800801952624004,
          "max": 0.031800801952624004,
          "median": 0.031800801952624004,
          "values": [
            0.031800801952624004
          ]
        },
        "recall": {
          "mean": 0.3685907785025432,
          "std": 0.0,
          "min": 0.3685907785025432,
          "max": 0.3685907785025432,
          "median": 0.3685907785025432,
          "values": [
            0.3685907785025432
          ]
        },
        "f1_score": {
          "mean": 0.05553081088872693,
          "std": 0.0,
          "min": 0.05553081088872693,
          "max": 0.05553081088872693,
          "median": 0.05553081088872693,
          "values": [
            0.05553081088872693
          ]
        },
        "jaccard": {
          "mean": 0.029320485292624158,
          "std": 0.0,
          "min": 0.029320485292624158,
          "max": 0.029320485292624158,
          "median": 0.029320485292624158,
          "values": [
            0.029320485292624158
          ]
        },
        "semantic_similarity": {
          "mean": 0.47987603928893807,
          "std": 0.0,
          "min": 0.47987603928893807,
          "max": 0.47987603928893807,
          "median": 0.47987603928893807,
          "values": [
            0.47987603928893807
          ]
        },
        "response_brevity": {
          "mean": 0.349,
          "std": 0.0,
          "min": 0.349,
          "max": 0.349,
          "median": 0.349,
          "values": [
            0.349
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8709999999999998,
          "std": 0.0,
          "min": 0.8709999999999998,
          "max": 0.8709999999999998,
          "median": 0.8709999999999998,
          "values": [
            0.8709999999999998
          ]
        },
        "single_sentence_format": {
          "mean": 0.20600000000000002,
          "std": 0.0,
          "min": 0.20600000000000002,
          "max": 0.20600000000000002,
          "median": 0.20600000000000002,
          "values": [
            0.20600000000000002
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.027251699997913034,
          "std": 0.0,
          "min": 0.027251699997913034,
          "max": 0.027251699997913034,
          "median": 0.027251699997913034,
          "values": [
            0.027251699997913034
          ]
        },
        "recall": {
          "mean": 0.3883164858017799,
          "std": 0.0,
          "min": 0.3883164858017799,
          "max": 0.3883164858017799,
          "median": 0.3883164858017799,
          "values": [
            0.3883164858017799
          ]
        },
        "f1_score": {
          "mean": 0.049418115407379164,
          "std": 0.0,
          "min": 0.049418115407379164,
          "max": 0.049418115407379164,
          "median": 0.049418115407379164,
          "values": [
            0.049418115407379164
          ]
        },
        "jaccard": {
          "mean": 0.025848739188611792,
          "std": 0.0,
          "min": 0.025848739188611792,
          "max": 0.025848739188611792,
          "median": 0.025848739188611792,
          "values": [
            0.025848739188611792
          ]
        },
        "semantic_similarity": {
          "mean": 0.44903883112594484,
          "std": 0.0,
          "min": 0.44903883112594484,
          "max": 0.44903883112594484,
          "median": 0.44903883112594484,
          "values": [
            0.44903883112594484
          ]
        },
        "response_brevity": {
          "mean": 0.17500000000000007,
          "std": 0.0,
          "min": 0.17500000000000007,
          "max": 0.17500000000000007,
          "median": 0.17500000000000007,
          "values": [
            0.17500000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8035,
          "std": 0.0,
          "min": 0.8035,
          "max": 0.8035,
          "median": 0.8035,
          "values": [
            0.8035
          ]
        },
        "single_sentence_format": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.025944357927579927,
          "std": 0.0,
          "min": 0.025944357927579927,
          "max": 0.025944357927579927,
          "median": 0.025944357927579927,
          "values": [
            0.025944357927579927
          ]
        },
        "recall": {
          "mean": 0.28173212898212896,
          "std": 0.0,
          "min": 0.28173212898212896,
          "max": 0.28173212898212896,
          "median": 0.28173212898212896,
          "values": [
            0.28173212898212896
          ]
        },
        "f1_score": {
          "mean": 0.0441087072533744,
          "std": 0.0,
          "min": 0.0441087072533744,
          "max": 0.0441087072533744,
          "median": 0.0441087072533744,
          "values": [
            0.0441087072533744
          ]
        },
        "jaccard": {
          "mean": 0.02329791715520711,
          "std": 0.0,
          "min": 0.02329791715520711,
          "max": 0.02329791715520711,
          "median": 0.02329791715520711,
          "values": [
            0.02329791715520711
          ]
        },
        "semantic_similarity": {
          "mean": 0.34247431508963927,
          "std": 0.0,
          "min": 0.34247431508963927,
          "max": 0.34247431508963927,
          "median": 0.34247431508963927,
          "values": [
            0.34247431508963927
          ]
        },
        "response_brevity": {
          "mean": 0.44799999999999995,
          "std": 0.0,
          "min": 0.44799999999999995,
          "max": 0.44799999999999995,
          "median": 0.44799999999999995,
          "values": [
            0.44799999999999995
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8980000000000001,
          "std": 0.0,
          "min": 0.8980000000000001,
          "max": 0.8980000000000001,
          "median": 0.8980000000000001,
          "values": [
            0.8980000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.32599999999999996,
          "std": 0.0,
          "min": 0.32599999999999996,
          "max": 0.32599999999999996,
          "median": 0.32599999999999996,
          "values": [
            0.32599999999999996
          ]
        }
      }
    },
    "ecare_explanation_generation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.036954425637616285,
          "std": 0.0,
          "min": 0.036954425637616285,
          "max": 0.036954425637616285,
          "median": 0.036954425637616285,
          "values": [
            0.036954425637616285
          ]
        },
        "recall": {
          "mean": 0.4921035696329814,
          "std": 0.0,
          "min": 0.4921035696329814,
          "max": 0.4921035696329814,
          "median": 0.4921035696329814,
          "values": [
            0.4921035696329814
          ]
        },
        "f1_score": {
          "mean": 0.06693846813737894,
          "std": 0.0,
          "min": 0.06693846813737894,
          "max": 0.06693846813737894,
          "median": 0.06693846813737894,
          "values": [
            0.06693846813737894
          ]
        },
        "jaccard": {
          "mean": 0.03562790305928242,
          "std": 0.0,
          "min": 0.03562790305928242,
          "max": 0.03562790305928242,
          "median": 0.03562790305928242,
          "values": [
            0.03562790305928242
          ]
        },
        "semantic_similarity": {
          "mean": 0.43876771464943887,
          "std": 0.0,
          "min": 0.43876771464943887,
          "max": 0.43876771464943887,
          "median": 0.43876771464943887,
          "values": [
            0.43876771464943887
          ]
        },
        "response_brevity": {
          "mean": 0.20800000000000007,
          "std": 0.0,
          "min": 0.20800000000000007,
          "max": 0.20800000000000007,
          "median": 0.20800000000000007,
          "values": [
            0.20800000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8295,
          "std": 0.0,
          "min": 0.8295,
          "max": 0.8295,
          "median": 0.8295,
          "values": [
            0.8295
          ]
        },
        "single_sentence_format": {
          "mean": 0.132,
          "std": 0.0,
          "min": 0.132,
          "max": 0.132,
          "median": 0.132,
          "values": [
            0.132
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.028358256961141675,
          "std": 0.0,
          "min": 0.028358256961141675,
          "max": 0.028358256961141675,
          "median": 0.028358256961141675,
          "values": [
            0.028358256961141675
          ]
        },
        "recall": {
          "mean": 0.44613055082172726,
          "std": 0.0,
          "min": 0.44613055082172726,
          "max": 0.44613055082172726,
          "median": 0.44613055082172726,
          "values": [
            0.44613055082172726
          ]
        },
        "f1_score": {
          "mean": 0.052499853913394796,
          "std": 0.0,
          "min": 0.052499853913394796,
          "max": 0.052499853913394796,
          "median": 0.052499853913394796,
          "values": [
            0.052499853913394796
          ]
        },
        "jaccard": {
          "mean": 0.027414583850661388,
          "std": 0.0,
          "min": 0.027414583850661388,
          "max": 0.027414583850661388,
          "median": 0.027414583850661388,
          "values": [
            0.027414583850661388
          ]
        },
        "semantic_similarity": {
          "mean": 0.5359494306147099,
          "std": 0.0,
          "min": 0.5359494306147099,
          "max": 0.5359494306147099,
          "median": 0.5359494306147099,
          "values": [
            0.5359494306147099
          ]
        },
        "response_brevity": {
          "mean": 0.19800000000000004,
          "std": 0.0,
          "min": 0.19800000000000004,
          "max": 0.19800000000000004,
          "median": 0.19800000000000004,
          "values": [
            0.19800000000000004
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8655000000000002,
          "std": 0.0,
          "min": 0.8655000000000002,
          "max": 0.8655000000000002,
          "median": 0.8655000000000002,
          "values": [
            0.8655000000000002
          ]
        },
        "single_sentence_format": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03178070791227817,
          "std": 0.0,
          "min": 0.03178070791227817,
          "max": 0.03178070791227817,
          "median": 0.03178070791227817,
          "values": [
            0.03178070791227817
          ]
        },
        "recall": {
          "mean": 0.48813311361840783,
          "std": 0.0,
          "min": 0.48813311361840783,
          "max": 0.48813311361840783,
          "median": 0.48813311361840783,
          "values": [
            0.48813311361840783
          ]
        },
        "f1_score": {
          "mean": 0.05890285424088801,
          "std": 0.0,
          "min": 0.05890285424088801,
          "max": 0.05890285424088801,
          "median": 0.05890285424088801,
          "values": [
            0.05890285424088801
          ]
        },
        "jaccard": {
          "mean": 0.030851910116611946,
          "std": 0.0,
          "min": 0.030851910116611946,
          "max": 0.030851910116611946,
          "median": 0.030851910116611946,
          "values": [
            0.030851910116611946
          ]
        },
        "semantic_similarity": {
          "mean": 0.5241868364065886,
          "std": 0.0,
          "min": 0.5241868364065886,
          "max": 0.5241868364065886,
          "median": 0.5241868364065886,
          "values": [
            0.5241868364065886
          ]
        },
        "response_brevity": {
          "mean": 0.16800000000000007,
          "std": 0.0,
          "min": 0.16800000000000007,
          "max": 0.16800000000000007,
          "median": 0.16800000000000007,
          "values": [
            0.16800000000000007
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8685000000000003,
          "std": 0.0,
          "min": 0.8685000000000003,
          "max": 0.8685000000000003,
          "median": 0.8685000000000003,
          "values": [
            0.8685000000000003
          ]
        },
        "single_sentence_format": {
          "mean": 0.066,
          "std": 0.0,
          "min": 0.066,
          "max": 0.066,
          "median": 0.066,
          "values": [
            0.066
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.026071979794883696,
          "std": 0.0,
          "min": 0.026071979794883696,
          "max": 0.026071979794883696,
          "median": 0.026071979794883696,
          "values": [
            0.026071979794883696
          ]
        },
        "recall": {
          "mean": 0.44494033417562834,
          "std": 0.0,
          "min": 0.44494033417562834,
          "max": 0.44494033417562834,
          "median": 0.44494033417562834,
          "values": [
            0.44494033417562834
          ]
        },
        "f1_score": {
          "mean": 0.04867357754273953,
          "std": 0.0,
          "min": 0.04867357754273953,
          "max": 0.04867357754273953,
          "median": 0.04867357754273953,
          "values": [
            0.04867357754273953
          ]
        },
        "jaccard": {
          "mean": 0.025276762124913837,
          "std": 0.0,
          "min": 0.025276762124913837,
          "max": 0.025276762124913837,
          "median": 0.025276762124913837,
          "values": [
            0.025276762124913837
          ]
        },
        "semantic_similarity": {
          "mean": 0.4558450558036566,
          "std": 0.0,
          "min": 0.4558450558036566,
          "max": 0.4558450558036566,
          "median": 0.4558450558036566,
          "values": [
            0.4558450558036566
          ]
        },
        "response_brevity": {
          "mean": 0.132,
          "std": 0.0,
          "min": 0.132,
          "max": 0.132,
          "median": 0.132,
          "values": [
            0.132
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8155000000000002,
          "std": 0.0,
          "min": 0.8155000000000002,
          "max": 0.8155000000000002,
          "median": 0.8155000000000002,
          "values": [
            0.8155000000000002
          ]
        },
        "single_sentence_format": {
          "mean": 0.04800000000000001,
          "std": 0.0,
          "min": 0.04800000000000001,
          "max": 0.04800000000000001,
          "median": 0.04800000000000001,
          "values": [
            0.04800000000000001
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-llama3.3-70b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02803827301960644,
          "std": 0.0,
          "min": 0.02803827301960644,
          "max": 0.02803827301960644,
          "median": 0.02803827301960644,
          "values": [
            0.02803827301960644
          ]
        },
        "recall": {
          "mean": 0.4213058118352236,
          "std": 0.0,
          "min": 0.4213058118352236,
          "max": 0.4213058118352236,
          "median": 0.4213058118352236,
          "values": [
            0.4213058118352236
          ]
        },
        "f1_score": {
          "mean": 0.05191380799356806,
          "std": 0.0,
          "min": 0.05191380799356806,
          "max": 0.05191380799356806,
          "median": 0.05191380799356806,
          "values": [
            0.05191380799356806
          ]
        },
        "jaccard": {
          "mean": 0.02711375677050899,
          "std": 0.0,
          "min": 0.02711375677050899,
          "max": 0.02711375677050899,
          "median": 0.02711375677050899,
          "values": [
            0.02711375677050899
          ]
        },
        "semantic_similarity": {
          "mean": 0.4574032038450241,
          "std": 0.0,
          "min": 0.4574032038450241,
          "max": 0.4574032038450241,
          "median": 0.4574032038450241,
          "values": [
            0.4574032038450241
          ]
        },
        "response_brevity": {
          "mean": 0.24200000000000002,
          "std": 0.0,
          "min": 0.24200000000000002,
          "max": 0.24200000000000002,
          "median": 0.24200000000000002,
          "values": [
            0.24200000000000002
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8935000000000001,
          "std": 0.0,
          "min": 0.8935000000000001,
          "max": 0.8935000000000001,
          "median": 0.8935000000000001,
          "values": [
            0.8935000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.168,
          "std": 0.0,
          "min": 0.168,
          "max": 0.168,
          "median": 0.168,
          "values": [
            0.168
          ]
        }
      }
    },
    "ecare_explanation_generation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.048265080545658645,
          "std": 0.0,
          "min": 0.048265080545658645,
          "max": 0.048265080545658645,
          "median": 0.048265080545658645,
          "values": [
            0.048265080545658645
          ]
        },
        "recall": {
          "mean": 0.49142961287078935,
          "std": 0.0,
          "min": 0.49142961287078935,
          "max": 0.49142961287078935,
          "median": 0.49142961287078935,
          "values": [
            0.49142961287078935
          ]
        },
        "f1_score": {
          "mean": 0.0862324113979467,
          "std": 0.0,
          "min": 0.0862324113979467,
          "max": 0.0862324113979467,
          "median": 0.0862324113979467,
          "values": [
            0.0862324113979467
          ]
        },
        "jaccard": {
          "mean": 0.04608547698920304,
          "std": 0.0,
          "min": 0.04608547698920304,
          "max": 0.04608547698920304,
          "median": 0.04608547698920304,
          "values": [
            0.04608547698920304
          ]
        },
        "semantic_similarity": {
          "mean": 0.5523444086313247,
          "std": 0.0,
          "min": 0.5523444086313247,
          "max": 0.5523444086313247,
          "median": 0.5523444086313247,
          "values": [
            0.5523444086313247
          ]
        },
        "response_brevity": {
          "mean": 0.4249999999999999,
          "std": 0.0,
          "min": 0.4249999999999999,
          "max": 0.4249999999999999,
          "median": 0.4249999999999999,
          "values": [
            0.4249999999999999
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8640000000000001,
          "std": 0.0,
          "min": 0.8640000000000001,
          "max": 0.8640000000000001,
          "median": 0.8640000000000001,
          "values": [
            0.8640000000000001
          ]
        },
        "single_sentence_format": {
          "mean": 0.37199999999999994,
          "std": 0.0,
          "min": 0.37199999999999994,
          "max": 0.37199999999999994,
          "median": 0.37199999999999994,
          "values": [
            0.37199999999999994
          ]
        }
      }
    },
    "ecare_explanation_generation_demographic_young_american_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03471821090973017,
          "std": 0.0,
          "min": 0.03471821090973017,
          "max": 0.03471821090973017,
          "median": 0.03471821090973017,
          "values": [
            0.03471821090973017
          ]
        },
        "recall": {
          "mean": 0.5147183143653732,
          "std": 0.0,
          "min": 0.5147183143653732,
          "max": 0.5147183143653732,
          "median": 0.5147183143653732,
          "values": [
            0.5147183143653732
          ]
        },
        "f1_score": {
          "mean": 0.0642778376021098,
          "std": 0.0,
          "min": 0.0642778376021098,
          "max": 0.0642778376021098,
          "median": 0.0642778376021098,
          "values": [
            0.0642778376021098
          ]
        },
        "jaccard": {
          "mean": 0.03374377868076859,
          "std": 0.0,
          "min": 0.03374377868076859,
          "max": 0.03374377868076859,
          "median": 0.03374377868076859,
          "values": [
            0.03374377868076859
          ]
        },
        "semantic_similarity": {
          "mean": 0.5751970493793488,
          "std": 0.0,
          "min": 0.5751970493793488,
          "max": 0.5751970493793488,
          "median": 0.5751970493793488,
          "values": [
            0.5751970493793488
          ]
        },
        "response_brevity": {
          "mean": 0.292,
          "std": 0.0,
          "min": 0.292,
          "max": 0.292,
          "median": 0.292,
          "values": [
            0.292
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8735,
          "std": 0.0,
          "min": 0.8735,
          "max": 0.8735,
          "median": 0.8735,
          "values": [
            0.8735
          ]
        },
        "single_sentence_format": {
          "mean": 0.118,
          "std": 0.0,
          "min": 0.118,
          "max": 0.118,
          "median": 0.118,
          "values": [
            0.118
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_medical_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_medical__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03246709410402508,
          "std": 0.0,
          "min": 0.03246709410402508,
          "max": 0.03246709410402508,
          "median": 0.03246709410402508,
          "values": [
            0.03246709410402508
          ]
        },
        "recall": {
          "mean": 0.4878397125750067,
          "std": 0.0,
          "min": 0.4878397125750067,
          "max": 0.4878397125750067,
          "median": 0.4878397125750067,
          "values": [
            0.4878397125750067
          ]
        },
        "f1_score": {
          "mean": 0.060162972182120834,
          "std": 0.0,
          "min": 0.060162972182120834,
          "max": 0.060162972182120834,
          "median": 0.060162972182120834,
          "values": [
            0.060162972182120834
          ]
        },
        "jaccard": {
          "mean": 0.031521197770612505,
          "std": 0.0,
          "min": 0.031521197770612505,
          "max": 0.031521197770612505,
          "median": 0.031521197770612505,
          "values": [
            0.031521197770612505
          ]
        },
        "semantic_similarity": {
          "mean": 0.5680893646925688,
          "std": 0.0,
          "min": 0.5680893646925688,
          "max": 0.5680893646925688,
          "median": 0.5680893646925688,
          "values": [
            0.5680893646925688
          ]
        },
        "response_brevity": {
          "mean": 0.29,
          "std": 0.0,
          "min": 0.29,
          "max": 0.29,
          "median": 0.29,
          "values": [
            0.29
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.884,
          "std": 0.0,
          "min": 0.884,
          "max": 0.884,
          "median": 0.884,
          "values": [
            0.884
          ]
        },
        "single_sentence_format": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        }
      }
    },
    "ecare_explanation_generation_expert_psychologist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02929200460140548,
          "std": 0.0,
          "min": 0.02929200460140548,
          "max": 0.02929200460140548,
          "median": 0.02929200460140548,
          "values": [
            0.02929200460140548
          ]
        },
        "recall": {
          "mean": 0.467410169895464,
          "std": 0.0,
          "min": 0.467410169895464,
          "max": 0.467410169895464,
          "median": 0.467410169895464,
          "values": [
            0.467410169895464
          ]
        },
        "f1_score": {
          "mean": 0.054526152588432734,
          "std": 0.0,
          "min": 0.054526152588432734,
          "max": 0.054526152588432734,
          "median": 0.054526152588432734,
          "values": [
            0.054526152588432734
          ]
        },
        "jaccard": {
          "mean": 0.02844595563828819,
          "std": 0.0,
          "min": 0.02844595563828819,
          "max": 0.02844595563828819,
          "median": 0.02844595563828819,
          "values": [
            0.02844595563828819
          ]
        },
        "semantic_similarity": {
          "mean": 0.5020062896609306,
          "std": 0.0,
          "min": 0.5020062896609306,
          "max": 0.5020062896609306,
          "median": 0.5020062896609306,
          "values": [
            0.5020062896609306
          ]
        },
        "response_brevity": {
          "mean": 0.284,
          "std": 0.0,
          "min": 0.284,
          "max": 0.284,
          "median": 0.284,
          "values": [
            0.284
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "single_sentence_format": {
          "mean": 0.012000000000000002,
          "std": 0.0,
          "min": 0.012000000000000002,
          "max": 0.012000000000000002,
          "median": 0.012000000000000002,
          "values": [
            0.012000000000000002
          ]
        }
      }
    },
    "ecare_explanation_generation_stakeholder_parent_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecare_explanation_generation__u4b-mistral-7b__zero-shot__ecare_explanation_generation_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecare_explanation_generation"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.02496805721240127,
          "std": 0.0,
          "min": 0.02496805721240127,
          "max": 0.02496805721240127,
          "median": 0.02496805721240127,
          "values": [
            0.02496805721240127
          ]
        },
        "recall": {
          "mean": 0.425678641619818,
          "std": 0.0,
          "min": 0.425678641619818,
          "max": 0.425678641619818,
          "median": 0.425678641619818,
          "values": [
            0.425678641619818
          ]
        },
        "f1_score": {
          "mean": 0.04675197749346605,
          "std": 0.0,
          "min": 0.04675197749346605,
          "max": 0.04675197749346605,
          "median": 0.04675197749346605,
          "values": [
            0.04675197749346605
          ]
        },
        "jaccard": {
          "mean": 0.024265730515130007,
          "std": 0.0,
          "min": 0.024265730515130007,
          "max": 0.024265730515130007,
          "median": 0.024265730515130007,
          "values": [
            0.024265730515130007
          ]
        },
        "semantic_similarity": {
          "mean": 0.4963519271463156,
          "std": 0.0,
          "min": 0.4963519271463156,
          "max": 0.4963519271463156,
          "median": 0.4963519271463156,
          "values": [
            0.4963519271463156
          ]
        },
        "response_brevity": {
          "mean": 0.276,
          "std": 0.0,
          "min": 0.276,
          "max": 0.276,
          "median": 0.276,
          "values": [
            0.276
          ]
        },
        "conceptual_abstraction_level": {
          "mean": 0.8339999999999997,
          "std": 0.0,
          "min": 0.8339999999999997,
          "max": 0.8339999999999997,
          "median": 0.8339999999999997,
          "values": [
            0.8339999999999997
          ]
        },
        "single_sentence_format": {
          "mean": 0.014000000000000002,
          "std": 0.0,
          "min": 0.014000000000000002,
          "max": 0.014000000000000002,
          "median": 0.014000000000000002,
          "values": [
            0.014000000000000002
          ]
        }
      }
    },
    "ecqa_choice_selection_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.01392248062886549,
          "std": 0.0,
          "min": 0.01392248062886549,
          "max": 0.01392248062886549,
          "median": 0.01392248062886549,
          "values": [
            0.01392248062886549
          ]
        },
        "recall": {
          "mean": 0.33,
          "std": 0.0,
          "min": 0.33,
          "max": 0.33,
          "median": 0.33,
          "values": [
            0.33
          ]
        },
        "f1_score": {
          "mean": 0.026068600412325373,
          "std": 0.0,
          "min": 0.026068600412325373,
          "max": 0.026068600412325373,
          "median": 0.026068600412325373,
          "values": [
            0.026068600412325373
          ]
        },
        "jaccard": {
          "mean": 0.013670879765186989,
          "std": 0.0,
          "min": 0.013670879765186989,
          "max": 0.013670879765186989,
          "median": 0.013670879765186989,
          "values": [
            0.013670879765186989
          ]
        },
        "semantic_similarity": {
          "mean": 0.4548057132959366,
          "std": 0.0,
          "min": 0.4548057132959366,
          "max": 0.4548057132959366,
          "median": 0.4548057132959366,
          "values": [
            0.4548057132959366
          ]
        },
        "answer_correctness": {
          "mean": 0.7,
          "std": 0.0,
          "min": 0.7,
          "max": 0.7,
          "median": 0.7,
          "values": [
            0.7
          ]
        },
        "follows_format_instruction": {
          "mean": 0.161,
          "std": 0.0,
          "min": 0.161,
          "max": 0.161,
          "median": 0.161,
          "values": [
            0.161
          ]
        },
        "answer_extractability": {
          "mean": 0.7632000000000001,
          "std": 0.0,
          "min": 0.7632000000000001,
          "max": 0.7632000000000001,
          "median": 0.7632000000000001,
          "values": [
            0.7632000000000001
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0075244364532344185,
          "std": 0.0,
          "min": 0.0075244364532344185,
          "max": 0.0075244364532344185,
          "median": 0.0075244364532344185,
          "values": [
            0.0075244364532344185
          ]
        },
        "recall": {
          "mean": 0.435,
          "std": 0.0,
          "min": 0.435,
          "max": 0.435,
          "median": 0.435,
          "values": [
            0.435
          ]
        },
        "f1_score": {
          "mean": 0.014743151517810566,
          "std": 0.0,
          "min": 0.014743151517810566,
          "max": 0.014743151517810566,
          "median": 0.014743151517810566,
          "values": [
            0.014743151517810566
          ]
        },
        "jaccard": {
          "mean": 0.007500635352431935,
          "std": 0.0,
          "min": 0.007500635352431935,
          "max": 0.007500635352431935,
          "median": 0.007500635352431935,
          "values": [
            0.007500635352431935
          ]
        },
        "semantic_similarity": {
          "mean": 0.3790281207114458,
          "std": 0.0,
          "min": 0.3790281207114458,
          "max": 0.3790281207114458,
          "median": 0.3790281207114458,
          "values": [
            0.3790281207114458
          ]
        },
        "answer_correctness": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09999999999999998,
          "std": 0.0,
          "min": 0.09999999999999998,
          "max": 0.09999999999999998,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.6062,
          "std": 0.0,
          "min": 0.6062,
          "max": 0.6062,
          "median": 0.6062,
          "values": [
            0.6062
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.007321897345832062,
          "std": 0.0,
          "min": 0.007321897345832062,
          "max": 0.007321897345832062,
          "median": 0.007321897345832062,
          "values": [
            0.007321897345832062
          ]
        },
        "recall": {
          "mean": 0.42,
          "std": 0.0,
          "min": 0.42,
          "max": 0.42,
          "median": 0.42,
          "values": [
            0.42
          ]
        },
        "f1_score": {
          "mean": 0.014357012832301902,
          "std": 0.0,
          "min": 0.014357012832301902,
          "max": 0.014357012832301902,
          "median": 0.014357012832301902,
          "values": [
            0.014357012832301902
          ]
        },
        "jaccard": {
          "mean": 0.007296665108754196,
          "std": 0.0,
          "min": 0.007296665108754196,
          "max": 0.007296665108754196,
          "median": 0.007296665108754196,
          "values": [
            0.007296665108754196
          ]
        },
        "semantic_similarity": {
          "mean": 0.38342820569872854,
          "std": 0.0,
          "min": 0.38342820569872854,
          "max": 0.38342820569872854,
          "median": 0.38342820569872854,
          "values": [
            0.38342820569872854
          ]
        },
        "answer_correctness": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09999999999999998,
          "std": 0.0,
          "min": 0.09999999999999998,
          "max": 0.09999999999999998,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.6078000000000001,
          "std": 0.0,
          "min": 0.6078000000000001,
          "max": 0.6078000000000001,
          "median": 0.6078000000000001,
          "values": [
            0.6078000000000001
          ]
        }
      }
    },
    "ecqa_demographic_young_american_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-mistral-nemo-12b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008851904336562421,
          "std": 0.0,
          "min": 0.008851904336562421,
          "max": 0.008851904336562421,
          "median": 0.008851904336562421,
          "values": [
            0.008851904336562421
          ]
        },
        "recall": {
          "mean": 0.465,
          "std": 0.0,
          "min": 0.465,
          "max": 0.465,
          "median": 0.465,
          "values": [
            0.465
          ]
        },
        "f1_score": {
          "mean": 0.01731503299164184,
          "std": 0.0,
          "min": 0.01731503299164184,
          "max": 0.01731503299164184,
          "median": 0.01731503299164184,
          "values": [
            0.01731503299164184
          ]
        },
        "jaccard": {
          "mean": 0.008818959330429315,
          "std": 0.0,
          "min": 0.008818959330429315,
          "max": 0.008818959330429315,
          "median": 0.008818959330429315,
          "values": [
            0.008818959330429315
          ]
        },
        "semantic_similarity": {
          "mean": 0.4007721272110939,
          "std": 0.0,
          "min": 0.4007721272110939,
          "max": 0.4007721272110939,
          "median": 0.4007721272110939,
          "values": [
            0.4007721272110939
          ]
        },
        "answer_correctness": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "follows_format_instruction": {
          "mean": 0.10399999999999998,
          "std": 0.0,
          "min": 0.10399999999999998,
          "max": 0.10399999999999998,
          "median": 0.10399999999999998,
          "values": [
            0.10399999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.5799,
          "std": 0.0,
          "min": 0.5799,
          "max": 0.5799,
          "median": 0.5799,
          "values": [
            0.5799
          ]
        }
      }
    },
    "ecqa_choice_selection_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.71,
          "std": 0.0,
          "min": 0.71,
          "max": 0.71,
          "median": 0.71,
          "values": [
            0.71
          ]
        },
        "precision": {
          "mean": 0.7200303295285826,
          "std": 0.0,
          "min": 0.7200303295285826,
          "max": 0.7200303295285826,
          "median": 0.7200303295285826,
          "values": [
            0.7200303295285826
          ]
        },
        "recall": {
          "mean": 0.81,
          "std": 0.0,
          "min": 0.81,
          "max": 0.81,
          "median": 0.81,
          "values": [
            0.81
          ]
        },
        "f1_score": {
          "mean": 0.7257207795377792,
          "std": 0.0,
          "min": 0.7257207795377792,
          "max": 0.7257207795377792,
          "median": 0.7257207795377792,
          "values": [
            0.7257207795377792
          ]
        },
        "jaccard": {
          "mean": 0.7198487332966234,
          "std": 0.0,
          "min": 0.7198487332966234,
          "max": 0.7198487332966234,
          "median": 0.7198487332966234,
          "values": [
            0.7198487332966234
          ]
        },
        "semantic_similarity": {
          "mean": 0.8337197145819664,
          "std": 0.0,
          "min": 0.8337197145819664,
          "max": 0.8337197145819664,
          "median": 0.8337197145819664,
          "values": [
            0.8337197145819664
          ]
        },
        "answer_correctness": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "follows_format_instruction": {
          "mean": 0.889,
          "std": 0.0,
          "min": 0.889,
          "max": 0.889,
          "median": 0.889,
          "values": [
            0.889
          ]
        },
        "answer_extractability": {
          "mean": 0.9538,
          "std": 0.0,
          "min": 0.9538,
          "max": 0.9538,
          "median": 0.9538,
          "values": [
            0.9538
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008522336538908305,
          "std": 0.0,
          "min": 0.008522336538908305,
          "max": 0.008522336538908305,
          "median": 0.008522336538908305,
          "values": [
            0.008522336538908305
          ]
        },
        "recall": {
          "mean": 0.54,
          "std": 0.0,
          "min": 0.54,
          "max": 0.54,
          "median": 0.54,
          "values": [
            0.54
          ]
        },
        "f1_score": {
          "mean": 0.016716160835231995,
          "std": 0.0,
          "min": 0.016716160835231995,
          "max": 0.016716160835231995,
          "median": 0.016716160835231995,
          "values": [
            0.016716160835231995
          ]
        },
        "jaccard": {
          "mean": 0.00849899385853161,
          "std": 0.0,
          "min": 0.00849899385853161,
          "max": 0.00849899385853161,
          "median": 0.00849899385853161,
          "values": [
            0.00849899385853161
          ]
        },
        "semantic_similarity": {
          "mean": 0.41642322421073913,
          "std": 0.0,
          "min": 0.41642322421073913,
          "max": 0.41642322421073913,
          "median": 0.41642322421073913,
          "values": [
            0.41642322421073913
          ]
        },
        "answer_correctness": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09999999999999998,
          "std": 0.0,
          "min": 0.09999999999999998,
          "max": 0.09999999999999998,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.6435999999999998,
          "std": 0.0,
          "min": 0.6435999999999998,
          "max": 0.6435999999999998,
          "median": 0.6435999999999998,
          "values": [
            0.6435999999999998
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008787499082289697,
          "std": 0.0,
          "min": 0.008787499082289697,
          "max": 0.008787499082289697,
          "median": 0.008787499082289697,
          "values": [
            0.008787499082289697
          ]
        },
        "recall": {
          "mean": 0.53,
          "std": 0.0,
          "min": 0.53,
          "max": 0.53,
          "median": 0.53,
          "values": [
            0.53
          ]
        },
        "f1_score": {
          "mean": 0.017228950058844103,
          "std": 0.0,
          "min": 0.017228950058844103,
          "max": 0.017228950058844103,
          "median": 0.017228950058844103,
          "values": [
            0.017228950058844103
          ]
        },
        "jaccard": {
          "mean": 0.00875311524941499,
          "std": 0.0,
          "min": 0.00875311524941499,
          "max": 0.00875311524941499,
          "median": 0.00875311524941499,
          "values": [
            0.00875311524941499
          ]
        },
        "semantic_similarity": {
          "mean": 0.4014344811439514,
          "std": 0.0,
          "min": 0.4014344811439514,
          "max": 0.4014344811439514,
          "median": 0.4014344811439514,
          "values": [
            0.4014344811439514
          ]
        },
        "answer_correctness": {
          "mean": 0.43,
          "std": 0.0,
          "min": 0.43,
          "max": 0.43,
          "median": 0.43,
          "values": [
            0.43
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09999999999999998,
          "std": 0.0,
          "min": 0.09999999999999998,
          "max": 0.09999999999999998,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.6483999999999999,
          "std": 0.0,
          "min": 0.6483999999999999,
          "max": 0.6483999999999999,
          "median": 0.6483999999999999,
          "values": [
            0.6483999999999999
          ]
        }
      }
    },
    "ecqa_demographic_young_american_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__hf-qwen2.5-3b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008311672919133647,
          "std": 0.0,
          "min": 0.008311672919133647,
          "max": 0.008311672919133647,
          "median": 0.008311672919133647,
          "values": [
            0.008311672919133647
          ]
        },
        "recall": {
          "mean": 0.585,
          "std": 0.0,
          "min": 0.585,
          "max": 0.585,
          "median": 0.585,
          "values": [
            0.585
          ]
        },
        "f1_score": {
          "mean": 0.01633992298797158,
          "std": 0.0,
          "min": 0.01633992298797158,
          "max": 0.01633992298797158,
          "median": 0.01633992298797158,
          "values": [
            0.01633992298797158
          ]
        },
        "jaccard": {
          "mean": 0.008289133337773414,
          "std": 0.0,
          "min": 0.008289133337773414,
          "max": 0.008289133337773414,
          "median": 0.008289133337773414,
          "values": [
            0.008289133337773414
          ]
        },
        "semantic_similarity": {
          "mean": 0.4062493397295475,
          "std": 0.0,
          "min": 0.4062493397295475,
          "max": 0.4062493397295475,
          "median": 0.4062493397295475,
          "values": [
            0.4062493397295475
          ]
        },
        "answer_correctness": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09999999999999998,
          "std": 0.0,
          "min": 0.09999999999999998,
          "max": 0.09999999999999998,
          "median": 0.09999999999999998,
          "values": [
            0.09999999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.5955999999999999,
          "std": 0.0,
          "min": 0.5955999999999999,
          "max": 0.5955999999999999,
          "median": 0.5955999999999999,
          "values": [
            0.5955999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.025762460610937087,
          "std": 0.0,
          "min": 0.025762460610937087,
          "max": 0.025762460610937087,
          "median": 0.025762460610937087,
          "values": [
            0.025762460610937087
          ]
        },
        "recall": {
          "mean": 0.54,
          "std": 0.0,
          "min": 0.54,
          "max": 0.54,
          "median": 0.54,
          "values": [
            0.54
          ]
        },
        "f1_score": {
          "mean": 0.04777438834102674,
          "std": 0.0,
          "min": 0.04777438834102674,
          "max": 0.04777438834102674,
          "median": 0.04777438834102674,
          "values": [
            0.04777438834102674
          ]
        },
        "jaccard": {
          "mean": 0.02571569986949813,
          "std": 0.0,
          "min": 0.02571569986949813,
          "max": 0.02571569986949813,
          "median": 0.02571569986949813,
          "values": [
            0.02571569986949813
          ]
        },
        "semantic_similarity": {
          "mean": 0.4356939936056733,
          "std": 0.0,
          "min": 0.4356939936056733,
          "max": 0.4356939936056733,
          "median": 0.4356939936056733,
          "values": [
            0.4356939936056733
          ]
        },
        "answer_correctness": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "follows_format_instruction": {
          "mean": 0.21300000000000008,
          "std": 0.0,
          "min": 0.21300000000000008,
          "max": 0.21300000000000008,
          "median": 0.21300000000000008,
          "values": [
            0.21300000000000008
          ]
        },
        "answer_extractability": {
          "mean": 0.8299000000000002,
          "std": 0.0,
          "min": 0.8299000000000002,
          "max": 0.8299000000000002,
          "median": 0.8299000000000002,
          "values": [
            0.8299000000000002
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.006771988861443903,
          "std": 0.0,
          "min": 0.006771988861443903,
          "max": 0.006771988861443903,
          "median": 0.006771988861443903,
          "values": [
            0.006771988861443903
          ]
        },
        "recall": {
          "mean": 0.485,
          "std": 0.0,
          "min": 0.485,
          "max": 0.485,
          "median": 0.485,
          "values": [
            0.485
          ]
        },
        "f1_score": {
          "mean": 0.013331199194835587,
          "std": 0.0,
          "min": 0.013331199194835587,
          "max": 0.013331199194835587,
          "median": 0.013331199194835587,
          "values": [
            0.013331199194835587
          ]
        },
        "jaccard": {
          "mean": 0.006752608745969655,
          "std": 0.0,
          "min": 0.006752608745969655,
          "max": 0.006752608745969655,
          "median": 0.006752608745969655,
          "values": [
            0.006752608745969655
          ]
        },
        "semantic_similarity": {
          "mean": 0.36877889998257163,
          "std": 0.0,
          "min": 0.36877889998257163,
          "max": 0.36877889998257163,
          "median": 0.36877889998257163,
          "values": [
            0.36877889998257163
          ]
        },
        "answer_correctness": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09899999999999999,
          "std": 0.0,
          "min": 0.09899999999999999,
          "max": 0.09899999999999999,
          "median": 0.09899999999999999,
          "values": [
            0.09899999999999999
          ]
        },
        "answer_extractability": {
          "mean": 0.6574999999999999,
          "std": 0.0,
          "min": 0.6574999999999999,
          "max": 0.6574999999999999,
          "median": 0.6574999999999999,
          "values": [
            0.6574999999999999
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.008099413110993587,
          "std": 0.0,
          "min": 0.008099413110993587,
          "max": 0.008099413110993587,
          "median": 0.008099413110993587,
          "values": [
            0.008099413110993587
          ]
        },
        "recall": {
          "mean": 0.425,
          "std": 0.0,
          "min": 0.425,
          "max": 0.425,
          "median": 0.425,
          "values": [
            0.425
          ]
        },
        "f1_score": {
          "mean": 0.015852074479994788,
          "std": 0.0,
          "min": 0.015852074479994788,
          "max": 0.015852074479994788,
          "median": 0.015852074479994788,
          "values": [
            0.015852074479994788
          ]
        },
        "jaccard": {
          "mean": 0.008069842468604512,
          "std": 0.0,
          "min": 0.008069842468604512,
          "max": 0.008069842468604512,
          "median": 0.008069842468604512,
          "values": [
            0.008069842468604512
          ]
        },
        "semantic_similarity": {
          "mean": 0.3479398596659303,
          "std": 0.0,
          "min": 0.3479398596659303,
          "max": 0.3479398596659303,
          "median": 0.3479398596659303,
          "values": [
            0.3479398596659303
          ]
        },
        "answer_correctness": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09799999999999998,
          "std": 0.0,
          "min": 0.09799999999999998,
          "max": 0.09799999999999998,
          "median": 0.09799999999999998,
          "values": [
            0.09799999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.7571000000000001,
          "std": 0.0,
          "min": 0.7571000000000001,
          "max": 0.7571000000000001,
          "median": 0.7571000000000001,
          "values": [
            0.7571000000000001
          ]
        }
      }
    },
    "ecqa_demographic_young_american_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3-8b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.010349293673168674,
          "std": 0.0,
          "min": 0.010349293673168674,
          "max": 0.010349293673168674,
          "median": 0.010349293673168674,
          "values": [
            0.010349293673168674
          ]
        },
        "recall": {
          "mean": 0.47,
          "std": 0.0,
          "min": 0.47,
          "max": 0.47,
          "median": 0.47,
          "values": [
            0.47
          ]
        },
        "f1_score": {
          "mean": 0.020178023662466838,
          "std": 0.0,
          "min": 0.020178023662466838,
          "max": 0.020178023662466838,
          "median": 0.020178023662466838,
          "values": [
            0.020178023662466838
          ]
        },
        "jaccard": {
          "mean": 0.010298951629070146,
          "std": 0.0,
          "min": 0.010298951629070146,
          "max": 0.010298951629070146,
          "median": 0.010298951629070146,
          "values": [
            0.010298951629070146
          ]
        },
        "semantic_similarity": {
          "mean": 0.4352716864645481,
          "std": 0.0,
          "min": 0.4352716864645481,
          "max": 0.4352716864645481,
          "median": 0.4352716864645481,
          "values": [
            0.4352716864645481
          ]
        },
        "answer_correctness": {
          "mean": 0.58,
          "std": 0.0,
          "min": 0.58,
          "max": 0.58,
          "median": 0.58,
          "values": [
            0.58
          ]
        },
        "follows_format_instruction": {
          "mean": 0.10099999999999998,
          "std": 0.0,
          "min": 0.10099999999999998,
          "max": 0.10099999999999998,
          "median": 0.10099999999999998,
          "values": [
            0.10099999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8152000000000001,
          "std": 0.0,
          "min": 0.8152000000000001,
          "max": 0.8152000000000001,
          "median": 0.8152000000000001,
          "values": [
            0.8152000000000001
          ]
        }
      }
    },
    "ecqa_choice_selection_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.010596463102426823,
          "std": 0.0,
          "min": 0.010596463102426823,
          "max": 0.010596463102426823,
          "median": 0.010596463102426823,
          "values": [
            0.010596463102426823
          ]
        },
        "recall": {
          "mean": 0.2833333333333333,
          "std": 0.0,
          "min": 0.2833333333333333,
          "max": 0.2833333333333333,
          "median": 0.2833333333333333,
          "values": [
            0.2833333333333333
          ]
        },
        "f1_score": {
          "mean": 0.020271237832488128,
          "std": 0.0,
          "min": 0.020271237832488128,
          "max": 0.020271237832488128,
          "median": 0.020271237832488128,
          "values": [
            0.020271237832488128
          ]
        },
        "jaccard": {
          "mean": 0.010506532081025269,
          "std": 0.0,
          "min": 0.010506532081025269,
          "max": 0.010506532081025269,
          "median": 0.010506532081025269,
          "values": [
            0.010506532081025269
          ]
        },
        "semantic_similarity": {
          "mean": 0.35456543466076257,
          "std": 0.0,
          "min": 0.35456543466076257,
          "max": 0.35456543466076257,
          "median": 0.35456543466076257,
          "values": [
            0.35456543466076257
          ]
        },
        "answer_correctness": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "follows_format_instruction": {
          "mean": 0.18600000000000008,
          "std": 0.0,
          "min": 0.18600000000000008,
          "max": 0.18600000000000008,
          "median": 0.18600000000000008,
          "values": [
            0.18600000000000008
          ]
        },
        "answer_extractability": {
          "mean": 0.8846000000000003,
          "std": 0.0,
          "min": 0.8846000000000003,
          "max": 0.8846000000000003,
          "median": 0.8846000000000003,
          "values": [
            0.8846000000000003
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.004456594705640506,
          "std": 0.0,
          "min": 0.004456594705640506,
          "max": 0.004456594705640506,
          "median": 0.004456594705640506,
          "values": [
            0.004456594705640506
          ]
        },
        "recall": {
          "mean": 0.335,
          "std": 0.0,
          "min": 0.335,
          "max": 0.335,
          "median": 0.335,
          "values": [
            0.335
          ]
        },
        "f1_score": {
          "mean": 0.008781695072649216,
          "std": 0.0,
          "min": 0.008781695072649216,
          "max": 0.008781695072649216,
          "median": 0.008781695072649216,
          "values": [
            0.008781695072649216
          ]
        },
        "jaccard": {
          "mean": 0.004442679345564411,
          "std": 0.0,
          "min": 0.004442679345564411,
          "max": 0.004442679345564411,
          "median": 0.004442679345564411,
          "values": [
            0.004442679345564411
          ]
        },
        "semantic_similarity": {
          "mean": 0.33140153259038924,
          "std": 0.0,
          "min": 0.33140153259038924,
          "max": 0.33140153259038924,
          "median": 0.33140153259038924,
          "values": [
            0.33140153259038924
          ]
        },
        "answer_correctness": {
          "mean": 0.24,
          "std": 0.0,
          "min": 0.24,
          "max": 0.24,
          "median": 0.24,
          "values": [
            0.24
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09799999999999998,
          "std": 0.0,
          "min": 0.09799999999999998,
          "max": 0.09799999999999998,
          "median": 0.09799999999999998,
          "values": [
            0.09799999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.7006,
          "std": 0.0,
          "min": 0.7006,
          "max": 0.7006,
          "median": 0.7006,
          "values": [
            0.7006
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.005083942634255996,
          "std": 0.0,
          "min": 0.005083942634255996,
          "max": 0.005083942634255996,
          "median": 0.005083942634255996,
          "values": [
            0.005083942634255996
          ]
        },
        "recall": {
          "mean": 0.34,
          "std": 0.0,
          "min": 0.34,
          "max": 0.34,
          "median": 0.34,
          "values": [
            0.34
          ]
        },
        "f1_score": {
          "mean": 0.009984408741376986,
          "std": 0.0,
          "min": 0.009984408741376986,
          "max": 0.009984408741376986,
          "median": 0.009984408741376986,
          "values": [
            0.009984408741376986
          ]
        },
        "jaccard": {
          "mean": 0.00507128939950518,
          "std": 0.0,
          "min": 0.00507128939950518,
          "max": 0.00507128939950518,
          "median": 0.00507128939950518,
          "values": [
            0.00507128939950518
          ]
        },
        "semantic_similarity": {
          "mean": 0.28721057476475836,
          "std": 0.0,
          "min": 0.28721057476475836,
          "max": 0.28721057476475836,
          "median": 0.28721057476475836,
          "values": [
            0.28721057476475836
          ]
        },
        "answer_correctness": {
          "mean": 0.18,
          "std": 0.0,
          "min": 0.18,
          "max": 0.18,
          "median": 0.18,
          "values": [
            0.18
          ]
        },
        "follows_format_instruction": {
          "mean": 0.09799999999999998,
          "std": 0.0,
          "min": 0.09799999999999998,
          "max": 0.09799999999999998,
          "median": 0.09799999999999998,
          "values": [
            0.09799999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.5879000000000001,
          "std": 0.0,
          "min": 0.5879000000000001,
          "max": 0.5879000000000001,
          "median": 0.5879000000000001,
          "values": [
            0.5879000000000001
          ]
        }
      }
    },
    "ecqa_demographic_young_american_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.2-1b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00624003478957561,
          "std": 0.0,
          "min": 0.00624003478957561,
          "max": 0.00624003478957561,
          "median": 0.00624003478957561,
          "values": [
            0.00624003478957561
          ]
        },
        "recall": {
          "mean": 0.405,
          "std": 0.0,
          "min": 0.405,
          "max": 0.405,
          "median": 0.405,
          "values": [
            0.405
          ]
        },
        "f1_score": {
          "mean": 0.01226328848181181,
          "std": 0.0,
          "min": 0.01226328848181181,
          "max": 0.01226328848181181,
          "median": 0.01226328848181181,
          "values": [
            0.01226328848181181
          ]
        },
        "jaccard": {
          "mean": 0.0062284589483395745,
          "std": 0.0,
          "min": 0.0062284589483395745,
          "max": 0.0062284589483395745,
          "median": 0.0062284589483395745,
          "values": [
            0.0062284589483395745
          ]
        },
        "semantic_similarity": {
          "mean": 0.31469399413093924,
          "std": 0.0,
          "min": 0.31469399413093924,
          "max": 0.31469399413093924,
          "median": 0.31469399413093924,
          "values": [
            0.31469399413093924
          ]
        },
        "answer_correctness": {
          "mean": 0.22,
          "std": 0.0,
          "min": 0.22,
          "max": 0.22,
          "median": 0.22,
          "values": [
            0.22
          ]
        },
        "follows_format_instruction": {
          "mean": 0.10099999999999998,
          "std": 0.0,
          "min": 0.10099999999999998,
          "max": 0.10099999999999998,
          "median": 0.10099999999999998,
          "values": [
            0.10099999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.7683000000000001,
          "std": 0.0,
          "min": 0.7683000000000001,
          "max": 0.7683000000000001,
          "median": 0.7683000000000001,
          "values": [
            0.7683000000000001
          ]
        }
      }
    },
    "ecqa_choice_selection_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.04550674217708795,
          "std": 0.0,
          "min": 0.04550674217708795,
          "max": 0.04550674217708795,
          "median": 0.04550674217708795,
          "values": [
            0.04550674217708795
          ]
        },
        "recall": {
          "mean": 0.54,
          "std": 0.0,
          "min": 0.54,
          "max": 0.54,
          "median": 0.54,
          "values": [
            0.54
          ]
        },
        "f1_score": {
          "mean": 0.06863770129462438,
          "std": 0.0,
          "min": 0.06863770129462438,
          "max": 0.06863770129462438,
          "median": 0.06863770129462438,
          "values": [
            0.06863770129462438
          ]
        },
        "jaccard": {
          "mean": 0.04336012654998884,
          "std": 0.0,
          "min": 0.04336012654998884,
          "max": 0.04336012654998884,
          "median": 0.04336012654998884,
          "values": [
            0.04336012654998884
          ]
        },
        "semantic_similarity": {
          "mean": 0.5132720082998276,
          "std": 0.0,
          "min": 0.5132720082998276,
          "max": 0.5132720082998276,
          "median": 0.5132720082998276,
          "values": [
            0.5132720082998276
          ]
        },
        "answer_correctness": {
          "mean": 0.64,
          "std": 0.0,
          "min": 0.64,
          "max": 0.64,
          "median": 0.64,
          "values": [
            0.64
          ]
        },
        "follows_format_instruction": {
          "mean": 0.36599999999999994,
          "std": 0.0,
          "min": 0.36599999999999994,
          "max": 0.36599999999999994,
          "median": 0.36599999999999994,
          "values": [
            0.36599999999999994
          ]
        },
        "answer_extractability": {
          "mean": 0.6979,
          "std": 0.0,
          "min": 0.6979,
          "max": 0.6979,
          "median": 0.6979,
          "values": [
            0.6979
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.010897709452009716,
          "std": 0.0,
          "min": 0.010897709452009716,
          "max": 0.010897709452009716,
          "median": 0.010897709452009716,
          "values": [
            0.010897709452009716
          ]
        },
        "recall": {
          "mean": 0.615,
          "std": 0.0,
          "min": 0.615,
          "max": 0.615,
          "median": 0.615,
          "values": [
            0.615
          ]
        },
        "f1_score": {
          "mean": 0.020587236977757863,
          "std": 0.0,
          "min": 0.020587236977757863,
          "max": 0.020587236977757863,
          "median": 0.020587236977757863,
          "values": [
            0.020587236977757863
          ]
        },
        "jaccard": {
          "mean": 0.01063337095222363,
          "std": 0.0,
          "min": 0.01063337095222363,
          "max": 0.01063337095222363,
          "median": 0.01063337095222363,
          "values": [
            0.01063337095222363
          ]
        },
        "semantic_similarity": {
          "mean": 0.43076386410743,
          "std": 0.0,
          "min": 0.43076386410743,
          "max": 0.43076386410743,
          "median": 0.43076386410743,
          "values": [
            0.43076386410743
          ]
        },
        "answer_correctness": {
          "mean": 0.64,
          "std": 0.0,
          "min": 0.64,
          "max": 0.64,
          "median": 0.64,
          "values": [
            0.64
          ]
        },
        "follows_format_instruction": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        },
        "answer_extractability": {
          "mean": 0.7589000000000001,
          "std": 0.0,
          "min": 0.7589000000000001,
          "max": 0.7589000000000001,
          "median": 0.7589000000000001,
          "values": [
            0.7589000000000001
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.015900449904138577,
          "std": 0.0,
          "min": 0.015900449904138577,
          "max": 0.015900449904138577,
          "median": 0.015900449904138577,
          "values": [
            0.015900449904138577
          ]
        },
        "recall": {
          "mean": 0.595,
          "std": 0.0,
          "min": 0.595,
          "max": 0.595,
          "median": 0.595,
          "values": [
            0.595
          ]
        },
        "f1_score": {
          "mean": 0.028667053839553005,
          "std": 0.0,
          "min": 0.028667053839553005,
          "max": 0.028667053839553005,
          "median": 0.028667053839553005,
          "values": [
            0.028667053839553005
          ]
        },
        "jaccard": {
          "mean": 0.01513401512745671,
          "std": 0.0,
          "min": 0.01513401512745671,
          "max": 0.01513401512745671,
          "median": 0.01513401512745671,
          "values": [
            0.01513401512745671
          ]
        },
        "semantic_similarity": {
          "mean": 0.4604719139635563,
          "std": 0.0,
          "min": 0.4604719139635563,
          "max": 0.4604719139635563,
          "median": 0.4604719139635563,
          "values": [
            0.4604719139635563
          ]
        },
        "answer_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "follows_format_instruction": {
          "mean": 0.20300000000000007,
          "std": 0.0,
          "min": 0.20300000000000007,
          "max": 0.20300000000000007,
          "median": 0.20300000000000007,
          "values": [
            0.20300000000000007
          ]
        },
        "answer_extractability": {
          "mean": 0.7271,
          "std": 0.0,
          "min": 0.7271,
          "max": 0.7271,
          "median": 0.7271,
          "values": [
            0.7271
          ]
        }
      }
    },
    "ecqa_demographic_young_american_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-llama3.3-70b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.013395739682222275,
          "std": 0.0,
          "min": 0.013395739682222275,
          "max": 0.013395739682222275,
          "median": 0.013395739682222275,
          "values": [
            0.013395739682222275
          ]
        },
        "recall": {
          "mean": 0.675,
          "std": 0.0,
          "min": 0.675,
          "max": 0.675,
          "median": 0.675,
          "values": [
            0.675
          ]
        },
        "f1_score": {
          "mean": 0.024727815174296598,
          "std": 0.0,
          "min": 0.024727815174296598,
          "max": 0.024727815174296598,
          "median": 0.024727815174296598,
          "values": [
            0.024727815174296598
          ]
        },
        "jaccard": {
          "mean": 0.012910120331113056,
          "std": 0.0,
          "min": 0.012910120331113056,
          "max": 0.012910120331113056,
          "median": 0.012910120331113056,
          "values": [
            0.012910120331113056
          ]
        },
        "semantic_similarity": {
          "mean": 0.4487741879373789,
          "std": 0.0,
          "min": 0.4487741879373789,
          "max": 0.4487741879373789,
          "median": 0.4487741879373789,
          "values": [
            0.4487741879373789
          ]
        },
        "answer_correctness": {
          "mean": 0.59,
          "std": 0.0,
          "min": 0.59,
          "max": 0.59,
          "median": 0.59,
          "values": [
            0.59
          ]
        },
        "follows_format_instruction": {
          "mean": 0.14300000000000002,
          "std": 0.0,
          "min": 0.14300000000000002,
          "max": 0.14300000000000002,
          "median": 0.14300000000000002,
          "values": [
            0.14300000000000002
          ]
        },
        "answer_extractability": {
          "mean": 0.7260999999999999,
          "std": 0.0,
          "min": 0.7260999999999999,
          "max": 0.7260999999999999,
          "median": 0.7260999999999999,
          "values": [
            0.7260999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_choice_selection__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.24,
          "std": 0.0,
          "min": 0.24,
          "max": 0.24,
          "median": 0.24,
          "values": [
            0.24
          ]
        },
        "precision": {
          "mean": 0.2597808004396367,
          "std": 0.0,
          "min": 0.2597808004396367,
          "max": 0.2597808004396367,
          "median": 0.2597808004396367,
          "values": [
            0.2597808004396367
          ]
        },
        "recall": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "f1_score": {
          "mean": 0.2708388738163502,
          "std": 0.0,
          "min": 0.2708388738163502,
          "max": 0.2708388738163502,
          "median": 0.2708388738163502,
          "values": [
            0.2708388738163502
          ]
        },
        "jaccard": {
          "mean": 0.25772324526474283,
          "std": 0.0,
          "min": 0.25772324526474283,
          "max": 0.25772324526474283,
          "median": 0.25772324526474283,
          "values": [
            0.25772324526474283
          ]
        },
        "semantic_similarity": {
          "mean": 0.5654132095724345,
          "std": 0.0,
          "min": 0.5654132095724345,
          "max": 0.5654132095724345,
          "median": 0.5654132095724345,
          "values": [
            0.5654132095724345
          ]
        },
        "answer_correctness": {
          "mean": 0.49,
          "std": 0.0,
          "min": 0.49,
          "max": 0.49,
          "median": 0.49,
          "values": [
            0.49
          ]
        },
        "follows_format_instruction": {
          "mean": 0.5309999999999999,
          "std": 0.0,
          "min": 0.5309999999999999,
          "max": 0.5309999999999999,
          "median": 0.5309999999999999,
          "values": [
            0.5309999999999999
          ]
        },
        "answer_extractability": {
          "mean": 0.811,
          "std": 0.0,
          "min": 0.811,
          "max": 0.811,
          "median": 0.811,
          "values": [
            0.811
          ]
        }
      }
    },
    "ecqa_demographic_middle_aged_asian_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_middle_aged_asian__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "precision": {
          "mean": 0.058802392947091234,
          "std": 0.0,
          "min": 0.058802392947091234,
          "max": 0.058802392947091234,
          "median": 0.058802392947091234,
          "values": [
            0.058802392947091234
          ]
        },
        "recall": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "f1_score": {
          "mean": 0.06717507399267747,
          "std": 0.0,
          "min": 0.06717507399267747,
          "max": 0.06717507399267747,
          "median": 0.06717507399267747,
          "values": [
            0.06717507399267747
          ]
        },
        "jaccard": {
          "mean": 0.05877274591003262,
          "std": 0.0,
          "min": 0.05877274591003262,
          "max": 0.05877274591003262,
          "median": 0.05877274591003262,
          "values": [
            0.05877274591003262
          ]
        },
        "semantic_similarity": {
          "mean": 0.4076335646212101,
          "std": 0.0,
          "min": 0.4076335646212101,
          "max": 0.4076335646212101,
          "median": 0.4076335646212101,
          "values": [
            0.4076335646212101
          ]
        },
        "answer_correctness": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "follows_format_instruction": {
          "mean": 0.153,
          "std": 0.0,
          "min": 0.153,
          "max": 0.153,
          "median": 0.153,
          "values": [
            0.153
          ]
        },
        "answer_extractability": {
          "mean": 0.616,
          "std": 0.0,
          "min": 0.616,
          "max": 0.616,
          "median": 0.616,
          "values": [
            0.616
          ]
        }
      }
    },
    "ecqa_demographic_senior_european_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_senior_european__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "precision": {
          "mean": 0.06235942907808319,
          "std": 0.0,
          "min": 0.06235942907808319,
          "max": 0.06235942907808319,
          "median": 0.06235942907808319,
          "values": [
            0.06235942907808319
          ]
        },
        "recall": {
          "mean": 0.58,
          "std": 0.0,
          "min": 0.58,
          "max": 0.58,
          "median": 0.58,
          "values": [
            0.58
          ]
        },
        "f1_score": {
          "mean": 0.07405976287381924,
          "std": 0.0,
          "min": 0.07405976287381924,
          "max": 0.07405976287381924,
          "median": 0.07405976287381924,
          "values": [
            0.07405976287381924
          ]
        },
        "jaccard": {
          "mean": 0.06230743797721426,
          "std": 0.0,
          "min": 0.06230743797721426,
          "max": 0.06230743797721426,
          "median": 0.06230743797721426,
          "values": [
            0.06230743797721426
          ]
        },
        "semantic_similarity": {
          "mean": 0.39791543390601875,
          "std": 0.0,
          "min": 0.39791543390601875,
          "max": 0.39791543390601875,
          "median": 0.39791543390601875,
          "values": [
            0.39791543390601875
          ]
        },
        "answer_correctness": {
          "mean": 0.42,
          "std": 0.0,
          "min": 0.42,
          "max": 0.42,
          "median": 0.42,
          "values": [
            0.42
          ]
        },
        "follows_format_instruction": {
          "mean": 0.158,
          "std": 0.0,
          "min": 0.158,
          "max": 0.158,
          "median": 0.158,
          "values": [
            0.158
          ]
        },
        "answer_extractability": {
          "mean": 0.6086,
          "std": 0.0,
          "min": 0.6086,
          "max": 0.6086,
          "median": 0.6086,
          "values": [
            0.6086
          ]
        }
      }
    },
    "ecqa_demographic_young_american_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice__u4b-mistral-7b__zero-shot__ecqa_demographic_young_american__100__0p100"
      ],
      "setups": [
        "ecqa_choice"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.05081169626119044,
          "std": 0.0,
          "min": 0.05081169626119044,
          "max": 0.05081169626119044,
          "median": 0.05081169626119044,
          "values": [
            0.05081169626119044
          ]
        },
        "recall": {
          "mean": 0.4666666666666666,
          "std": 0.0,
          "min": 0.4666666666666666,
          "max": 0.4666666666666666,
          "median": 0.4666666666666666,
          "values": [
            0.4666666666666666
          ]
        },
        "f1_score": {
          "mean": 0.06100051377676524,
          "std": 0.0,
          "min": 0.06100051377676524,
          "max": 0.06100051377676524,
          "median": 0.06100051377676524,
          "values": [
            0.06100051377676524
          ]
        },
        "jaccard": {
          "mean": 0.050740899211527586,
          "std": 0.0,
          "min": 0.050740899211527586,
          "max": 0.050740899211527586,
          "median": 0.050740899211527586,
          "values": [
            0.050740899211527586
          ]
        },
        "semantic_similarity": {
          "mean": 0.3956282104551792,
          "std": 0.0,
          "min": 0.3956282104551792,
          "max": 0.3956282104551792,
          "median": 0.3956282104551792,
          "values": [
            0.3956282104551792
          ]
        },
        "answer_correctness": {
          "mean": 0.33,
          "std": 0.0,
          "min": 0.33,
          "max": 0.33,
          "median": 0.33,
          "values": [
            0.33
          ]
        },
        "follows_format_instruction": {
          "mean": 0.158,
          "std": 0.0,
          "min": 0.158,
          "max": 0.158,
          "median": 0.158,
          "values": [
            0.158
          ]
        },
        "answer_extractability": {
          "mean": 0.6476999999999999,
          "std": 0.0,
          "min": 0.6476999999999999,
          "max": 0.6476999999999999,
          "median": 0.6476999999999999,
          "values": [
            0.6476999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.015070599419300978,
          "std": 0.0,
          "min": 0.015070599419300978,
          "max": 0.015070599419300978,
          "median": 0.015070599419300978,
          "values": [
            0.015070599419300978
          ]
        },
        "recall": {
          "mean": 0.325,
          "std": 0.0,
          "min": 0.325,
          "max": 0.325,
          "median": 0.325,
          "values": [
            0.325
          ]
        },
        "f1_score": {
          "mean": 0.028028060643947743,
          "std": 0.0,
          "min": 0.028028060643947743,
          "max": 0.028028060643947743,
          "median": 0.028028060643947743,
          "values": [
            0.028028060643947743
          ]
        },
        "jaccard": {
          "mean": 0.014757363766174309,
          "std": 0.0,
          "min": 0.014757363766174309,
          "max": 0.014757363766174309,
          "median": 0.014757363766174309,
          "values": [
            0.014757363766174309
          ]
        },
        "semantic_similarity": {
          "mean": 0.47166656263172624,
          "std": 0.0,
          "min": 0.47166656263172624,
          "max": 0.47166656263172624,
          "median": 0.47166656263172624,
          "values": [
            0.47166656263172624
          ]
        },
        "answer_correctness": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "follows_format_instruction": {
          "mean": 0.275,
          "std": 0.0,
          "min": 0.275,
          "max": 0.275,
          "median": 0.275,
          "values": [
            0.275
          ]
        },
        "answer_extractability": {
          "mean": 0.742,
          "std": 0.0,
          "min": 0.742,
          "max": 0.742,
          "median": 0.742,
          "values": [
            0.742
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.88,
          "std": 0.0,
          "min": 0.88,
          "max": 0.88,
          "median": 0.88,
          "values": [
            0.88
          ]
        },
        "precision": {
          "mean": 0.8861134676564157,
          "std": 0.0,
          "min": 0.8861134676564157,
          "max": 0.8861134676564157,
          "median": 0.8861134676564157,
          "values": [
            0.8861134676564157
          ]
        },
        "recall": {
          "mean": 0.915,
          "std": 0.0,
          "min": 0.915,
          "max": 0.915,
          "median": 0.915,
          "values": [
            0.915
          ]
        },
        "f1_score": {
          "mean": 0.8871428571428572,
          "std": 0.0,
          "min": 0.8871428571428572,
          "max": 0.8871428571428572,
          "median": 0.8871428571428572,
          "values": [
            0.8871428571428572
          ]
        },
        "jaccard": {
          "mean": 0.8844468009897491,
          "std": 0.0,
          "min": 0.8844468009897491,
          "max": 0.8844468009897491,
          "median": 0.8844468009897491,
          "values": [
            0.8844468009897491
          ]
        },
        "semantic_similarity": {
          "mean": 0.9308395344018936,
          "std": 0.0,
          "min": 0.9308395344018936,
          "max": 0.9308395344018936,
          "median": 0.9308395344018936,
          "values": [
            0.9308395344018936
          ]
        },
        "answer_correctness": {
          "mean": 0.9,
          "std": 0.0,
          "min": 0.9,
          "max": 0.9,
          "median": 0.9,
          "values": [
            0.9
          ]
        },
        "follows_format_instruction": {
          "mean": 0.9710000000000001,
          "std": 0.0,
          "min": 0.9710000000000001,
          "max": 0.9710000000000001,
          "median": 0.9710000000000001,
          "values": [
            0.9710000000000001
          ]
        },
        "answer_extractability": {
          "mean": 0.995,
          "std": 0.0,
          "min": 0.995,
          "max": 0.995,
          "median": 0.995,
          "values": [
            0.995
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__u4b-llama3-8b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.05439928258493284,
          "std": 0.0,
          "min": 0.05439928258493284,
          "max": 0.05439928258493284,
          "median": 0.05439928258493284,
          "values": [
            0.05439928258493284
          ]
        },
        "recall": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "f1_score": {
          "mean": 0.0937732139126142,
          "std": 0.0,
          "min": 0.0937732139126142,
          "max": 0.0937732139126142,
          "median": 0.0937732139126142,
          "values": [
            0.0937732139126142
          ]
        },
        "jaccard": {
          "mean": 0.05436054496686274,
          "std": 0.0,
          "min": 0.05436054496686274,
          "max": 0.05436054496686274,
          "median": 0.05436054496686274,
          "values": [
            0.05436054496686274
          ]
        },
        "semantic_similarity": {
          "mean": 0.4854020461440086,
          "std": 0.0,
          "min": 0.4854020461440086,
          "max": 0.4854020461440086,
          "median": 0.4854020461440086,
          "values": [
            0.4854020461440086
          ]
        },
        "answer_correctness": {
          "mean": 0.71,
          "std": 0.0,
          "min": 0.71,
          "max": 0.71,
          "median": 0.71,
          "values": [
            0.71
          ]
        },
        "follows_format_instruction": {
          "mean": 0.293,
          "std": 0.0,
          "min": 0.293,
          "max": 0.293,
          "median": 0.293,
          "values": [
            0.293
          ]
        },
        "answer_extractability": {
          "mean": 0.7715000000000001,
          "std": 0.0,
          "min": 0.7715000000000001,
          "max": 0.7715000000000001,
          "median": 0.7715000000000001,
          "values": [
            0.7715000000000001
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.01796684672017075,
          "std": 0.0,
          "min": 0.01796684672017075,
          "max": 0.01796684672017075,
          "median": 0.01796684672017075,
          "values": [
            0.01796684672017075
          ]
        },
        "recall": {
          "mean": 0.475,
          "std": 0.0,
          "min": 0.475,
          "max": 0.475,
          "median": 0.475,
          "values": [
            0.475
          ]
        },
        "f1_score": {
          "mean": 0.034374142890135184,
          "std": 0.0,
          "min": 0.034374142890135184,
          "max": 0.034374142890135184,
          "median": 0.034374142890135184,
          "values": [
            0.034374142890135184
          ]
        },
        "jaccard": {
          "mean": 0.017903803683989073,
          "std": 0.0,
          "min": 0.017903803683989073,
          "max": 0.017903803683989073,
          "median": 0.017903803683989073,
          "values": [
            0.017903803683989073
          ]
        },
        "semantic_similarity": {
          "mean": 0.43138746194541455,
          "std": 0.0,
          "min": 0.43138746194541455,
          "max": 0.43138746194541455,
          "median": 0.43138746194541455,
          "values": [
            0.43138746194541455
          ]
        },
        "answer_correctness": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "follows_format_instruction": {
          "mean": 0.1960000000000001,
          "std": 0.0,
          "min": 0.1960000000000001,
          "max": 0.1960000000000001,
          "median": 0.1960000000000001,
          "values": [
            0.1960000000000001
          ]
        },
        "answer_extractability": {
          "mean": 0.8757,
          "std": 0.0,
          "min": 0.8757,
          "max": 0.8757,
          "median": 0.8757,
          "values": [
            0.8757
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03487299022950155,
          "std": 0.0,
          "min": 0.03487299022950155,
          "max": 0.03487299022950155,
          "median": 0.03487299022950155,
          "values": [
            0.03487299022950155
          ]
        },
        "recall": {
          "mean": 0.585,
          "std": 0.0,
          "min": 0.585,
          "max": 0.585,
          "median": 0.585,
          "values": [
            0.585
          ]
        },
        "f1_score": {
          "mean": 0.05871220056958485,
          "std": 0.0,
          "min": 0.05871220056958485,
          "max": 0.05871220056958485,
          "median": 0.05871220056958485,
          "values": [
            0.05871220056958485
          ]
        },
        "jaccard": {
          "mean": 0.03216381606537508,
          "std": 0.0,
          "min": 0.03216381606537508,
          "max": 0.03216381606537508,
          "median": 0.03216381606537508,
          "values": [
            0.03216381606537508
          ]
        },
        "semantic_similarity": {
          "mean": 0.5449167729914188,
          "std": 0.0,
          "min": 0.5449167729914188,
          "max": 0.5449167729914188,
          "median": 0.5449167729914188,
          "values": [
            0.5449167729914188
          ]
        },
        "answer_correctness": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "follows_format_instruction": {
          "mean": 0.3440000000000001,
          "std": 0.0,
          "min": 0.3440000000000001,
          "max": 0.3440000000000001,
          "median": 0.3440000000000001,
          "values": [
            0.3440000000000001
          ]
        },
        "answer_extractability": {
          "mean": 0.7243999999999998,
          "std": 0.0,
          "min": 0.7243999999999998,
          "max": 0.7243999999999998,
          "median": 0.7243999999999998,
          "values": [
            0.7243999999999998
          ]
        }
      }
    },
    "ecqa_choice_selection_3choices_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_3choices__u4b-mistral-7b__zero-shot__ecqa_choice_selection_3choices__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_3choices"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.43,
          "std": 0.0,
          "min": 0.43,
          "max": 0.43,
          "median": 0.43,
          "values": [
            0.43
          ]
        },
        "precision": {
          "mean": 0.4432781474748814,
          "std": 0.0,
          "min": 0.4432781474748814,
          "max": 0.4432781474748814,
          "median": 0.4432781474748814,
          "values": [
            0.4432781474748814
          ]
        },
        "recall": {
          "mean": 0.655,
          "std": 0.0,
          "min": 0.655,
          "max": 0.655,
          "median": 0.655,
          "values": [
            0.655
          ]
        },
        "f1_score": {
          "mean": 0.4534595060833736,
          "std": 0.0,
          "min": 0.4534595060833736,
          "max": 0.4534595060833736,
          "median": 0.4534595060833736,
          "values": [
            0.4534595060833736
          ]
        },
        "jaccard": {
          "mean": 0.4430084735608898,
          "std": 0.0,
          "min": 0.4430084735608898,
          "max": 0.4430084735608898,
          "median": 0.4430084735608898,
          "values": [
            0.4430084735608898
          ]
        },
        "semantic_similarity": {
          "mean": 0.6941603437066078,
          "std": 0.0,
          "min": 0.6941603437066078,
          "max": 0.6941603437066078,
          "median": 0.6941603437066078,
          "values": [
            0.6941603437066078
          ]
        },
        "answer_correctness": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "follows_format_instruction": {
          "mean": 0.639,
          "std": 0.0,
          "min": 0.639,
          "max": 0.639,
          "median": 0.639,
          "values": [
            0.639
          ]
        },
        "answer_extractability": {
          "mean": 0.8578999999999999,
          "std": 0.0,
          "min": 0.8578999999999999,
          "max": 0.8578999999999999,
          "median": 0.8578999999999999,
          "values": [
            0.8578999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__hf-mistral-nemo-12b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.025522110573418848,
          "std": 0.0,
          "min": 0.025522110573418848,
          "max": 0.025522110573418848,
          "median": 0.025522110573418848,
          "values": [
            0.025522110573418848
          ]
        },
        "recall": {
          "mean": 0.3516666666666666,
          "std": 0.0,
          "min": 0.3516666666666666,
          "max": 0.3516666666666666,
          "median": 0.3516666666666666,
          "values": [
            0.3516666666666666
          ]
        },
        "f1_score": {
          "mean": 0.0385387951016717,
          "std": 0.0,
          "min": 0.0385387951016717,
          "max": 0.0385387951016717,
          "median": 0.0385387951016717,
          "values": [
            0.0385387951016717
          ]
        },
        "jaccard": {
          "mean": 0.02539481979440683,
          "std": 0.0,
          "min": 0.02539481979440683,
          "max": 0.02539481979440683,
          "median": 0.02539481979440683,
          "values": [
            0.02539481979440683
          ]
        },
        "semantic_similarity": {
          "mean": 0.47294129610061647,
          "std": 0.0,
          "min": 0.47294129610061647,
          "max": 0.47294129610061647,
          "median": 0.47294129610061647,
          "values": [
            0.47294129610061647
          ]
        },
        "answer_correctness": {
          "mean": 0.74,
          "std": 0.0,
          "min": 0.74,
          "max": 0.74,
          "median": 0.74,
          "values": [
            0.74
          ]
        },
        "follows_format_instruction": {
          "mean": 0.22500000000000003,
          "std": 0.0,
          "min": 0.22500000000000003,
          "max": 0.22500000000000003,
          "median": 0.22500000000000003,
          "values": [
            0.22500000000000003
          ]
        },
        "answer_extractability": {
          "mean": 0.8022,
          "std": 0.0,
          "min": 0.8022,
          "max": 0.8022,
          "median": 0.8022,
          "values": [
            0.8022
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.71,
          "std": 0.0,
          "min": 0.71,
          "max": 0.71,
          "median": 0.71,
          "values": [
            0.71
          ]
        },
        "precision": {
          "mean": 0.713981449444274,
          "std": 0.0,
          "min": 0.713981449444274,
          "max": 0.713981449444274,
          "median": 0.713981449444274,
          "values": [
            0.713981449444274
          ]
        },
        "recall": {
          "mean": 0.775,
          "std": 0.0,
          "min": 0.775,
          "max": 0.775,
          "median": 0.775,
          "values": [
            0.775
          ]
        },
        "f1_score": {
          "mean": 0.7170445398747617,
          "std": 0.0,
          "min": 0.7170445398747617,
          "max": 0.7170445398747617,
          "median": 0.7170445398747617,
          "values": [
            0.7170445398747617
          ]
        },
        "jaccard": {
          "mean": 0.7138028780157026,
          "std": 0.0,
          "min": 0.7138028780157026,
          "max": 0.7138028780157026,
          "median": 0.7138028780157026,
          "values": [
            0.7138028780157026
          ]
        },
        "semantic_similarity": {
          "mean": 0.8369075068831444,
          "std": 0.0,
          "min": 0.8369075068831444,
          "max": 0.8369075068831444,
          "median": 0.8369075068831444,
          "values": [
            0.8369075068831444
          ]
        },
        "answer_correctness": {
          "mean": 0.81,
          "std": 0.0,
          "min": 0.81,
          "max": 0.81,
          "median": 0.81,
          "values": [
            0.81
          ]
        },
        "follows_format_instruction": {
          "mean": 0.8859999999999999,
          "std": 0.0,
          "min": 0.8859999999999999,
          "max": 0.8859999999999999,
          "median": 0.8859999999999999,
          "values": [
            0.8859999999999999
          ]
        },
        "answer_extractability": {
          "mean": 0.9483,
          "std": 0.0,
          "min": 0.9483,
          "max": 0.9483,
          "median": 0.9483,
          "values": [
            0.9483
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__u4b-llama3-8b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.034395576029195414,
          "std": 0.0,
          "min": 0.034395576029195414,
          "max": 0.034395576029195414,
          "median": 0.034395576029195414,
          "values": [
            0.034395576029195414
          ]
        },
        "recall": {
          "mean": 0.6,
          "std": 0.0,
          "min": 0.6,
          "max": 0.6,
          "median": 0.6,
          "values": [
            0.6
          ]
        },
        "f1_score": {
          "mean": 0.061479280912402456,
          "std": 0.0,
          "min": 0.061479280912402456,
          "max": 0.061479280912402456,
          "median": 0.061479280912402456,
          "values": [
            0.061479280912402456
          ]
        },
        "jaccard": {
          "mean": 0.034357544259054885,
          "std": 0.0,
          "min": 0.034357544259054885,
          "max": 0.034357544259054885,
          "median": 0.034357544259054885,
          "values": [
            0.034357544259054885
          ]
        },
        "semantic_similarity": {
          "mean": 0.4459284994006157,
          "std": 0.0,
          "min": 0.4459284994006157,
          "max": 0.4459284994006157,
          "median": 0.4459284994006157,
          "values": [
            0.4459284994006157
          ]
        },
        "answer_correctness": {
          "mean": 0.61,
          "std": 0.0,
          "min": 0.61,
          "max": 0.61,
          "median": 0.61,
          "values": [
            0.61
          ]
        },
        "follows_format_instruction": {
          "mean": 0.262,
          "std": 0.0,
          "min": 0.262,
          "max": 0.262,
          "median": 0.262,
          "values": [
            0.262
          ]
        },
        "answer_extractability": {
          "mean": 0.8004000000000001,
          "std": 0.0,
          "min": 0.8004000000000001,
          "max": 0.8004000000000001,
          "median": 0.8004000000000001,
          "values": [
            0.8004000000000001
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__u4b-llama3.2-1b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.012120390341419882,
          "std": 0.0,
          "min": 0.012120390341419882,
          "max": 0.012120390341419882,
          "median": 0.012120390341419882,
          "values": [
            0.012120390341419882
          ]
        },
        "recall": {
          "mean": 0.31166666666666665,
          "std": 0.0,
          "min": 0.31166666666666665,
          "max": 0.31166666666666665,
          "median": 0.31166666666666665,
          "values": [
            0.31166666666666665
          ]
        },
        "f1_score": {
          "mean": 0.02315734635492113,
          "std": 0.0,
          "min": 0.02315734635492113,
          "max": 0.02315734635492113,
          "median": 0.02315734635492113,
          "values": [
            0.02315734635492113
          ]
        },
        "jaccard": {
          "mean": 0.012029112390921175,
          "std": 0.0,
          "min": 0.012029112390921175,
          "max": 0.012029112390921175,
          "median": 0.012029112390921175,
          "values": [
            0.012029112390921175
          ]
        },
        "semantic_similarity": {
          "mean": 0.37241387432906775,
          "std": 0.0,
          "min": 0.37241387432906775,
          "max": 0.37241387432906775,
          "median": 0.37241387432906775,
          "values": [
            0.37241387432906775
          ]
        },
        "answer_correctness": {
          "mean": 0.26,
          "std": 0.0,
          "min": 0.26,
          "max": 0.26,
          "median": 0.26,
          "values": [
            0.26
          ]
        },
        "follows_format_instruction": {
          "mean": 0.21300000000000005,
          "std": 0.0,
          "min": 0.21300000000000005,
          "max": 0.21300000000000005,
          "median": 0.21300000000000005,
          "values": [
            0.21300000000000005
          ]
        },
        "answer_extractability": {
          "mean": 0.8685,
          "std": 0.0,
          "min": 0.8685,
          "max": 0.8685,
          "median": 0.8685,
          "values": [
            0.8685
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__u4b-llama3.3-70b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.03438889843834528,
          "std": 0.0,
          "min": 0.03438889843834528,
          "max": 0.03438889843834528,
          "median": 0.03438889843834528,
          "values": [
            0.03438889843834528
          ]
        },
        "recall": {
          "mean": 0.57,
          "std": 0.0,
          "min": 0.57,
          "max": 0.57,
          "median": 0.57,
          "values": [
            0.57
          ]
        },
        "f1_score": {
          "mean": 0.05706266295843628,
          "std": 0.0,
          "min": 0.05706266295843628,
          "max": 0.05706266295843628,
          "median": 0.05706266295843628,
          "values": [
            0.05706266295843628
          ]
        },
        "jaccard": {
          "mean": 0.03173746015704794,
          "std": 0.0,
          "min": 0.03173746015704794,
          "max": 0.03173746015704794,
          "median": 0.03173746015704794,
          "values": [
            0.03173746015704794
          ]
        },
        "semantic_similarity": {
          "mean": 0.5219494701176882,
          "std": 0.0,
          "min": 0.5219494701176882,
          "max": 0.5219494701176882,
          "median": 0.5219494701176882,
          "values": [
            0.5219494701176882
          ]
        },
        "answer_correctness": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "follows_format_instruction": {
          "mean": 0.333,
          "std": 0.0,
          "min": 0.333,
          "max": 0.333,
          "median": 0.333,
          "values": [
            0.333
          ]
        },
        "answer_extractability": {
          "mean": 0.7214999999999999,
          "std": 0.0,
          "min": 0.7214999999999999,
          "max": 0.7214999999999999,
          "median": 0.7214999999999999,
          "values": [
            0.7214999999999999
          ]
        }
      }
    },
    "ecqa_choice_selection_no_concept_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_choice_selection_no_concept__u4b-mistral-7b__zero-shot__ecqa_choice_selection_no_concept__100__0p100"
      ],
      "setups": [
        "ecqa_choice_selection_no_concept"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        },
        "precision": {
          "mean": 0.3943153894018689,
          "std": 0.0,
          "min": 0.3943153894018689,
          "max": 0.3943153894018689,
          "median": 0.3943153894018689,
          "values": [
            0.3943153894018689
          ]
        },
        "recall": {
          "mean": 0.56,
          "std": 0.0,
          "min": 0.56,
          "max": 0.56,
          "median": 0.56,
          "values": [
            0.56
          ]
        },
        "f1_score": {
          "mean": 0.4045523737232297,
          "std": 0.0,
          "min": 0.4045523737232297,
          "max": 0.4045523737232297,
          "median": 0.4045523737232297,
          "values": [
            0.4045523737232297
          ]
        },
        "jaccard": {
          "mean": 0.39381479792301577,
          "std": 0.0,
          "min": 0.39381479792301577,
          "max": 0.39381479792301577,
          "median": 0.39381479792301577,
          "values": [
            0.39381479792301577
          ]
        },
        "semantic_similarity": {
          "mean": 0.6547163240611553,
          "std": 0.0,
          "min": 0.6547163240611553,
          "max": 0.6547163240611553,
          "median": 0.6547163240611553,
          "values": [
            0.6547163240611553
          ]
        },
        "answer_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "follows_format_instruction": {
          "mean": 0.6529999999999998,
          "std": 0.0,
          "min": 0.6529999999999998,
          "max": 0.6529999999999998,
          "median": 0.6529999999999998,
          "values": [
            0.6529999999999998
          ]
        },
        "answer_extractability": {
          "mean": 0.8569999999999999,
          "std": 0.0,
          "min": 0.8569999999999999,
          "max": 0.8569999999999999,
          "median": 0.8569999999999999,
          "values": [
            0.8569999999999999
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09650138003581887,
          "std": 0.0,
          "min": 0.09650138003581887,
          "max": 0.09650138003581887,
          "median": 0.09650138003581887,
          "values": [
            0.09650138003581887
          ]
        },
        "recall": {
          "mean": 0.38084291264397324,
          "std": 0.0,
          "min": 0.38084291264397324,
          "max": 0.38084291264397324,
          "median": 0.38084291264397324,
          "values": [
            0.38084291264397324
          ]
        },
        "f1_score": {
          "mean": 0.15099882675974377,
          "std": 0.0,
          "min": 0.15099882675974377,
          "max": 0.15099882675974377,
          "median": 0.15099882675974377,
          "values": [
            0.15099882675974377
          ]
        },
        "jaccard": {
          "mean": 0.08225184840743578,
          "std": 0.0,
          "min": 0.08225184840743578,
          "max": 0.08225184840743578,
          "median": 0.08225184840743578,
          "values": [
            0.08225184840743578
          ]
        },
        "semantic_similarity": {
          "mean": 0.6301788941025734,
          "std": 0.0,
          "min": 0.6301788941025734,
          "max": 0.6301788941025734,
          "median": 0.6301788941025734,
          "values": [
            0.6301788941025734
          ]
        },
        "all_choices_addressed": {
          "mean": 0.302,
          "std": 0.0,
          "min": 0.302,
          "max": 0.302,
          "median": 0.302,
          "values": [
            0.302
          ]
        },
        "single_paragraph_format": {
          "mean": 0.972,
          "std": 0.0,
          "min": 0.972,
          "max": 0.972,
          "median": 0.972,
          "values": [
            0.972
          ]
        },
        "explanation_depth": {
          "mean": 0.543,
          "std": 0.0,
          "min": 0.543,
          "max": 0.543,
          "median": 0.543,
          "values": [
            0.543
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09607084698848553,
          "std": 0.0,
          "min": 0.09607084698848553,
          "max": 0.09607084698848553,
          "median": 0.09607084698848553,
          "values": [
            0.09607084698848553
          ]
        },
        "recall": {
          "mean": 0.37047164898773843,
          "std": 0.0,
          "min": 0.37047164898773843,
          "max": 0.37047164898773843,
          "median": 0.37047164898773843,
          "values": [
            0.37047164898773843
          ]
        },
        "f1_score": {
          "mean": 0.14936706195967742,
          "std": 0.0,
          "min": 0.14936706195967742,
          "max": 0.14936706195967742,
          "median": 0.14936706195967742,
          "values": [
            0.14936706195967742
          ]
        },
        "jaccard": {
          "mean": 0.08134974725981296,
          "std": 0.0,
          "min": 0.08134974725981296,
          "max": 0.08134974725981296,
          "median": 0.08134974725981296,
          "values": [
            0.08134974725981296
          ]
        },
        "semantic_similarity": {
          "mean": 0.5396952456235886,
          "std": 0.0,
          "min": 0.5396952456235886,
          "max": 0.5396952456235886,
          "median": 0.5396952456235886,
          "values": [
            0.5396952456235886
          ]
        },
        "all_choices_addressed": {
          "mean": 0.4160000000000001,
          "std": 0.0,
          "min": 0.4160000000000001,
          "max": 0.4160000000000001,
          "median": 0.4160000000000001,
          "values": [
            0.4160000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.4467000000000001,
          "std": 0.0,
          "min": 0.4467000000000001,
          "max": 0.4467000000000001,
          "median": 0.4467000000000001,
          "values": [
            0.4467000000000001
          ]
        }
      }
    },
    "ecqa_expert_psychologist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09450176493727287,
          "std": 0.0,
          "min": 0.09450176493727287,
          "max": 0.09450176493727287,
          "median": 0.09450176493727287,
          "values": [
            0.09450176493727287
          ]
        },
        "recall": {
          "mean": 0.38188838754118026,
          "std": 0.0,
          "min": 0.38188838754118026,
          "max": 0.38188838754118026,
          "median": 0.38188838754118026,
          "values": [
            0.38188838754118026
          ]
        },
        "f1_score": {
          "mean": 0.14893942173234243,
          "std": 0.0,
          "min": 0.14893942173234243,
          "max": 0.14893942173234243,
          "median": 0.14893942173234243,
          "values": [
            0.14893942173234243
          ]
        },
        "jaccard": {
          "mean": 0.08103738515521011,
          "std": 0.0,
          "min": 0.08103738515521011,
          "max": 0.08103738515521011,
          "median": 0.08103738515521011,
          "values": [
            0.08103738515521011
          ]
        },
        "semantic_similarity": {
          "mean": 0.5997585594654083,
          "std": 0.0,
          "min": 0.5997585594654083,
          "max": 0.5997585594654083,
          "median": 0.5997585594654083,
          "values": [
            0.5997585594654083
          ]
        },
        "all_choices_addressed": {
          "mean": 0.41400000000000003,
          "std": 0.0,
          "min": 0.41400000000000003,
          "max": 0.41400000000000003,
          "median": 0.41400000000000003,
          "values": [
            0.41400000000000003
          ]
        },
        "single_paragraph_format": {
          "mean": 0.986,
          "std": 0.0,
          "min": 0.986,
          "max": 0.986,
          "median": 0.986,
          "values": [
            0.986
          ]
        },
        "explanation_depth": {
          "mean": 0.4244,
          "std": 0.0,
          "min": 0.4244,
          "max": 0.4244,
          "median": 0.4244,
          "values": [
            0.4244
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1835645434735207,
          "std": 0.0,
          "min": 0.1835645434735207,
          "max": 0.1835645434735207,
          "median": 0.1835645434735207,
          "values": [
            0.1835645434735207
          ]
        },
        "recall": {
          "mean": 0.39513423805964415,
          "std": 0.0,
          "min": 0.39513423805964415,
          "max": 0.39513423805964415,
          "median": 0.39513423805964415,
          "values": [
            0.39513423805964415
          ]
        },
        "f1_score": {
          "mean": 0.24417745411887654,
          "std": 0.0,
          "min": 0.24417745411887654,
          "max": 0.24417745411887654,
          "median": 0.24417745411887654,
          "values": [
            0.24417745411887654
          ]
        },
        "jaccard": {
          "mean": 0.1411079925248137,
          "std": 0.0,
          "min": 0.1411079925248137,
          "max": 0.1411079925248137,
          "median": 0.1411079925248137,
          "values": [
            0.1411079925248137
          ]
        },
        "semantic_similarity": {
          "mean": 0.7116961959004402,
          "std": 0.0,
          "min": 0.7116961959004402,
          "max": 0.7116961959004402,
          "median": 0.7116961959004402,
          "values": [
            0.7116961959004402
          ]
        },
        "all_choices_addressed": {
          "mean": 0.9039999999999999,
          "std": 0.0,
          "min": 0.9039999999999999,
          "max": 0.9039999999999999,
          "median": 0.9039999999999999,
          "values": [
            0.9039999999999999
          ]
        },
        "single_paragraph_format": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "explanation_depth": {
          "mean": 0.4403000000000001,
          "std": 0.0,
          "min": 0.4403000000000001,
          "max": 0.4403000000000001,
          "median": 0.4403000000000001,
          "values": [
            0.4403000000000001
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10169878430134373,
          "std": 0.0,
          "min": 0.10169878430134373,
          "max": 0.10169878430134373,
          "median": 0.10169878430134373,
          "values": [
            0.10169878430134373
          ]
        },
        "recall": {
          "mean": 0.3557897897627871,
          "std": 0.0,
          "min": 0.3557897897627871,
          "max": 0.3557897897627871,
          "median": 0.3557897897627871,
          "values": [
            0.3557897897627871
          ]
        },
        "f1_score": {
          "mean": 0.15528852842993202,
          "std": 0.0,
          "min": 0.15528852842993202,
          "max": 0.15528852842993202,
          "median": 0.15528852842993202,
          "values": [
            0.15528852842993202
          ]
        },
        "jaccard": {
          "mean": 0.0847125591934511,
          "std": 0.0,
          "min": 0.0847125591934511,
          "max": 0.0847125591934511,
          "median": 0.0847125591934511,
          "values": [
            0.0847125591934511
          ]
        },
        "semantic_similarity": {
          "mean": 0.4699423840641975,
          "std": 0.0,
          "min": 0.4699423840641975,
          "max": 0.4699423840641975,
          "median": 0.4699423840641975,
          "values": [
            0.4699423840641975
          ]
        },
        "all_choices_addressed": {
          "mean": 0.30999999999999994,
          "std": 0.0,
          "min": 0.30999999999999994,
          "max": 0.30999999999999994,
          "median": 0.30999999999999994,
          "values": [
            0.30999999999999994
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.30060000000000003,
          "std": 0.0,
          "min": 0.30060000000000003,
          "max": 0.30060000000000003,
          "median": 0.30060000000000003,
          "values": [
            0.30060000000000003
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09410443772872569,
          "std": 0.0,
          "min": 0.09410443772872569,
          "max": 0.09410443772872569,
          "median": 0.09410443772872569,
          "values": [
            0.09410443772872569
          ]
        },
        "recall": {
          "mean": 0.3312279727082102,
          "std": 0.0,
          "min": 0.3312279727082102,
          "max": 0.3312279727082102,
          "median": 0.3312279727082102,
          "values": [
            0.3312279727082102
          ]
        },
        "f1_score": {
          "mean": 0.14367045420676094,
          "std": 0.0,
          "min": 0.14367045420676094,
          "max": 0.14367045420676094,
          "median": 0.14367045420676094,
          "values": [
            0.14367045420676094
          ]
        },
        "jaccard": {
          "mean": 0.07790322868928407,
          "std": 0.0,
          "min": 0.07790322868928407,
          "max": 0.07790322868928407,
          "median": 0.07790322868928407,
          "values": [
            0.07790322868928407
          ]
        },
        "semantic_similarity": {
          "mean": 0.5527061250805855,
          "std": 0.0,
          "min": 0.5527061250805855,
          "max": 0.5527061250805855,
          "median": 0.5527061250805855,
          "values": [
            0.5527061250805855
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3880000000000001,
          "std": 0.0,
          "min": 0.3880000000000001,
          "max": 0.3880000000000001,
          "median": 0.3880000000000001,
          "values": [
            0.3880000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.3159,
          "std": 0.0,
          "min": 0.3159,
          "max": 0.3159,
          "median": 0.3159,
          "values": [
            0.3159
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09740329080545494,
          "std": 0.0,
          "min": 0.09740329080545494,
          "max": 0.09740329080545494,
          "median": 0.09740329080545494,
          "values": [
            0.09740329080545494
          ]
        },
        "recall": {
          "mean": 0.3391893547719219,
          "std": 0.0,
          "min": 0.3391893547719219,
          "max": 0.3391893547719219,
          "median": 0.3391893547719219,
          "values": [
            0.3391893547719219
          ]
        },
        "f1_score": {
          "mean": 0.14808039843271317,
          "std": 0.0,
          "min": 0.14808039843271317,
          "max": 0.14808039843271317,
          "median": 0.14808039843271317,
          "values": [
            0.14808039843271317
          ]
        },
        "jaccard": {
          "mean": 0.08049937836942817,
          "std": 0.0,
          "min": 0.08049937836942817,
          "max": 0.08049937836942817,
          "median": 0.08049937836942817,
          "values": [
            0.08049937836942817
          ]
        },
        "semantic_similarity": {
          "mean": 0.5446191549301147,
          "std": 0.0,
          "min": 0.5446191549301147,
          "max": 0.5446191549301147,
          "median": 0.5446191549301147,
          "values": [
            0.5446191549301147
          ]
        },
        "all_choices_addressed": {
          "mean": 0.4420000000000001,
          "std": 0.0,
          "min": 0.4420000000000001,
          "max": 0.4420000000000001,
          "median": 0.4420000000000001,
          "values": [
            0.4420000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.3622000000000001,
          "std": 0.0,
          "min": 0.3622000000000001,
          "max": 0.3622000000000001,
          "median": 0.3622000000000001,
          "values": [
            0.3622000000000001
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10244197370875899,
          "std": 0.0,
          "min": 0.10244197370875899,
          "max": 0.10244197370875899,
          "median": 0.10244197370875899,
          "values": [
            0.10244197370875899
          ]
        },
        "recall": {
          "mean": 0.4260019471045889,
          "std": 0.0,
          "min": 0.4260019471045889,
          "max": 0.4260019471045889,
          "median": 0.4260019471045889,
          "values": [
            0.4260019471045889
          ]
        },
        "f1_score": {
          "mean": 0.1623083618713913,
          "std": 0.0,
          "min": 0.1623083618713913,
          "max": 0.1623083618713913,
          "median": 0.1623083618713913,
          "values": [
            0.1623083618713913
          ]
        },
        "jaccard": {
          "mean": 0.0889264163712685,
          "std": 0.0,
          "min": 0.0889264163712685,
          "max": 0.0889264163712685,
          "median": 0.0889264163712685,
          "values": [
            0.0889264163712685
          ]
        },
        "semantic_similarity": {
          "mean": 0.5942050370573998,
          "std": 0.0,
          "min": 0.5942050370573998,
          "max": 0.5942050370573998,
          "median": 0.5942050370573998,
          "values": [
            0.5942050370573998
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3880000000000001,
          "std": 0.0,
          "min": 0.3880000000000001,
          "max": 0.3880000000000001,
          "median": 0.3880000000000001,
          "values": [
            0.3880000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 0.96,
          "std": 0.0,
          "min": 0.96,
          "max": 0.96,
          "median": 0.96,
          "values": [
            0.96
          ]
        },
        "explanation_depth": {
          "mean": 0.48340000000000005,
          "std": 0.0,
          "min": 0.48340000000000005,
          "max": 0.48340000000000005,
          "median": 0.48340000000000005,
          "values": [
            0.48340000000000005
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10027596243556176,
          "std": 0.0,
          "min": 0.10027596243556176,
          "max": 0.10027596243556176,
          "median": 0.10027596243556176,
          "values": [
            0.10027596243556176
          ]
        },
        "recall": {
          "mean": 0.4257268849721244,
          "std": 0.0,
          "min": 0.4257268849721244,
          "max": 0.4257268849721244,
          "median": 0.4257268849721244,
          "values": [
            0.4257268849721244
          ]
        },
        "f1_score": {
          "mean": 0.1593944547925764,
          "std": 0.0,
          "min": 0.1593944547925764,
          "max": 0.1593944547925764,
          "median": 0.1593944547925764,
          "values": [
            0.1593944547925764
          ]
        },
        "jaccard": {
          "mean": 0.08722653665749008,
          "std": 0.0,
          "min": 0.08722653665749008,
          "max": 0.08722653665749008,
          "median": 0.08722653665749008,
          "values": [
            0.08722653665749008
          ]
        },
        "semantic_similarity": {
          "mean": 0.4992094074189663,
          "std": 0.0,
          "min": 0.4992094074189663,
          "max": 0.4992094074189663,
          "median": 0.4992094074189663,
          "values": [
            0.4992094074189663
          ]
        },
        "all_choices_addressed": {
          "mean": 0.41200000000000014,
          "std": 0.0,
          "min": 0.41200000000000014,
          "max": 0.41200000000000014,
          "median": 0.41200000000000014,
          "values": [
            0.41200000000000014
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9570000000000001,
          "std": 0.0,
          "min": 0.9570000000000001,
          "max": 0.9570000000000001,
          "median": 0.9570000000000001,
          "values": [
            0.9570000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.4709,
          "std": 0.0,
          "min": 0.4709,
          "max": 0.4709,
          "median": 0.4709,
          "values": [
            0.4709
          ]
        }
      }
    },
    "ecqa_expert_psychologist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09829710417820166,
          "std": 0.0,
          "min": 0.09829710417820166,
          "max": 0.09829710417820166,
          "median": 0.09829710417820166,
          "values": [
            0.09829710417820166
          ]
        },
        "recall": {
          "mean": 0.4238067413167893,
          "std": 0.0,
          "min": 0.4238067413167893,
          "max": 0.4238067413167893,
          "median": 0.4238067413167893,
          "values": [
            0.4238067413167893
          ]
        },
        "f1_score": {
          "mean": 0.15682926718908757,
          "std": 0.0,
          "min": 0.15682926718908757,
          "max": 0.15682926718908757,
          "median": 0.15682926718908757,
          "values": [
            0.15682926718908757
          ]
        },
        "jaccard": {
          "mean": 0.08559920749288855,
          "std": 0.0,
          "min": 0.08559920749288855,
          "max": 0.08559920749288855,
          "median": 0.08559920749288855,
          "values": [
            0.08559920749288855
          ]
        },
        "semantic_similarity": {
          "mean": 0.548807926774025,
          "std": 0.0,
          "min": 0.548807926774025,
          "max": 0.548807926774025,
          "median": 0.548807926774025,
          "values": [
            0.548807926774025
          ]
        },
        "all_choices_addressed": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "single_paragraph_format": {
          "mean": 0.904,
          "std": 0.0,
          "min": 0.904,
          "max": 0.904,
          "median": 0.904,
          "values": [
            0.904
          ]
        },
        "explanation_depth": {
          "mean": 0.4129,
          "std": 0.0,
          "min": 0.4129,
          "max": 0.4129,
          "median": 0.4129,
          "values": [
            0.4129
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.19209061512850714,
          "std": 0.0,
          "min": 0.19209061512850714,
          "max": 0.19209061512850714,
          "median": 0.19209061512850714,
          "values": [
            0.19209061512850714
          ]
        },
        "recall": {
          "mean": 0.43468391689912195,
          "std": 0.0,
          "min": 0.43468391689912195,
          "max": 0.43468391689912195,
          "median": 0.43468391689912195,
          "values": [
            0.43468391689912195
          ]
        },
        "f1_score": {
          "mean": 0.25993281414743463,
          "std": 0.0,
          "min": 0.25993281414743463,
          "max": 0.25993281414743463,
          "median": 0.25993281414743463,
          "values": [
            0.25993281414743463
          ]
        },
        "jaccard": {
          "mean": 0.1512591168452178,
          "std": 0.0,
          "min": 0.1512591168452178,
          "max": 0.1512591168452178,
          "median": 0.1512591168452178,
          "values": [
            0.1512591168452178
          ]
        },
        "semantic_similarity": {
          "mean": 0.7224323785305024,
          "std": 0.0,
          "min": 0.7224323785305024,
          "max": 0.7224323785305024,
          "median": 0.7224323785305024,
          "values": [
            0.7224323785305024
          ]
        },
        "all_choices_addressed": {
          "mean": 0.8480000000000001,
          "std": 0.0,
          "min": 0.8480000000000001,
          "max": 0.8480000000000001,
          "median": 0.8480000000000001,
          "values": [
            0.8480000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.44260000000000005,
          "std": 0.0,
          "min": 0.44260000000000005,
          "max": 0.44260000000000005,
          "median": 0.44260000000000005,
          "values": [
            0.44260000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10194196490250626,
          "std": 0.0,
          "min": 0.10194196490250626,
          "max": 0.10194196490250626,
          "median": 0.10194196490250626,
          "values": [
            0.10194196490250626
          ]
        },
        "recall": {
          "mean": 0.40929083684962725,
          "std": 0.0,
          "min": 0.40929083684962725,
          "max": 0.40929083684962725,
          "median": 0.40929083684962725,
          "values": [
            0.40929083684962725
          ]
        },
        "f1_score": {
          "mean": 0.16056623958804195,
          "std": 0.0,
          "min": 0.16056623958804195,
          "max": 0.16056623958804195,
          "median": 0.16056623958804195,
          "values": [
            0.16056623958804195
          ]
        },
        "jaccard": {
          "mean": 0.08785645893139245,
          "std": 0.0,
          "min": 0.08785645893139245,
          "max": 0.08785645893139245,
          "median": 0.08785645893139245,
          "values": [
            0.08785645893139245
          ]
        },
        "semantic_similarity": {
          "mean": 0.4261318787932396,
          "std": 0.0,
          "min": 0.4261318787932396,
          "max": 0.4261318787932396,
          "median": 0.4261318787932396,
          "values": [
            0.4261318787932396
          ]
        },
        "all_choices_addressed": {
          "mean": 0.262,
          "std": 0.0,
          "min": 0.262,
          "max": 0.262,
          "median": 0.262,
          "values": [
            0.262
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9840000000000001,
          "std": 0.0,
          "min": 0.9840000000000001,
          "max": 0.9840000000000001,
          "median": 0.9840000000000001,
          "values": [
            0.9840000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.26060000000000005,
          "std": 0.0,
          "min": 0.26060000000000005,
          "max": 0.26060000000000005,
          "median": 0.26060000000000005,
          "values": [
            0.26060000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09517211936859704,
          "std": 0.0,
          "min": 0.09517211936859704,
          "max": 0.09517211936859704,
          "median": 0.09517211936859704,
          "values": [
            0.09517211936859704
          ]
        },
        "recall": {
          "mean": 0.42174494376373195,
          "std": 0.0,
          "min": 0.42174494376373195,
          "max": 0.42174494376373195,
          "median": 0.42174494376373195,
          "values": [
            0.42174494376373195
          ]
        },
        "f1_score": {
          "mean": 0.15250814850198202,
          "std": 0.0,
          "min": 0.15250814850198202,
          "max": 0.15250814850198202,
          "median": 0.15250814850198202,
          "values": [
            0.15250814850198202
          ]
        },
        "jaccard": {
          "mean": 0.08310825444747387,
          "std": 0.0,
          "min": 0.08310825444747387,
          "max": 0.08310825444747387,
          "median": 0.08310825444747387,
          "values": [
            0.08310825444747387
          ]
        },
        "semantic_similarity": {
          "mean": 0.5269784726202488,
          "std": 0.0,
          "min": 0.5269784726202488,
          "max": 0.5269784726202488,
          "median": 0.5269784726202488,
          "values": [
            0.5269784726202488
          ]
        },
        "all_choices_addressed": {
          "mean": 0.3280000000000001,
          "std": 0.0,
          "min": 0.3280000000000001,
          "max": 0.3280000000000001,
          "median": 0.3280000000000001,
          "values": [
            0.3280000000000001
          ]
        },
        "single_paragraph_format": {
          "mean": 0.88,
          "std": 0.0,
          "min": 0.88,
          "max": 0.88,
          "median": 0.88,
          "values": [
            0.88
          ]
        },
        "explanation_depth": {
          "mean": 0.31560000000000005,
          "std": 0.0,
          "min": 0.31560000000000005,
          "max": 0.31560000000000005,
          "median": 0.31560000000000005,
          "values": [
            0.31560000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_perspective_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_perspective__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11599080094633628,
          "std": 0.0,
          "min": 0.11599080094633628,
          "max": 0.11599080094633628,
          "median": 0.11599080094633628,
          "values": [
            0.11599080094633628
          ]
        },
        "recall": {
          "mean": 0.4161228393532694,
          "std": 0.0,
          "min": 0.4161228393532694,
          "max": 0.4161228393532694,
          "median": 0.4161228393532694,
          "values": [
            0.4161228393532694
          ]
        },
        "f1_score": {
          "mean": 0.17809793375638397,
          "std": 0.0,
          "min": 0.17809793375638397,
          "max": 0.17809793375638397,
          "median": 0.17809793375638397,
          "values": [
            0.17809793375638397
          ]
        },
        "jaccard": {
          "mean": 0.09844440164412212,
          "std": 0.0,
          "min": 0.09844440164412212,
          "max": 0.09844440164412212,
          "median": 0.09844440164412212,
          "values": [
            0.09844440164412212
          ]
        },
        "semantic_similarity": {
          "mean": 0.5653007115423679,
          "std": 0.0,
          "min": 0.5653007115423679,
          "max": 0.5653007115423679,
          "median": 0.5653007115423679,
          "values": [
            0.5653007115423679
          ]
        },
        "all_choices_addressed": {
          "mean": 0.43000000000000016,
          "std": 0.0,
          "min": 0.43000000000000016,
          "max": 0.43000000000000016,
          "median": 0.43000000000000016,
          "values": [
            0.43000000000000016
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.35810000000000003,
          "std": 0.0,
          "min": 0.35810000000000003,
          "max": 0.35810000000000003,
          "median": 0.35810000000000003,
          "values": [
            0.35810000000000003
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__hf-qwen2.5-3b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09267020932367967,
          "std": 0.0,
          "min": 0.09267020932367967,
          "max": 0.09267020932367967,
          "median": 0.09267020932367967,
          "values": [
            0.09267020932367967
          ]
        },
        "recall": {
          "mean": 0.39350521618497175,
          "std": 0.0,
          "min": 0.39350521618497175,
          "max": 0.39350521618497175,
          "median": 0.39350521618497175,
          "values": [
            0.39350521618497175
          ]
        },
        "f1_score": {
          "mean": 0.14737658577530466,
          "std": 0.0,
          "min": 0.14737658577530466,
          "max": 0.14737658577530466,
          "median": 0.14737658577530466,
          "values": [
            0.14737658577530466
          ]
        },
        "jaccard": {
          "mean": 0.08005199009215799,
          "std": 0.0,
          "min": 0.08005199009215799,
          "max": 0.08005199009215799,
          "median": 0.08005199009215799,
          "values": [
            0.08005199009215799
          ]
        },
        "semantic_similarity": {
          "mean": 0.4667537838220596,
          "std": 0.0,
          "min": 0.4667537838220596,
          "max": 0.4667537838220596,
          "median": 0.4667537838220596,
          "values": [
            0.4667537838220596
          ]
        },
        "all_choices_addressed": {
          "mean": 0.27599999999999997,
          "std": 0.0,
          "min": 0.27599999999999997,
          "max": 0.27599999999999997,
          "median": 0.27599999999999997,
          "values": [
            0.27599999999999997
          ]
        },
        "single_paragraph_format": {
          "mean": 0.944,
          "std": 0.0,
          "min": 0.944,
          "max": 0.944,
          "median": 0.944,
          "values": [
            0.944
          ]
        },
        "explanation_depth": {
          "mean": 0.3303000000000001,
          "std": 0.0,
          "min": 0.3303000000000001,
          "max": 0.3303000000000001,
          "median": 0.3303000000000001,
          "values": [
            0.3303000000000001
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1192233368466715,
          "std": 0.0,
          "min": 0.1192233368466715,
          "max": 0.1192233368466715,
          "median": 0.1192233368466715,
          "values": [
            0.1192233368466715
          ]
        },
        "recall": {
          "mean": 0.4055359033221203,
          "std": 0.0,
          "min": 0.4055359033221203,
          "max": 0.4055359033221203,
          "median": 0.4055359033221203,
          "values": [
            0.4055359033221203
          ]
        },
        "f1_score": {
          "mean": 0.18065707063999145,
          "std": 0.0,
          "min": 0.18065707063999145,
          "max": 0.18065707063999145,
          "median": 0.18065707063999145,
          "values": [
            0.18065707063999145
          ]
        },
        "jaccard": {
          "mean": 0.10006887092421408,
          "std": 0.0,
          "min": 0.10006887092421408,
          "max": 0.10006887092421408,
          "median": 0.10006887092421408,
          "values": [
            0.10006887092421408
          ]
        },
        "semantic_similarity": {
          "mean": 0.6502753323316575,
          "std": 0.0,
          "min": 0.6502753323316575,
          "max": 0.6502753323316575,
          "median": 0.6502753323316575,
          "values": [
            0.6502753323316575
          ]
        },
        "all_choices_addressed": {
          "mean": 0.30999999999999994,
          "std": 0.0,
          "min": 0.30999999999999994,
          "max": 0.30999999999999994,
          "median": 0.30999999999999994,
          "values": [
            0.30999999999999994
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.4103,
          "std": 0.0,
          "min": 0.4103,
          "max": 0.4103,
          "median": 0.4103,
          "values": [
            0.4103
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10885086322362555,
          "std": 0.0,
          "min": 0.10885086322362555,
          "max": 0.10885086322362555,
          "median": 0.10885086322362555,
          "values": [
            0.10885086322362555
          ]
        },
        "recall": {
          "mean": 0.3867849572008764,
          "std": 0.0,
          "min": 0.3867849572008764,
          "max": 0.3867849572008764,
          "median": 0.3867849572008764,
          "values": [
            0.3867849572008764
          ]
        },
        "f1_score": {
          "mean": 0.16683838327643852,
          "std": 0.0,
          "min": 0.16683838327643852,
          "max": 0.16683838327643852,
          "median": 0.16683838327643852,
          "values": [
            0.16683838327643852
          ]
        },
        "jaccard": {
          "mean": 0.09179937456138797,
          "std": 0.0,
          "min": 0.09179937456138797,
          "max": 0.09179937456138797,
          "median": 0.09179937456138797,
          "values": [
            0.09179937456138797
          ]
        },
        "semantic_similarity": {
          "mean": 0.5047750861942768,
          "std": 0.0,
          "min": 0.5047750861942768,
          "max": 0.5047750861942768,
          "median": 0.5047750861942768,
          "values": [
            0.5047750861942768
          ]
        },
        "all_choices_addressed": {
          "mean": 0.39000000000000007,
          "std": 0.0,
          "min": 0.39000000000000007,
          "max": 0.39000000000000007,
          "median": 0.39000000000000007,
          "values": [
            0.39000000000000007
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.3018,
          "std": 0.0,
          "min": 0.3018,
          "max": 0.3018,
          "median": 0.3018,
          "values": [
            0.3018
          ]
        }
      }
    },
    "ecqa_expert_psychologist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10855459641869158,
          "std": 0.0,
          "min": 0.10855459641869158,
          "max": 0.10855459641869158,
          "median": 0.10855459641869158,
          "values": [
            0.10855459641869158
          ]
        },
        "recall": {
          "mean": 0.37773488076639417,
          "std": 0.0,
          "min": 0.37773488076639417,
          "max": 0.37773488076639417,
          "median": 0.37773488076639417,
          "values": [
            0.37773488076639417
          ]
        },
        "f1_score": {
          "mean": 0.16559036685771542,
          "std": 0.0,
          "min": 0.16559036685771542,
          "max": 0.16559036685771542,
          "median": 0.16559036685771542,
          "values": [
            0.16559036685771542
          ]
        },
        "jaccard": {
          "mean": 0.0909111329082894,
          "std": 0.0,
          "min": 0.0909111329082894,
          "max": 0.0909111329082894,
          "median": 0.0909111329082894,
          "values": [
            0.0909111329082894
          ]
        },
        "semantic_similarity": {
          "mean": 0.5751877236366272,
          "std": 0.0,
          "min": 0.5751877236366272,
          "max": 0.5751877236366272,
          "median": 0.5751877236366272,
          "values": [
            0.5751877236366272
          ]
        },
        "all_choices_addressed": {
          "mean": 0.25199999999999995,
          "std": 0.0,
          "min": 0.25199999999999995,
          "max": 0.25199999999999995,
          "median": 0.25199999999999995,
          "values": [
            0.25199999999999995
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.278,
          "std": 0.0,
          "min": 0.278,
          "max": 0.278,
          "median": 0.278,
          "values": [
            0.278
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.18420417725092644,
          "std": 0.0,
          "min": 0.18420417725092644,
          "max": 0.18420417725092644,
          "median": 0.18420417725092644,
          "values": [
            0.18420417725092644
          ]
        },
        "recall": {
          "mean": 0.46781162855887837,
          "std": 0.0,
          "min": 0.46781162855887837,
          "max": 0.46781162855887837,
          "median": 0.46781162855887837,
          "values": [
            0.46781162855887837
          ]
        },
        "f1_score": {
          "mean": 0.25854952349008764,
          "std": 0.0,
          "min": 0.25854952349008764,
          "max": 0.25854952349008764,
          "median": 0.25854952349008764,
          "values": [
            0.25854952349008764
          ]
        },
        "jaccard": {
          "mean": 0.15019295777727315,
          "std": 0.0,
          "min": 0.15019295777727315,
          "max": 0.15019295777727315,
          "median": 0.15019295777727315,
          "values": [
            0.15019295777727315
          ]
        },
        "semantic_similarity": {
          "mean": 0.7253133010864258,
          "std": 0.0,
          "min": 0.7253133010864258,
          "max": 0.7253133010864258,
          "median": 0.7253133010864258,
          "values": [
            0.7253133010864258
          ]
        },
        "all_choices_addressed": {
          "mean": 0.9399999999999998,
          "std": 0.0,
          "min": 0.9399999999999998,
          "max": 0.9399999999999998,
          "median": 0.9399999999999998,
          "values": [
            0.9399999999999998
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9940000000000001,
          "std": 0.0,
          "min": 0.9940000000000001,
          "max": 0.9940000000000001,
          "median": 0.9940000000000001,
          "values": [
            0.9940000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.4152,
          "std": 0.0,
          "min": 0.4152,
          "max": 0.4152,
          "median": 0.4152,
          "values": [
            0.4152
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10497446321938876,
          "std": 0.0,
          "min": 0.10497446321938876,
          "max": 0.10497446321938876,
          "median": 0.10497446321938876,
          "values": [
            0.10497446321938876
          ]
        },
        "recall": {
          "mean": 0.36126505710264967,
          "std": 0.0,
          "min": 0.36126505710264967,
          "max": 0.36126505710264967,
          "median": 0.36126505710264967,
          "values": [
            0.36126505710264967
          ]
        },
        "f1_score": {
          "mean": 0.1592807578263989,
          "std": 0.0,
          "min": 0.1592807578263989,
          "max": 0.1592807578263989,
          "median": 0.1592807578263989,
          "values": [
            0.1592807578263989
          ]
        },
        "jaccard": {
          "mean": 0.08707964602152211,
          "std": 0.0,
          "min": 0.08707964602152211,
          "max": 0.08707964602152211,
          "median": 0.08707964602152211,
          "values": [
            0.08707964602152211
          ]
        },
        "semantic_similarity": {
          "mean": 0.43809329748153686,
          "std": 0.0,
          "min": 0.43809329748153686,
          "max": 0.43809329748153686,
          "median": 0.43809329748153686,
          "values": [
            0.43809329748153686
          ]
        },
        "all_choices_addressed": {
          "mean": 0.264,
          "std": 0.0,
          "min": 0.264,
          "max": 0.264,
          "median": 0.264,
          "values": [
            0.264
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.22700000000000006,
          "std": 0.0,
          "min": 0.22700000000000006,
          "max": 0.22700000000000006,
          "median": 0.22700000000000006,
          "values": [
            0.22700000000000006
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11350090768163518,
          "std": 0.0,
          "min": 0.11350090768163518,
          "max": 0.11350090768163518,
          "median": 0.11350090768163518,
          "values": [
            0.11350090768163518
          ]
        },
        "recall": {
          "mean": 0.3458292774510594,
          "std": 0.0,
          "min": 0.3458292774510594,
          "max": 0.3458292774510594,
          "median": 0.3458292774510594,
          "values": [
            0.3458292774510594
          ]
        },
        "f1_score": {
          "mean": 0.1644675873681548,
          "std": 0.0,
          "min": 0.1644675873681548,
          "max": 0.1644675873681548,
          "median": 0.1644675873681548,
          "values": [
            0.1644675873681548
          ]
        },
        "jaccard": {
          "mean": 0.09032989211945881,
          "std": 0.0,
          "min": 0.09032989211945881,
          "max": 0.09032989211945881,
          "median": 0.09032989211945881,
          "values": [
            0.09032989211945881
          ]
        },
        "semantic_similarity": {
          "mean": 0.5113372254371643,
          "std": 0.0,
          "min": 0.5113372254371643,
          "max": 0.5113372254371643,
          "median": 0.5113372254371643,
          "values": [
            0.5113372254371643
          ]
        },
        "all_choices_addressed": {
          "mean": 0.23200000000000004,
          "std": 0.0,
          "min": 0.23200000000000004,
          "max": 0.23200000000000004,
          "median": 0.23200000000000004,
          "values": [
            0.23200000000000004
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.1986,
          "std": 0.0,
          "min": 0.1986,
          "max": 0.1986,
          "median": 0.1986,
          "values": [
            0.1986
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3-8b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11232939067723872,
          "std": 0.0,
          "min": 0.11232939067723872,
          "max": 0.11232939067723872,
          "median": 0.11232939067723872,
          "values": [
            0.11232939067723872
          ]
        },
        "recall": {
          "mean": 0.3573079271995745,
          "std": 0.0,
          "min": 0.3573079271995745,
          "max": 0.3573079271995745,
          "median": 0.3573079271995745,
          "values": [
            0.3573079271995745
          ]
        },
        "f1_score": {
          "mean": 0.16502439795099125,
          "std": 0.0,
          "min": 0.16502439795099125,
          "max": 0.16502439795099125,
          "median": 0.16502439795099125,
          "values": [
            0.16502439795099125
          ]
        },
        "jaccard": {
          "mean": 0.0905432225537786,
          "std": 0.0,
          "min": 0.0905432225537786,
          "max": 0.0905432225537786,
          "median": 0.0905432225537786,
          "values": [
            0.0905432225537786
          ]
        },
        "semantic_similarity": {
          "mean": 0.5277336171269417,
          "std": 0.0,
          "min": 0.5277336171269417,
          "max": 0.5277336171269417,
          "median": 0.5277336171269417,
          "values": [
            0.5277336171269417
          ]
        },
        "all_choices_addressed": {
          "mean": 0.25,
          "std": 0.0,
          "min": 0.25,
          "max": 0.25,
          "median": 0.25,
          "values": [
            0.25
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.2998,
          "std": 0.0,
          "min": 0.2998,
          "max": 0.2998,
          "median": 0.2998,
          "values": [
            0.2998
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.12267641724595217,
          "std": 0.0,
          "min": 0.12267641724595217,
          "max": 0.12267641724595217,
          "median": 0.12267641724595217,
          "values": [
            0.12267641724595217
          ]
        },
        "recall": {
          "mean": 0.34810781854230255,
          "std": 0.0,
          "min": 0.34810781854230255,
          "max": 0.34810781854230255,
          "median": 0.34810781854230255,
          "values": [
            0.34810781854230255
          ]
        },
        "f1_score": {
          "mean": 0.17635583715970152,
          "std": 0.0,
          "min": 0.17635583715970152,
          "max": 0.17635583715970152,
          "median": 0.17635583715970152,
          "values": [
            0.17635583715970152
          ]
        },
        "jaccard": {
          "mean": 0.09760465403467201,
          "std": 0.0,
          "min": 0.09760465403467201,
          "max": 0.09760465403467201,
          "median": 0.09760465403467201,
          "values": [
            0.09760465403467201
          ]
        },
        "semantic_similarity": {
          "mean": 0.5743074625730514,
          "std": 0.0,
          "min": 0.5743074625730514,
          "max": 0.5743074625730514,
          "median": 0.5743074625730514,
          "values": [
            0.5743074625730514
          ]
        },
        "all_choices_addressed": {
          "mean": 0.256,
          "std": 0.0,
          "min": 0.256,
          "max": 0.256,
          "median": 0.256,
          "values": [
            0.256
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.4108,
          "std": 0.0,
          "min": 0.4108,
          "max": 0.4108,
          "median": 0.4108,
          "values": [
            0.4108
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11263724615167627,
          "std": 0.0,
          "min": 0.11263724615167627,
          "max": 0.11263724615167627,
          "median": 0.11263724615167627,
          "values": [
            0.11263724615167627
          ]
        },
        "recall": {
          "mean": 0.3708385351724781,
          "std": 0.0,
          "min": 0.3708385351724781,
          "max": 0.3708385351724781,
          "median": 0.3708385351724781,
          "values": [
            0.3708385351724781
          ]
        },
        "f1_score": {
          "mean": 0.1692936614355088,
          "std": 0.0,
          "min": 0.1692936614355088,
          "max": 0.1692936614355088,
          "median": 0.1692936614355088,
          "values": [
            0.1692936614355088
          ]
        },
        "jaccard": {
          "mean": 0.09312802898286413,
          "std": 0.0,
          "min": 0.09312802898286413,
          "max": 0.09312802898286413,
          "median": 0.09312802898286413,
          "values": [
            0.09312802898286413
          ]
        },
        "semantic_similarity": {
          "mean": 0.4907579793035984,
          "std": 0.0,
          "min": 0.4907579793035984,
          "max": 0.4907579793035984,
          "median": 0.4907579793035984,
          "values": [
            0.4907579793035984
          ]
        },
        "all_choices_addressed": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.338,
          "std": 0.0,
          "min": 0.338,
          "max": 0.338,
          "median": 0.338,
          "values": [
            0.338
          ]
        }
      }
    },
    "ecqa_expert_psychologist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11468414071289271,
          "std": 0.0,
          "min": 0.11468414071289271,
          "max": 0.11468414071289271,
          "median": 0.11468414071289271,
          "values": [
            0.11468414071289271
          ]
        },
        "recall": {
          "mean": 0.35153406303252865,
          "std": 0.0,
          "min": 0.35153406303252865,
          "max": 0.35153406303252865,
          "median": 0.35153406303252865,
          "values": [
            0.35153406303252865
          ]
        },
        "f1_score": {
          "mean": 0.168990563731321,
          "std": 0.0,
          "min": 0.168990563731321,
          "max": 0.168990563731321,
          "median": 0.168990563731321,
          "values": [
            0.168990563731321
          ]
        },
        "jaccard": {
          "mean": 0.09307741938536879,
          "std": 0.0,
          "min": 0.09307741938536879,
          "max": 0.09307741938536879,
          "median": 0.09307741938536879,
          "values": [
            0.09307741938536879
          ]
        },
        "semantic_similarity": {
          "mean": 0.5240909847617149,
          "std": 0.0,
          "min": 0.5240909847617149,
          "max": 0.5240909847617149,
          "median": 0.5240909847617149,
          "values": [
            0.5240909847617149
          ]
        },
        "all_choices_addressed": {
          "mean": 0.2,
          "std": 0.0,
          "min": 0.2,
          "max": 0.2,
          "median": 0.2,
          "values": [
            0.2
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.25370000000000004,
          "std": 0.0,
          "min": 0.25370000000000004,
          "max": 0.25370000000000004,
          "median": 0.25370000000000004,
          "values": [
            0.25370000000000004
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1896884999747779,
          "std": 0.0,
          "min": 0.1896884999747779,
          "max": 0.1896884999747779,
          "median": 0.1896884999747779,
          "values": [
            0.1896884999747779
          ]
        },
        "recall": {
          "mean": 0.43879755377743423,
          "std": 0.0,
          "min": 0.43879755377743423,
          "max": 0.43879755377743423,
          "median": 0.43879755377743423,
          "values": [
            0.43879755377743423
          ]
        },
        "f1_score": {
          "mean": 0.25720573009007575,
          "std": 0.0,
          "min": 0.25720573009007575,
          "max": 0.25720573009007575,
          "median": 0.25720573009007575,
          "values": [
            0.25720573009007575
          ]
        },
        "jaccard": {
          "mean": 0.14951504201669757,
          "std": 0.0,
          "min": 0.14951504201669757,
          "max": 0.14951504201669757,
          "median": 0.14951504201669757,
          "values": [
            0.14951504201669757
          ]
        },
        "semantic_similarity": {
          "mean": 0.710413635969162,
          "std": 0.0,
          "min": 0.710413635969162,
          "max": 0.710413635969162,
          "median": 0.710413635969162,
          "values": [
            0.710413635969162
          ]
        },
        "all_choices_addressed": {
          "mean": 0.674,
          "std": 0.0,
          "min": 0.674,
          "max": 0.674,
          "median": 0.674,
          "values": [
            0.674
          ]
        },
        "single_paragraph_format": {
          "mean": 0.915,
          "std": 0.0,
          "min": 0.915,
          "max": 0.915,
          "median": 0.915,
          "values": [
            0.915
          ]
        },
        "explanation_depth": {
          "mean": 0.39519999999999994,
          "std": 0.0,
          "min": 0.39519999999999994,
          "max": 0.39519999999999994,
          "median": 0.39519999999999994,
          "values": [
            0.39519999999999994
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11732151619218938,
          "std": 0.0,
          "min": 0.11732151619218938,
          "max": 0.11732151619218938,
          "median": 0.11732151619218938,
          "values": [
            0.11732151619218938
          ]
        },
        "recall": {
          "mean": 0.3420798601238221,
          "std": 0.0,
          "min": 0.3420798601238221,
          "max": 0.3420798601238221,
          "median": 0.3420798601238221,
          "values": [
            0.3420798601238221
          ]
        },
        "f1_score": {
          "mean": 0.16813189049462218,
          "std": 0.0,
          "min": 0.16813189049462218,
          "max": 0.16813189049462218,
          "median": 0.16813189049462218,
          "values": [
            0.16813189049462218
          ]
        },
        "jaccard": {
          "mean": 0.09268273828787235,
          "std": 0.0,
          "min": 0.09268273828787235,
          "max": 0.09268273828787235,
          "median": 0.09268273828787235,
          "values": [
            0.09268273828787235
          ]
        },
        "semantic_similarity": {
          "mean": 0.4277626851759851,
          "std": 0.0,
          "min": 0.4277626851759851,
          "max": 0.4277626851759851,
          "median": 0.4277626851759851,
          "values": [
            0.4277626851759851
          ]
        },
        "all_choices_addressed": {
          "mean": 0.256,
          "std": 0.0,
          "min": 0.256,
          "max": 0.256,
          "median": 0.256,
          "values": [
            0.256
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.22880000000000003,
          "std": 0.0,
          "min": 0.22880000000000003,
          "max": 0.22880000000000003,
          "median": 0.22880000000000003,
          "values": [
            0.22880000000000003
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11887180013745242,
          "std": 0.0,
          "min": 0.11887180013745242,
          "max": 0.11887180013745242,
          "median": 0.11887180013745242,
          "values": [
            0.11887180013745242
          ]
        },
        "recall": {
          "mean": 0.3577549998994147,
          "std": 0.0,
          "min": 0.3577549998994147,
          "max": 0.3577549998994147,
          "median": 0.3577549998994147,
          "values": [
            0.3577549998994147
          ]
        },
        "f1_score": {
          "mean": 0.17490467140552698,
          "std": 0.0,
          "min": 0.17490467140552698,
          "max": 0.17490467140552698,
          "median": 0.17490467140552698,
          "values": [
            0.17490467140552698
          ]
        },
        "jaccard": {
          "mean": 0.09646742000800676,
          "std": 0.0,
          "min": 0.09646742000800676,
          "max": 0.09646742000800676,
          "median": 0.09646742000800676,
          "values": [
            0.09646742000800676
          ]
        },
        "semantic_similarity": {
          "mean": 0.5211908048391343,
          "std": 0.0,
          "min": 0.5211908048391343,
          "max": 0.5211908048391343,
          "median": 0.5211908048391343,
          "values": [
            0.5211908048391343
          ]
        },
        "all_choices_addressed": {
          "mean": 0.264,
          "std": 0.0,
          "min": 0.264,
          "max": 0.264,
          "median": 0.264,
          "values": [
            0.264
          ]
        },
        "single_paragraph_format": {
          "mean": 0.997,
          "std": 0.0,
          "min": 0.997,
          "max": 0.997,
          "median": 0.997,
          "values": [
            0.997
          ]
        },
        "explanation_depth": {
          "mean": 0.23440000000000008,
          "std": 0.0,
          "min": 0.23440000000000008,
          "max": 0.23440000000000008,
          "median": 0.23440000000000008,
          "values": [
            0.23440000000000008
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.2-1b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1144162082803993,
          "std": 0.0,
          "min": 0.1144162082803993,
          "max": 0.1144162082803993,
          "median": 0.1144162082803993,
          "values": [
            0.1144162082803993
          ]
        },
        "recall": {
          "mean": 0.3387622980194823,
          "std": 0.0,
          "min": 0.3387622980194823,
          "max": 0.3387622980194823,
          "median": 0.3387622980194823,
          "values": [
            0.3387622980194823
          ]
        },
        "f1_score": {
          "mean": 0.16756314639097386,
          "std": 0.0,
          "min": 0.16756314639097386,
          "max": 0.16756314639097386,
          "median": 0.16756314639097386,
          "values": [
            0.16756314639097386
          ]
        },
        "jaccard": {
          "mean": 0.09209594482762284,
          "std": 0.0,
          "min": 0.09209594482762284,
          "max": 0.09209594482762284,
          "median": 0.09209594482762284,
          "values": [
            0.09209594482762284
          ]
        },
        "semantic_similarity": {
          "mean": 0.4970189430564642,
          "std": 0.0,
          "min": 0.4970189430564642,
          "max": 0.4970189430564642,
          "median": 0.4970189430564642,
          "values": [
            0.4970189430564642
          ]
        },
        "all_choices_addressed": {
          "mean": 0.27,
          "std": 0.0,
          "min": 0.27,
          "max": 0.27,
          "median": 0.27,
          "values": [
            0.27
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.26360000000000006,
          "std": 0.0,
          "min": 0.26360000000000006,
          "max": 0.26360000000000006,
          "median": 0.26360000000000006,
          "values": [
            0.26360000000000006
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10190939493255025,
          "std": 0.0,
          "min": 0.10190939493255025,
          "max": 0.10190939493255025,
          "median": 0.10190939493255025,
          "values": [
            0.10190939493255025
          ]
        },
        "recall": {
          "mean": 0.3924597192007664,
          "std": 0.0,
          "min": 0.3924597192007664,
          "max": 0.3924597192007664,
          "median": 0.3924597192007664,
          "values": [
            0.3924597192007664
          ]
        },
        "f1_score": {
          "mean": 0.1590321106210627,
          "std": 0.0,
          "min": 0.1590321106210627,
          "max": 0.1590321106210627,
          "median": 0.1590321106210627,
          "values": [
            0.1590321106210627
          ]
        },
        "jaccard": {
          "mean": 0.08694414786914884,
          "std": 0.0,
          "min": 0.08694414786914884,
          "max": 0.08694414786914884,
          "median": 0.08694414786914884,
          "values": [
            0.08694414786914884
          ]
        },
        "semantic_similarity": {
          "mean": 0.6231924968957901,
          "std": 0.0,
          "min": 0.6231924968957901,
          "max": 0.6231924968957901,
          "median": 0.6231924968957901,
          "values": [
            0.6231924968957901
          ]
        },
        "all_choices_addressed": {
          "mean": 0.268,
          "std": 0.0,
          "min": 0.268,
          "max": 0.268,
          "median": 0.268,
          "values": [
            0.268
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9940000000000001,
          "std": 0.0,
          "min": 0.9940000000000001,
          "max": 0.9940000000000001,
          "median": 0.9940000000000001,
          "values": [
            0.9940000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.3849,
          "std": 0.0,
          "min": 0.3849,
          "max": 0.3849,
          "median": 0.3849,
          "values": [
            0.3849
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09863518069947419,
          "std": 0.0,
          "min": 0.09863518069947419,
          "max": 0.09863518069947419,
          "median": 0.09863518069947419,
          "values": [
            0.09863518069947419
          ]
        },
        "recall": {
          "mean": 0.3752638393508984,
          "std": 0.0,
          "min": 0.3752638393508984,
          "max": 0.3752638393508984,
          "median": 0.3752638393508984,
          "values": [
            0.3752638393508984
          ]
        },
        "f1_score": {
          "mean": 0.15344931376506538,
          "std": 0.0,
          "min": 0.15344931376506538,
          "max": 0.15344931376506538,
          "median": 0.15344931376506538,
          "values": [
            0.15344931376506538
          ]
        },
        "jaccard": {
          "mean": 0.08360831973868428,
          "std": 0.0,
          "min": 0.08360831973868428,
          "max": 0.08360831973868428,
          "median": 0.08360831973868428,
          "values": [
            0.08360831973868428
          ]
        },
        "semantic_similarity": {
          "mean": 0.445855346173048,
          "std": 0.0,
          "min": 0.445855346173048,
          "max": 0.445855346173048,
          "median": 0.445855346173048,
          "values": [
            0.445855346173048
          ]
        },
        "all_choices_addressed": {
          "mean": 0.31600000000000006,
          "std": 0.0,
          "min": 0.31600000000000006,
          "max": 0.31600000000000006,
          "median": 0.31600000000000006,
          "values": [
            0.31600000000000006
          ]
        },
        "single_paragraph_format": {
          "mean": 0.982,
          "std": 0.0,
          "min": 0.982,
          "max": 0.982,
          "median": 0.982,
          "values": [
            0.982
          ]
        },
        "explanation_depth": {
          "mean": 0.33760000000000007,
          "std": 0.0,
          "min": 0.33760000000000007,
          "max": 0.33760000000000007,
          "median": 0.33760000000000007,
          "values": [
            0.33760000000000007
          ]
        }
      }
    },
    "ecqa_expert_psychologist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0999787479998068,
          "std": 0.0,
          "min": 0.0999787479998068,
          "max": 0.0999787479998068,
          "median": 0.0999787479998068,
          "values": [
            0.0999787479998068
          ]
        },
        "recall": {
          "mean": 0.376034351919289,
          "std": 0.0,
          "min": 0.376034351919289,
          "max": 0.376034351919289,
          "median": 0.376034351919289,
          "values": [
            0.376034351919289
          ]
        },
        "f1_score": {
          "mean": 0.1552280589481312,
          "std": 0.0,
          "min": 0.1552280589481312,
          "max": 0.1552280589481312,
          "median": 0.1552280589481312,
          "values": [
            0.1552280589481312
          ]
        },
        "jaccard": {
          "mean": 0.08461068836630405,
          "std": 0.0,
          "min": 0.08461068836630405,
          "max": 0.08461068836630405,
          "median": 0.08461068836630405,
          "values": [
            0.08461068836630405
          ]
        },
        "semantic_similarity": {
          "mean": 0.5808199259638787,
          "std": 0.0,
          "min": 0.5808199259638787,
          "max": 0.5808199259638787,
          "median": 0.5808199259638787,
          "values": [
            0.5808199259638787
          ]
        },
        "all_choices_addressed": {
          "mean": 0.29600000000000004,
          "std": 0.0,
          "min": 0.29600000000000004,
          "max": 0.29600000000000004,
          "median": 0.29600000000000004,
          "values": [
            0.29600000000000004
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9940000000000001,
          "std": 0.0,
          "min": 0.9940000000000001,
          "max": 0.9940000000000001,
          "median": 0.9940000000000001,
          "values": [
            0.9940000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.317,
          "std": 0.0,
          "min": 0.317,
          "max": 0.317,
          "median": 0.317,
          "values": [
            0.317
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1628305079509147,
          "std": 0.0,
          "min": 0.1628305079509147,
          "max": 0.1628305079509147,
          "median": 0.1628305079509147,
          "values": [
            0.1628305079509147
          ]
        },
        "recall": {
          "mean": 0.502578757763266,
          "std": 0.0,
          "min": 0.502578757763266,
          "max": 0.502578757763266,
          "median": 0.502578757763266,
          "values": [
            0.502578757763266
          ]
        },
        "f1_score": {
          "mean": 0.24144336041957373,
          "std": 0.0,
          "min": 0.24144336041957373,
          "max": 0.24144336041957373,
          "median": 0.24144336041957373,
          "values": [
            0.24144336041957373
          ]
        },
        "jaccard": {
          "mean": 0.13860822528341799,
          "std": 0.0,
          "min": 0.13860822528341799,
          "max": 0.13860822528341799,
          "median": 0.13860822528341799,
          "values": [
            0.13860822528341799
          ]
        },
        "semantic_similarity": {
          "mean": 0.7214969116449356,
          "std": 0.0,
          "min": 0.7214969116449356,
          "max": 0.7214969116449356,
          "median": 0.7214969116449356,
          "values": [
            0.7214969116449356
          ]
        },
        "all_choices_addressed": {
          "mean": 0.98,
          "std": 0.0,
          "min": 0.98,
          "max": 0.98,
          "median": 0.98,
          "values": [
            0.98
          ]
        },
        "single_paragraph_format": {
          "mean": 0.861,
          "std": 0.0,
          "min": 0.861,
          "max": 0.861,
          "median": 0.861,
          "values": [
            0.861
          ]
        },
        "explanation_depth": {
          "mean": 0.5681,
          "std": 0.0,
          "min": 0.5681,
          "max": 0.5681,
          "median": 0.5681,
          "values": [
            0.5681
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0938173781271812,
          "std": 0.0,
          "min": 0.0938173781271812,
          "max": 0.0938173781271812,
          "median": 0.0938173781271812,
          "values": [
            0.0938173781271812
          ]
        },
        "recall": {
          "mean": 0.3646792588837652,
          "std": 0.0,
          "min": 0.3646792588837652,
          "max": 0.3646792588837652,
          "median": 0.3646792588837652,
          "values": [
            0.3646792588837652
          ]
        },
        "f1_score": {
          "mean": 0.14688288237361224,
          "std": 0.0,
          "min": 0.14688288237361224,
          "max": 0.14688288237361224,
          "median": 0.14688288237361224,
          "values": [
            0.14688288237361224
          ]
        },
        "jaccard": {
          "mean": 0.07967973503882565,
          "std": 0.0,
          "min": 0.07967973503882565,
          "max": 0.07967973503882565,
          "median": 0.07967973503882565,
          "values": [
            0.07967973503882565
          ]
        },
        "semantic_similarity": {
          "mean": 0.4440675215423107,
          "std": 0.0,
          "min": 0.4440675215423107,
          "max": 0.4440675215423107,
          "median": 0.4440675215423107,
          "values": [
            0.4440675215423107
          ]
        },
        "all_choices_addressed": {
          "mean": 0.244,
          "std": 0.0,
          "min": 0.244,
          "max": 0.244,
          "median": 0.244,
          "values": [
            0.244
          ]
        },
        "single_paragraph_format": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "explanation_depth": {
          "mean": 0.21980000000000005,
          "std": 0.0,
          "min": 0.21980000000000005,
          "max": 0.21980000000000005,
          "median": 0.21980000000000005,
          "values": [
            0.21980000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09313842837060643,
          "std": 0.0,
          "min": 0.09313842837060643,
          "max": 0.09313842837060643,
          "median": 0.09313842837060643,
          "values": [
            0.09313842837060643
          ]
        },
        "recall": {
          "mean": 0.36144394533247004,
          "std": 0.0,
          "min": 0.36144394533247004,
          "max": 0.36144394533247004,
          "median": 0.36144394533247004,
          "values": [
            0.36144394533247004
          ]
        },
        "f1_score": {
          "mean": 0.14564836214695653,
          "std": 0.0,
          "min": 0.14564836214695653,
          "max": 0.14564836214695653,
          "median": 0.14564836214695653,
          "values": [
            0.14564836214695653
          ]
        },
        "jaccard": {
          "mean": 0.07902866750474985,
          "std": 0.0,
          "min": 0.07902866750474985,
          "max": 0.07902866750474985,
          "median": 0.07902866750474985,
          "values": [
            0.07902866750474985
          ]
        },
        "semantic_similarity": {
          "mean": 0.530372180789709,
          "std": 0.0,
          "min": 0.530372180789709,
          "max": 0.530372180789709,
          "median": 0.530372180789709,
          "values": [
            0.530372180789709
          ]
        },
        "all_choices_addressed": {
          "mean": 0.236,
          "std": 0.0,
          "min": 0.236,
          "max": 0.236,
          "median": 0.236,
          "values": [
            0.236
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9249999999999998,
          "std": 0.0,
          "min": 0.9249999999999998,
          "max": 0.9249999999999998,
          "median": 0.9249999999999998,
          "values": [
            0.9249999999999998
          ]
        },
        "explanation_depth": {
          "mean": 0.21860000000000002,
          "std": 0.0,
          "min": 0.21860000000000002,
          "max": 0.21860000000000002,
          "median": 0.21860000000000002,
          "values": [
            0.21860000000000002
          ]
        }
      }
    },
    "ecqa_stakeholder_perspective_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_perspective__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10680542256256137,
          "std": 0.0,
          "min": 0.10680542256256137,
          "max": 0.10680542256256137,
          "median": 0.10680542256256137,
          "values": [
            0.10680542256256137
          ]
        },
        "recall": {
          "mean": 0.376333911124093,
          "std": 0.0,
          "min": 0.376333911124093,
          "max": 0.376333911124093,
          "median": 0.376333911124093,
          "values": [
            0.376333911124093
          ]
        },
        "f1_score": {
          "mean": 0.16336908081235735,
          "std": 0.0,
          "min": 0.16336908081235735,
          "max": 0.16336908081235735,
          "median": 0.16336908081235735,
          "values": [
            0.16336908081235735
          ]
        },
        "jaccard": {
          "mean": 0.08971715709668075,
          "std": 0.0,
          "min": 0.08971715709668075,
          "max": 0.08971715709668075,
          "median": 0.08971715709668075,
          "values": [
            0.08971715709668075
          ]
        },
        "semantic_similarity": {
          "mean": 0.572981809079647,
          "std": 0.0,
          "min": 0.572981809079647,
          "max": 0.572981809079647,
          "median": 0.572981809079647,
          "values": [
            0.572981809079647
          ]
        },
        "all_choices_addressed": {
          "mean": 0.262,
          "std": 0.0,
          "min": 0.262,
          "max": 0.262,
          "median": 0.262,
          "values": [
            0.262
          ]
        },
        "single_paragraph_format": {
          "mean": 0.96,
          "std": 0.0,
          "min": 0.96,
          "max": 0.96,
          "median": 0.96,
          "values": [
            0.96
          ]
        },
        "explanation_depth": {
          "mean": 0.324,
          "std": 0.0,
          "min": 0.324,
          "max": 0.324,
          "median": 0.324,
          "values": [
            0.324
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09667820560075438,
          "std": 0.0,
          "min": 0.09667820560075438,
          "max": 0.09667820560075438,
          "median": 0.09667820560075438,
          "values": [
            0.09667820560075438
          ]
        },
        "recall": {
          "mean": 0.36825110682821893,
          "std": 0.0,
          "min": 0.36825110682821893,
          "max": 0.36825110682821893,
          "median": 0.36825110682821893,
          "values": [
            0.36825110682821893
          ]
        },
        "f1_score": {
          "mean": 0.15046952434153799,
          "std": 0.0,
          "min": 0.15046952434153799,
          "max": 0.15046952434153799,
          "median": 0.15046952434153799,
          "values": [
            0.15046952434153799
          ]
        },
        "jaccard": {
          "mean": 0.08188738578831059,
          "std": 0.0,
          "min": 0.08188738578831059,
          "max": 0.08188738578831059,
          "median": 0.08188738578831059,
          "values": [
            0.08188738578831059
          ]
        },
        "semantic_similarity": {
          "mean": 0.5192889314889908,
          "std": 0.0,
          "min": 0.5192889314889908,
          "max": 0.5192889314889908,
          "median": 0.5192889314889908,
          "values": [
            0.5192889314889908
          ]
        },
        "all_choices_addressed": {
          "mean": 0.262,
          "std": 0.0,
          "min": 0.262,
          "max": 0.262,
          "median": 0.262,
          "values": [
            0.262
          ]
        },
        "single_paragraph_format": {
          "mean": 0.9490000000000001,
          "std": 0.0,
          "min": 0.9490000000000001,
          "max": 0.9490000000000001,
          "median": 0.9490000000000001,
          "values": [
            0.9490000000000001
          ]
        },
        "explanation_depth": {
          "mean": 0.28980000000000006,
          "std": 0.0,
          "min": 0.28980000000000006,
          "max": 0.28980000000000006,
          "median": 0.28980000000000006,
          "values": [
            0.28980000000000006
          ]
        }
      }
    },
    "ecqa_expert_anthropologist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_anthropologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10796416233006301,
          "std": 0.0,
          "min": 0.10796416233006301,
          "max": 0.10796416233006301,
          "median": 0.10796416233006301,
          "values": [
            0.10796416233006301
          ]
        },
        "recall": {
          "mean": 0.37469508821511915,
          "std": 0.0,
          "min": 0.37469508821511915,
          "max": 0.37469508821511915,
          "median": 0.37469508821511915,
          "values": [
            0.37469508821511915
          ]
        },
        "f1_score": {
          "mean": 0.16456267846367328,
          "std": 0.0,
          "min": 0.16456267846367328,
          "max": 0.16456267846367328,
          "median": 0.16456267846367328,
          "values": [
            0.16456267846367328
          ]
        },
        "jaccard": {
          "mean": 0.09031078948217973,
          "std": 0.0,
          "min": 0.09031078948217973,
          "max": 0.09031078948217973,
          "median": 0.09031078948217973,
          "values": [
            0.09031078948217973
          ]
        },
        "semantic_similarity": {
          "mean": 0.5852298989892006,
          "std": 0.0,
          "min": 0.5852298989892006,
          "max": 0.5852298989892006,
          "median": 0.5852298989892006,
          "values": [
            0.5852298989892006
          ]
        },
        "all_choices_addressed": {
          "mean": 0.212,
          "std": 0.0,
          "min": 0.212,
          "max": 0.212,
          "median": 0.212,
          "values": [
            0.212
          ]
        },
        "single_paragraph_format": {
          "mean": 0.3740000000000002,
          "std": 0.0,
          "min": 0.3740000000000002,
          "max": 0.3740000000000002,
          "median": 0.3740000000000002,
          "values": [
            0.3740000000000002
          ]
        },
        "explanation_depth": {
          "mean": 0.431,
          "std": 0.0,
          "min": 0.431,
          "max": 0.431,
          "median": 0.431,
          "values": [
            0.431
          ]
        }
      }
    },
    "ecqa_expert_behavioral_economist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_behavioral_economist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10595046933584781,
          "std": 0.0,
          "min": 0.10595046933584781,
          "max": 0.10595046933584781,
          "median": 0.10595046933584781,
          "values": [
            0.10595046933584781
          ]
        },
        "recall": {
          "mean": 0.3831563819258223,
          "std": 0.0,
          "min": 0.3831563819258223,
          "max": 0.3831563819258223,
          "median": 0.3831563819258223,
          "values": [
            0.3831563819258223
          ]
        },
        "f1_score": {
          "mean": 0.16295950670669915,
          "std": 0.0,
          "min": 0.16295950670669915,
          "max": 0.16295950670669915,
          "median": 0.16295950670669915,
          "values": [
            0.16295950670669915
          ]
        },
        "jaccard": {
          "mean": 0.08929612684782043,
          "std": 0.0,
          "min": 0.08929612684782043,
          "max": 0.08929612684782043,
          "median": 0.08929612684782043,
          "values": [
            0.08929612684782043
          ]
        },
        "semantic_similarity": {
          "mean": 0.49143902480602264,
          "std": 0.0,
          "min": 0.49143902480602264,
          "max": 0.49143902480602264,
          "median": 0.49143902480602264,
          "values": [
            0.49143902480602264
          ]
        },
        "all_choices_addressed": {
          "mean": 0.28800000000000003,
          "std": 0.0,
          "min": 0.28800000000000003,
          "max": 0.28800000000000003,
          "median": 0.28800000000000003,
          "values": [
            0.28800000000000003
          ]
        },
        "single_paragraph_format": {
          "mean": 0.205,
          "std": 0.0,
          "min": 0.205,
          "max": 0.205,
          "median": 0.205,
          "values": [
            0.205
          ]
        },
        "explanation_depth": {
          "mean": 0.37829999999999997,
          "std": 0.0,
          "min": 0.37829999999999997,
          "max": 0.37829999999999997,
          "median": 0.37829999999999997,
          "values": [
            0.37829999999999997
          ]
        }
      }
    },
    "ecqa_expert_psychologist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_expert_psychologist__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10603403521361136,
          "std": 0.0,
          "min": 0.10603403521361136,
          "max": 0.10603403521361136,
          "median": 0.10603403521361136,
          "values": [
            0.10603403521361136
          ]
        },
        "recall": {
          "mean": 0.3727890177015011,
          "std": 0.0,
          "min": 0.3727890177015011,
          "max": 0.3727890177015011,
          "median": 0.3727890177015011,
          "values": [
            0.3727890177015011
          ]
        },
        "f1_score": {
          "mean": 0.1619262734177716,
          "std": 0.0,
          "min": 0.1619262734177716,
          "max": 0.1619262734177716,
          "median": 0.1619262734177716,
          "values": [
            0.1619262734177716
          ]
        },
        "jaccard": {
          "mean": 0.08866048842313738,
          "std": 0.0,
          "min": 0.08866048842313738,
          "max": 0.08866048842313738,
          "median": 0.08866048842313738,
          "values": [
            0.08866048842313738
          ]
        },
        "semantic_similarity": {
          "mean": 0.5422705674171447,
          "std": 0.0,
          "min": 0.5422705674171447,
          "max": 0.5422705674171447,
          "median": 0.5422705674171447,
          "values": [
            0.5422705674171447
          ]
        },
        "all_choices_addressed": {
          "mean": 0.27,
          "std": 0.0,
          "min": 0.27,
          "max": 0.27,
          "median": 0.27,
          "values": [
            0.27
          ]
        },
        "single_paragraph_format": {
          "mean": 0.267,
          "std": 0.0,
          "min": 0.267,
          "max": 0.267,
          "median": 0.267,
          "values": [
            0.267
          ]
        },
        "explanation_depth": {
          "mean": 0.32339999999999997,
          "std": 0.0,
          "min": 0.32339999999999997,
          "max": 0.32339999999999997,
          "median": 0.32339999999999997,
          "values": [
            0.32339999999999997
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1773530384609687,
          "std": 0.0,
          "min": 0.1773530384609687,
          "max": 0.1773530384609687,
          "median": 0.1773530384609687,
          "values": [
            0.1773530384609687
          ]
        },
        "recall": {
          "mean": 0.4797364105036326,
          "std": 0.0,
          "min": 0.4797364105036326,
          "max": 0.4797364105036326,
          "median": 0.4797364105036326,
          "values": [
            0.4797364105036326
          ]
        },
        "f1_score": {
          "mean": 0.25292745905479747,
          "std": 0.0,
          "min": 0.25292745905479747,
          "max": 0.25292745905479747,
          "median": 0.25292745905479747,
          "values": [
            0.25292745905479747
          ]
        },
        "jaccard": {
          "mean": 0.146148187211815,
          "std": 0.0,
          "min": 0.146148187211815,
          "max": 0.146148187211815,
          "median": 0.146148187211815,
          "values": [
            0.146148187211815
          ]
        },
        "semantic_similarity": {
          "mean": 0.7190728631615638,
          "std": 0.0,
          "min": 0.7190728631615638,
          "max": 0.7190728631615638,
          "median": 0.7190728631615638,
          "values": [
            0.7190728631615638
          ]
        },
        "all_choices_addressed": {
          "mean": 0.866,
          "std": 0.0,
          "min": 0.866,
          "max": 0.866,
          "median": 0.866,
          "values": [
            0.866
          ]
        },
        "single_paragraph_format": {
          "mean": 0.561,
          "std": 0.0,
          "min": 0.561,
          "max": 0.561,
          "median": 0.561,
          "values": [
            0.561
          ]
        },
        "explanation_depth": {
          "mean": 0.49770000000000003,
          "std": 0.0,
          "min": 0.49770000000000003,
          "max": 0.49770000000000003,
          "median": 0.49770000000000003,
          "values": [
            0.49770000000000003
          ]
        }
      }
    },
    "ecqa_stakeholder_disability_advocate_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_disability_advocate__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09663565079795029,
          "std": 0.0,
          "min": 0.09663565079795029,
          "max": 0.09663565079795029,
          "median": 0.09663565079795029,
          "values": [
            0.09663565079795029
          ]
        },
        "recall": {
          "mean": 0.36062526754018537,
          "std": 0.0,
          "min": 0.36062526754018537,
          "max": 0.36062526754018537,
          "median": 0.36062526754018537,
          "values": [
            0.36062526754018537
          ]
        },
        "f1_score": {
          "mean": 0.14971180719485916,
          "std": 0.0,
          "min": 0.14971180719485916,
          "max": 0.14971180719485916,
          "median": 0.14971180719485916,
          "values": [
            0.14971180719485916
          ]
        },
        "jaccard": {
          "mean": 0.08139133737419964,
          "std": 0.0,
          "min": 0.08139133737419964,
          "max": 0.08139133737419964,
          "median": 0.08139133737419964,
          "values": [
            0.08139133737419964
          ]
        },
        "semantic_similarity": {
          "mean": 0.42134753093123434,
          "std": 0.0,
          "min": 0.42134753093123434,
          "max": 0.42134753093123434,
          "median": 0.42134753093123434,
          "values": [
            0.42134753093123434
          ]
        },
        "all_choices_addressed": {
          "mean": 0.20799999999999996,
          "std": 0.0,
          "min": 0.20799999999999996,
          "max": 0.20799999999999996,
          "median": 0.20799999999999996,
          "values": [
            0.20799999999999996
          ]
        },
        "single_paragraph_format": {
          "mean": 0.21599999999999997,
          "std": 0.0,
          "min": 0.21599999999999997,
          "max": 0.21599999999999997,
          "median": 0.21599999999999997,
          "values": [
            0.21599999999999997
          ]
        },
        "explanation_depth": {
          "mean": 0.26720000000000005,
          "std": 0.0,
          "min": 0.26720000000000005,
          "max": 0.26720000000000005,
          "median": 0.26720000000000005,
          "values": [
            0.26720000000000005
          ]
        }
      }
    },
    "ecqa_stakeholder_parent_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_parent__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09606231894344798,
          "std": 0.0,
          "min": 0.09606231894344798,
          "max": 0.09606231894344798,
          "median": 0.09606231894344798,
          "values": [
            0.09606231894344798
          ]
        },
        "recall": {
          "mean": 0.3615476046966404,
          "std": 0.0,
          "min": 0.3615476046966404,
          "max": 0.3615476046966404,
          "median": 0.3615476046966404,
          "values": [
            0.3615476046966404
          ]
        },
        "f1_score": {
          "mean": 0.14924523587796787,
          "std": 0.0,
          "min": 0.14924523587796787,
          "max": 0.14924523587796787,
          "median": 0.14924523587796787,
          "values": [
            0.14924523587796787
          ]
        },
        "jaccard": {
          "mean": 0.08116362604542336,
          "std": 0.0,
          "min": 0.08116362604542336,
          "max": 0.08116362604542336,
          "median": 0.08116362604542336,
          "values": [
            0.08116362604542336
          ]
        },
        "semantic_similarity": {
          "mean": 0.5147403443604708,
          "std": 0.0,
          "min": 0.5147403443604708,
          "max": 0.5147403443604708,
          "median": 0.5147403443604708,
          "values": [
            0.5147403443604708
          ]
        },
        "all_choices_addressed": {
          "mean": 0.24600000000000002,
          "std": 0.0,
          "min": 0.24600000000000002,
          "max": 0.24600000000000002,
          "median": 0.24600000000000002,
          "values": [
            0.24600000000000002
          ]
        },
        "single_paragraph_format": {
          "mean": 0.22399999999999995,
          "std": 0.0,
          "min": 0.22399999999999995,
          "max": 0.22399999999999995,
          "median": 0.22399999999999995,
          "values": [
            0.22399999999999995
          ]
        },
        "explanation_depth": {
          "mean": 0.26310000000000006,
          "std": 0.0,
          "min": 0.26310000000000006,
          "max": 0.26310000000000006,
          "median": 0.26310000000000006,
          "values": [
            0.26310000000000006
          ]
        }
      }
    },
    "ecqa_stakeholder_perspective_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_perspective__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11493750108264075,
          "std": 0.0,
          "min": 0.11493750108264075,
          "max": 0.11493750108264075,
          "median": 0.11493750108264075,
          "values": [
            0.11493750108264075
          ]
        },
        "recall": {
          "mean": 0.38172674889351305,
          "std": 0.0,
          "min": 0.38172674889351305,
          "max": 0.38172674889351305,
          "median": 0.38172674889351305,
          "values": [
            0.38172674889351305
          ]
        },
        "f1_score": {
          "mean": 0.17338333336265654,
          "std": 0.0,
          "min": 0.17338333336265654,
          "max": 0.17338333336265654,
          "median": 0.17338333336265654,
          "values": [
            0.17338333336265654
          ]
        },
        "jaccard": {
          "mean": 0.09552494270400932,
          "std": 0.0,
          "min": 0.09552494270400932,
          "max": 0.09552494270400932,
          "median": 0.09552494270400932,
          "values": [
            0.09552494270400932
          ]
        },
        "semantic_similarity": {
          "mean": 0.5412483015656471,
          "std": 0.0,
          "min": 0.5412483015656471,
          "max": 0.5412483015656471,
          "median": 0.5412483015656471,
          "values": [
            0.5412483015656471
          ]
        },
        "all_choices_addressed": {
          "mean": 0.29600000000000004,
          "std": 0.0,
          "min": 0.29600000000000004,
          "max": 0.29600000000000004,
          "median": 0.29600000000000004,
          "values": [
            0.29600000000000004
          ]
        },
        "single_paragraph_format": {
          "mean": 0.725,
          "std": 0.0,
          "min": 0.725,
          "max": 0.725,
          "median": 0.725,
          "values": [
            0.725
          ]
        },
        "explanation_depth": {
          "mean": 0.3428,
          "std": 0.0,
          "min": 0.3428,
          "max": 0.3428,
          "median": 0.3428,
          "values": [
            0.3428
          ]
        }
      }
    },
    "ecqa_stakeholder_small_business_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b__zero-shot__ecqa_stakeholder_small_business__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09512191375148955,
          "std": 0.0,
          "min": 0.09512191375148955,
          "max": 0.09512191375148955,
          "median": 0.09512191375148955,
          "values": [
            0.09512191375148955
          ]
        },
        "recall": {
          "mean": 0.35082479271463884,
          "std": 0.0,
          "min": 0.35082479271463884,
          "max": 0.35082479271463884,
          "median": 0.35082479271463884,
          "values": [
            0.35082479271463884
          ]
        },
        "f1_score": {
          "mean": 0.14675153857318823,
          "std": 0.0,
          "min": 0.14675153857318823,
          "max": 0.14675153857318823,
          "median": 0.14675153857318823,
          "values": [
            0.14675153857318823
          ]
        },
        "jaccard": {
          "mean": 0.07969752174909459,
          "std": 0.0,
          "min": 0.07969752174909459,
          "max": 0.07969752174909459,
          "median": 0.07969752174909459,
          "values": [
            0.07969752174909459
          ]
        },
        "semantic_similarity": {
          "mean": 0.47006827145814895,
          "std": 0.0,
          "min": 0.47006827145814895,
          "max": 0.47006827145814895,
          "median": 0.47006827145814895,
          "values": [
            0.47006827145814895
          ]
        },
        "all_choices_addressed": {
          "mean": 0.266,
          "std": 0.0,
          "min": 0.266,
          "max": 0.266,
          "median": 0.266,
          "values": [
            0.266
          ]
        },
        "single_paragraph_format": {
          "mean": 0.29600000000000004,
          "std": 0.0,
          "min": 0.29600000000000004,
          "max": 0.29600000000000004,
          "median": 0.29600000000000004,
          "values": [
            0.29600000000000004
          ]
        },
        "explanation_depth": {
          "mean": 0.35239999999999994,
          "std": 0.0,
          "min": 0.35239999999999994,
          "max": 0.35239999999999994,
          "median": 0.35239999999999994,
          "values": [
            0.35239999999999994
          ]
        }
      }
    },
    "ecqa_freeflow_explanation_u4b-mistral-7b_ft_ecqa_freeflow_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_freeflow__u4b-mistral-7b_ft_ecqa_freeflow_ft__zero-shot__ecqa_freeflow_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_freeflow"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.4898914464228833,
          "std": 0.0,
          "min": 0.4898914464228833,
          "max": 0.4898914464228833,
          "median": 0.4898914464228833,
          "values": [
            0.4898914464228833
          ]
        },
        "recall": {
          "mean": 0.417051056580515,
          "std": 0.0,
          "min": 0.417051056580515,
          "max": 0.417051056580515,
          "median": 0.417051056580515,
          "values": [
            0.417051056580515
          ]
        },
        "f1_score": {
          "mean": 0.4354941311253262,
          "std": 0.0,
          "min": 0.4354941311253262,
          "max": 0.4354941311253262,
          "median": 0.4354941311253262,
          "values": [
            0.4354941311253262
          ]
        },
        "jaccard": {
          "mean": 0.2848766051944887,
          "std": 0.0,
          "min": 0.2848766051944887,
          "max": 0.2848766051944887,
          "median": 0.2848766051944887,
          "values": [
            0.2848766051944887
          ]
        },
        "semantic_similarity": {
          "mean": 0.7053428494930267,
          "std": 0.0,
          "min": 0.7053428494930267,
          "max": 0.7053428494930267,
          "median": 0.7053428494930267,
          "values": [
            0.7053428494930267
          ]
        },
        "all_choices_addressed": {
          "mean": 0.818,
          "std": 0.0,
          "min": 0.818,
          "max": 0.818,
          "median": 0.818,
          "values": [
            0.818
          ]
        },
        "single_paragraph_format": {
          "mean": 0.23099999999999998,
          "std": 0.0,
          "min": 0.23099999999999998,
          "max": 0.23099999999999998,
          "median": 0.23099999999999998,
          "values": [
            0.23099999999999998
          ]
        },
        "explanation_depth": {
          "mean": 0.08033856265439387,
          "std": 0.0,
          "min": 0.08033856265439387,
          "max": 0.08033856265439387,
          "median": 0.08033856265439387,
          "values": [
            0.08033856265439387
          ]
        }
      }
    },
    "ecqa_negative_explanation_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__hf-mistral-nemo-12b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1814849448190401,
          "std": 0.0,
          "min": 0.1814849448190401,
          "max": 0.1814849448190401,
          "median": 0.1814849448190401,
          "values": [
            0.1814849448190401
          ]
        },
        "recall": {
          "mean": 0.3443703285419432,
          "std": 0.0,
          "min": 0.3443703285419432,
          "max": 0.3443703285419432,
          "median": 0.3443703285419432,
          "values": [
            0.3443703285419432
          ]
        },
        "f1_score": {
          "mean": 0.22457098102242015,
          "std": 0.0,
          "min": 0.22457098102242015,
          "max": 0.22457098102242015,
          "median": 0.22457098102242015,
          "values": [
            0.22457098102242015
          ]
        },
        "jaccard": {
          "mean": 0.12835014809217857,
          "std": 0.0,
          "min": 0.12835014809217857,
          "max": 0.12835014809217857,
          "median": 0.12835014809217857,
          "values": [
            0.12835014809217857
          ]
        },
        "semantic_similarity": {
          "mean": 0.6748887285590172,
          "std": 0.0,
          "min": 0.6748887285590172,
          "max": 0.6748887285590172,
          "median": 0.6748887285590172,
          "values": [
            0.6748887285590172
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9631666666666666,
          "std": 0.0,
          "min": 0.9631666666666666,
          "max": 0.9631666666666666,
          "median": 0.9631666666666666,
          "values": [
            0.9631666666666666
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7615942307692307,
          "std": 0.0,
          "min": 0.7615942307692307,
          "max": 0.7615942307692307,
          "median": 0.7615942307692307,
          "values": [
            0.7615942307692307
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__hf-qwen2.5-3b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.2360652492606102,
          "std": 0.0,
          "min": 0.2360652492606102,
          "max": 0.2360652492606102,
          "median": 0.2360652492606102,
          "values": [
            0.2360652492606102
          ]
        },
        "recall": {
          "mean": 0.44029410367759925,
          "std": 0.0,
          "min": 0.44029410367759925,
          "max": 0.44029410367759925,
          "median": 0.44029410367759925,
          "values": [
            0.44029410367759925
          ]
        },
        "f1_score": {
          "mean": 0.2880224527919178,
          "std": 0.0,
          "min": 0.2880224527919178,
          "max": 0.2880224527919178,
          "median": 0.2880224527919178,
          "values": [
            0.2880224527919178
          ]
        },
        "jaccard": {
          "mean": 0.17076389996907515,
          "std": 0.0,
          "min": 0.17076389996907515,
          "max": 0.17076389996907515,
          "median": 0.17076389996907515,
          "values": [
            0.17076389996907515
          ]
        },
        "semantic_similarity": {
          "mean": 0.7280371448397637,
          "std": 0.0,
          "min": 0.7280371448397637,
          "max": 0.7280371448397637,
          "median": 0.7280371448397637,
          "values": [
            0.7280371448397637
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.6975,
          "std": 0.0,
          "min": 0.6975,
          "max": 0.6975,
          "median": 0.6975,
          "values": [
            0.6975
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7048857142857142,
          "std": 0.0,
          "min": 0.7048857142857142,
          "max": 0.7048857142857142,
          "median": 0.7048857142857142,
          "values": [
            0.7048857142857142
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-llama3-8b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1640683464500342,
          "std": 0.0,
          "min": 0.1640683464500342,
          "max": 0.1640683464500342,
          "median": 0.1640683464500342,
          "values": [
            0.1640683464500342
          ]
        },
        "recall": {
          "mean": 0.46629159451760577,
          "std": 0.0,
          "min": 0.46629159451760577,
          "max": 0.46629159451760577,
          "median": 0.46629159451760577,
          "values": [
            0.46629159451760577
          ]
        },
        "f1_score": {
          "mean": 0.22847661977633685,
          "std": 0.0,
          "min": 0.22847661977633685,
          "max": 0.22847661977633685,
          "median": 0.22847661977633685,
          "values": [
            0.22847661977633685
          ]
        },
        "jaccard": {
          "mean": 0.13105611733710348,
          "std": 0.0,
          "min": 0.13105611733710348,
          "max": 0.13105611733710348,
          "median": 0.13105611733710348,
          "values": [
            0.13105611733710348
          ]
        },
        "semantic_similarity": {
          "mean": 0.6311278274655342,
          "std": 0.0,
          "min": 0.6311278274655342,
          "max": 0.6311278274655342,
          "median": 0.6311278274655342,
          "values": [
            0.6311278274655342
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9576666666666667,
          "std": 0.0,
          "min": 0.9576666666666667,
          "max": 0.9576666666666667,
          "median": 0.9576666666666667,
          "values": [
            0.9576666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.509588888888889,
          "std": 0.0,
          "min": 0.509588888888889,
          "max": 0.509588888888889,
          "median": 0.509588888888889,
          "values": [
            0.509588888888889
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-llama3.2-1b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.14725606766815846,
          "std": 0.0,
          "min": 0.14725606766815846,
          "max": 0.14725606766815846,
          "median": 0.14725606766815846,
          "values": [
            0.14725606766815846
          ]
        },
        "recall": {
          "mean": 0.3874905023614697,
          "std": 0.0,
          "min": 0.3874905023614697,
          "max": 0.3874905023614697,
          "median": 0.3874905023614697,
          "values": [
            0.3874905023614697
          ]
        },
        "f1_score": {
          "mean": 0.1973631081199648,
          "std": 0.0,
          "min": 0.1973631081199648,
          "max": 0.1973631081199648,
          "median": 0.1973631081199648,
          "values": [
            0.1973631081199648
          ]
        },
        "jaccard": {
          "mean": 0.1128273650124004,
          "std": 0.0,
          "min": 0.1128273650124004,
          "max": 0.1128273650124004,
          "median": 0.1128273650124004,
          "values": [
            0.1128273650124004
          ]
        },
        "semantic_similarity": {
          "mean": 0.566499483268708,
          "std": 0.0,
          "min": 0.566499483268708,
          "max": 0.566499483268708,
          "median": 0.566499483268708,
          "values": [
            0.566499483268708
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.7106666666666668,
          "std": 0.0,
          "min": 0.7106666666666668,
          "max": 0.7106666666666668,
          "median": 0.7106666666666668,
          "values": [
            0.7106666666666668
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.7465775691585677,
          "std": 0.0,
          "min": 0.7465775691585677,
          "max": 0.7465775691585677,
          "median": 0.7465775691585677,
          "values": [
            0.7465775691585677
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-llama3.3-70b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1900079193383912,
          "std": 0.0,
          "min": 0.1900079193383912,
          "max": 0.1900079193383912,
          "median": 0.1900079193383912,
          "values": [
            0.1900079193383912
          ]
        },
        "recall": {
          "mean": 0.4785876572797395,
          "std": 0.0,
          "min": 0.4785876572797395,
          "max": 0.4785876572797395,
          "median": 0.4785876572797395,
          "values": [
            0.4785876572797395
          ]
        },
        "f1_score": {
          "mean": 0.2540873034091251,
          "std": 0.0,
          "min": 0.2540873034091251,
          "max": 0.2540873034091251,
          "median": 0.2540873034091251,
          "values": [
            0.2540873034091251
          ]
        },
        "jaccard": {
          "mean": 0.14906515089190975,
          "std": 0.0,
          "min": 0.14906515089190975,
          "max": 0.14906515089190975,
          "median": 0.14906515089190975,
          "values": [
            0.14906515089190975
          ]
        },
        "semantic_similarity": {
          "mean": 0.5594649736583233,
          "std": 0.0,
          "min": 0.5594649736583233,
          "max": 0.5594649736583233,
          "median": 0.5594649736583233,
          "values": [
            0.5594649736583233
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9475,
          "std": 0.0,
          "min": 0.9475,
          "max": 0.9475,
          "median": 0.9475,
          "values": [
            0.9475
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.9057197691884995,
          "std": 0.0,
          "min": 0.9057197691884995,
          "max": 0.9057197691884995,
          "median": 0.9057197691884995,
          "values": [
            0.9057197691884995
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-mistral-7b__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.19569829617799822,
          "std": 0.0,
          "min": 0.19569829617799822,
          "max": 0.19569829617799822,
          "median": 0.19569829617799822,
          "values": [
            0.19569829617799822
          ]
        },
        "recall": {
          "mean": 0.4721137596409772,
          "std": 0.0,
          "min": 0.4721137596409772,
          "max": 0.4721137596409772,
          "median": 0.4721137596409772,
          "values": [
            0.4721137596409772
          ]
        },
        "f1_score": {
          "mean": 0.2602851734506764,
          "std": 0.0,
          "min": 0.2602851734506764,
          "max": 0.2602851734506764,
          "median": 0.2602851734506764,
          "values": [
            0.2602851734506764
          ]
        },
        "jaccard": {
          "mean": 0.15175693650396982,
          "std": 0.0,
          "min": 0.15175693650396982,
          "max": 0.15175693650396982,
          "median": 0.15175693650396982,
          "values": [
            0.15175693650396982
          ]
        },
        "semantic_similarity": {
          "mean": 0.7106705468893051,
          "std": 0.0,
          "min": 0.7106705468893051,
          "max": 0.7106705468893051,
          "median": 0.7106705468893051,
          "values": [
            0.7106705468893051
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9631666666666667,
          "std": 0.0,
          "min": 0.9631666666666667,
          "max": 0.9631666666666667,
          "median": 0.9631666666666667,
          "values": [
            0.9631666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.6698304612054612,
          "std": 0.0,
          "min": 0.6698304612054612,
          "max": 0.6698304612054612,
          "median": 0.6698304612054612,
          "values": [
            0.6698304612054612
          ]
        },
        "external_knowledge_usage": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "ecqa_negative_explanation_u4b-mistral-7b_ft_ecqa_negative_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_negative__u4b-mistral-7b_ft_ecqa_negative_ft__zero-shot__ecqa_negative_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_negative"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.34900191824625354,
          "std": 0.0,
          "min": 0.34900191824625354,
          "max": 0.34900191824625354,
          "median": 0.34900191824625354,
          "values": [
            0.34900191824625354
          ]
        },
        "recall": {
          "mean": 0.40863570814618677,
          "std": 0.0,
          "min": 0.40863570814618677,
          "max": 0.40863570814618677,
          "median": 0.40863570814618677,
          "values": [
            0.40863570814618677
          ]
        },
        "f1_score": {
          "mean": 0.34303866741771744,
          "std": 0.0,
          "min": 0.34303866741771744,
          "max": 0.34303866741771744,
          "median": 0.34303866741771744,
          "values": [
            0.34303866741771744
          ]
        },
        "jaccard": {
          "mean": 0.21255279965018897,
          "std": 0.0,
          "min": 0.21255279965018897,
          "max": 0.21255279965018897,
          "median": 0.21255279965018897,
          "values": [
            0.21255279965018897
          ]
        },
        "semantic_similarity": {
          "mean": 0.6521517732739448,
          "std": 0.0,
          "min": 0.6521517732739448,
          "max": 0.6521517732739448,
          "median": 0.6521517732739448,
          "values": [
            0.6521517732739448
          ]
        },
        "incorrect_choices_coverage": {
          "mean": 0.9096666666666667,
          "std": 0.0,
          "min": 0.9096666666666667,
          "max": 0.9096666666666667,
          "median": 0.9096666666666667,
          "values": [
            0.9096666666666667
          ]
        },
        "atomic_sentence_format": {
          "mean": 0.8008666610194901,
          "std": 0.0,
          "min": 0.8008666610194901,
          "max": 0.8008666610194901,
          "median": 0.8008666610194901,
          "values": [
            0.8008666610194901
          ]
        },
        "external_knowledge_usage": {
          "mean": 0.9981407089151451,
          "std": 0.0,
          "min": 0.9981407089151451,
          "max": 0.9981407089151451,
          "median": 0.9981407089151451,
          "values": [
            0.9981407089151451
          ]
        }
      }
    },
    "ecqa_positive_explanation_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.3746907428025377,
          "std": 0.0,
          "min": 0.3746907428025377,
          "max": 0.3746907428025377,
          "median": 0.3746907428025377,
          "values": [
            0.3746907428025377
          ]
        },
        "recall": {
          "mean": 0.26725973729837293,
          "std": 0.0,
          "min": 0.26725973729837293,
          "max": 0.26725973729837293,
          "median": 0.26725973729837293,
          "values": [
            0.26725973729837293
          ]
        },
        "f1_score": {
          "mean": 0.2917493179502968,
          "std": 0.0,
          "min": 0.2917493179502968,
          "max": 0.2917493179502968,
          "median": 0.2917493179502968,
          "values": [
            0.2917493179502968
          ]
        },
        "jaccard": {
          "mean": 0.18023991309653273,
          "std": 0.0,
          "min": 0.18023991309653273,
          "max": 0.18023991309653273,
          "median": 0.18023991309653273,
          "values": [
            0.18023991309653273
          ]
        },
        "semantic_similarity": {
          "mean": 0.6730493840575218,
          "std": 0.0,
          "min": 0.6730493840575218,
          "max": 0.6730493840575218,
          "median": 0.6730493840575218,
          "values": [
            0.6730493840575218
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.7314999999999999,
          "std": 0.0,
          "min": 0.7314999999999999,
          "max": 0.7314999999999999,
          "median": 0.7314999999999999,
          "values": [
            0.7314999999999999
          ]
        },
        "correct_answer_focus": {
          "mean": 0.685,
          "std": 0.0,
          "min": 0.685,
          "max": 0.685,
          "median": 0.685,
          "values": [
            0.685
          ]
        },
        "concise_justification": {
          "mean": 0.9618999999999999,
          "std": 0.0,
          "min": 0.9618999999999999,
          "max": 0.9618999999999999,
          "median": 0.9618999999999999,
          "values": [
            0.9618999999999999
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.24300644826220846,
          "std": 0.0,
          "min": 0.24300644826220846,
          "max": 0.24300644826220846,
          "median": 0.24300644826220846,
          "values": [
            0.24300644826220846
          ]
        },
        "recall": {
          "mean": 0.2886052257904705,
          "std": 0.0,
          "min": 0.2886052257904705,
          "max": 0.2886052257904705,
          "median": 0.2886052257904705,
          "values": [
            0.2886052257904705
          ]
        },
        "f1_score": {
          "mean": 0.24871530612969905,
          "std": 0.0,
          "min": 0.24871530612969905,
          "max": 0.24871530612969905,
          "median": 0.24871530612969905,
          "values": [
            0.24871530612969905
          ]
        },
        "jaccard": {
          "mean": 0.1476088298107717,
          "std": 0.0,
          "min": 0.1476088298107717,
          "max": 0.1476088298107717,
          "median": 0.1476088298107717,
          "values": [
            0.1476088298107717
          ]
        },
        "semantic_similarity": {
          "mean": 0.6961110651493072,
          "std": 0.0,
          "min": 0.6961110651493072,
          "max": 0.6961110651493072,
          "median": 0.6961110651493072,
          "values": [
            0.6961110651493072
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.9596666666666667,
          "std": 0.0,
          "min": 0.9596666666666667,
          "max": 0.9596666666666667,
          "median": 0.9596666666666667,
          "values": [
            0.9596666666666667
          ]
        },
        "correct_answer_focus": {
          "mean": 0.4605,
          "std": 0.0,
          "min": 0.4605,
          "max": 0.4605,
          "median": 0.4605,
          "values": [
            0.4605
          ]
        },
        "concise_justification": {
          "mean": 0.9515,
          "std": 0.0,
          "min": 0.9515,
          "max": 0.9515,
          "median": 0.9515,
          "values": [
            0.9515
          ]
        }
      }
    },
    "ecqa_positive_explanation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.28283361234475163,
          "std": 0.0,
          "min": 0.28283361234475163,
          "max": 0.28283361234475163,
          "median": 0.28283361234475163,
          "values": [
            0.28283361234475163
          ]
        },
        "recall": {
          "mean": 0.33624103447413994,
          "std": 0.0,
          "min": 0.33624103447413994,
          "max": 0.33624103447413994,
          "median": 0.33624103447413994,
          "values": [
            0.33624103447413994
          ]
        },
        "f1_score": {
          "mean": 0.29677117818000276,
          "std": 0.0,
          "min": 0.29677117818000276,
          "max": 0.29677117818000276,
          "median": 0.29677117818000276,
          "values": [
            0.29677117818000276
          ]
        },
        "jaccard": {
          "mean": 0.1812543356959735,
          "std": 0.0,
          "min": 0.1812543356959735,
          "max": 0.1812543356959735,
          "median": 0.1812543356959735,
          "values": [
            0.1812543356959735
          ]
        },
        "semantic_similarity": {
          "mean": 0.7155673375725746,
          "std": 0.0,
          "min": 0.7155673375725746,
          "max": 0.7155673375725746,
          "median": 0.7155673375725746,
          "values": [
            0.7155673375725746
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.4531,
          "std": 0.0,
          "min": 0.4531,
          "max": 0.4531,
          "median": 0.4531,
          "values": [
            0.4531
          ]
        },
        "correct_answer_focus": {
          "mean": 0.8645,
          "std": 0.0,
          "min": 0.8645,
          "max": 0.8645,
          "median": 0.8645,
          "values": [
            0.8645
          ]
        },
        "concise_justification": {
          "mean": 0.8898999999999999,
          "std": 0.0,
          "min": 0.8898999999999999,
          "max": 0.8898999999999999,
          "median": 0.8898999999999999,
          "values": [
            0.8898999999999999
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__hf-qwen2.5-3b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.22713080245473005,
          "std": 0.0,
          "min": 0.22713080245473005,
          "max": 0.22713080245473005,
          "median": 0.22713080245473005,
          "values": [
            0.22713080245473005
          ]
        },
        "recall": {
          "mean": 0.33912661300515906,
          "std": 0.0,
          "min": 0.33912661300515906,
          "max": 0.33912661300515906,
          "median": 0.33912661300515906,
          "values": [
            0.33912661300515906
          ]
        },
        "f1_score": {
          "mean": 0.2600337834804731,
          "std": 0.0,
          "min": 0.2600337834804731,
          "max": 0.2600337834804731,
          "median": 0.2600337834804731,
          "values": [
            0.2600337834804731
          ]
        },
        "jaccard": {
          "mean": 0.15342230017879868,
          "std": 0.0,
          "min": 0.15342230017879868,
          "max": 0.15342230017879868,
          "median": 0.15342230017879868,
          "values": [
            0.15342230017879868
          ]
        },
        "semantic_similarity": {
          "mean": 0.7356852543354034,
          "std": 0.0,
          "min": 0.7356852543354034,
          "max": 0.7356852543354034,
          "median": 0.7356852543354034,
          "values": [
            0.7356852543354034
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.9494166666666668,
          "std": 0.0,
          "min": 0.9494166666666668,
          "max": 0.9494166666666668,
          "median": 0.9494166666666668,
          "values": [
            0.9494166666666668
          ]
        },
        "correct_answer_focus": {
          "mean": 0.6680000000000001,
          "std": 0.0,
          "min": 0.6680000000000001,
          "max": 0.6680000000000001,
          "median": 0.6680000000000001,
          "values": [
            0.6680000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.9486,
          "std": 0.0,
          "min": 0.9486,
          "max": 0.9486,
          "median": 0.9486,
          "values": [
            0.9486
          ]
        }
      }
    },
    "ecqa_positive_explanation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.264387653966471,
          "std": 0.0,
          "min": 0.264387653966471,
          "max": 0.264387653966471,
          "median": 0.264387653966471,
          "values": [
            0.264387653966471
          ]
        },
        "recall": {
          "mean": 0.3577853375793798,
          "std": 0.0,
          "min": 0.3577853375793798,
          "max": 0.3577853375793798,
          "median": 0.3577853375793798,
          "values": [
            0.3577853375793798
          ]
        },
        "f1_score": {
          "mean": 0.29290683675032814,
          "std": 0.0,
          "min": 0.29290683675032814,
          "max": 0.29290683675032814,
          "median": 0.29290683675032814,
          "values": [
            0.29290683675032814
          ]
        },
        "jaccard": {
          "mean": 0.17771054772563183,
          "std": 0.0,
          "min": 0.17771054772563183,
          "max": 0.17771054772563183,
          "median": 0.17771054772563183,
          "values": [
            0.17771054772563183
          ]
        },
        "semantic_similarity": {
          "mean": 0.6796818909049034,
          "std": 0.0,
          "min": 0.6796818909049034,
          "max": 0.6796818909049034,
          "median": 0.6796818909049034,
          "values": [
            0.6796818909049034
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.24783333333333327,
          "std": 0.0,
          "min": 0.24783333333333327,
          "max": 0.24783333333333327,
          "median": 0.24783333333333327,
          "values": [
            0.24783333333333327
          ]
        },
        "correct_answer_focus": {
          "mean": 0.9045000000000001,
          "std": 0.0,
          "min": 0.9045000000000001,
          "max": 0.9045000000000001,
          "median": 0.9045000000000001,
          "values": [
            0.9045000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.8417999999999998,
          "std": 0.0,
          "min": 0.8417999999999998,
          "max": 0.8417999999999998,
          "median": 0.8417999999999998,
          "values": [
            0.8417999999999998
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3-8b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1775280103685869,
          "std": 0.0,
          "min": 0.1775280103685869,
          "max": 0.1775280103685869,
          "median": 0.1775280103685869,
          "values": [
            0.1775280103685869
          ]
        },
        "recall": {
          "mean": 0.43983052902111197,
          "std": 0.0,
          "min": 0.43983052902111197,
          "max": 0.43983052902111197,
          "median": 0.43983052902111197,
          "values": [
            0.43983052902111197
          ]
        },
        "f1_score": {
          "mean": 0.24264238454235382,
          "std": 0.0,
          "min": 0.24264238454235382,
          "max": 0.24264238454235382,
          "median": 0.24264238454235382,
          "values": [
            0.24264238454235382
          ]
        },
        "jaccard": {
          "mean": 0.14124233824467206,
          "std": 0.0,
          "min": 0.14124233824467206,
          "max": 0.14124233824467206,
          "median": 0.14124233824467206,
          "values": [
            0.14124233824467206
          ]
        },
        "semantic_similarity": {
          "mean": 0.7010463732481003,
          "std": 0.0,
          "min": 0.7010463732481003,
          "max": 0.7010463732481003,
          "median": 0.7010463732481003,
          "values": [
            0.7010463732481003
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.6478416666666666,
          "std": 0.0,
          "min": 0.6478416666666666,
          "max": 0.6478416666666666,
          "median": 0.6478416666666666,
          "values": [
            0.6478416666666666
          ]
        },
        "correct_answer_focus": {
          "mean": 0.7440000000000001,
          "std": 0.0,
          "min": 0.7440000000000001,
          "max": 0.7440000000000001,
          "median": 0.7440000000000001,
          "values": [
            0.7440000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.8521,
          "std": 0.0,
          "min": 0.8521,
          "max": 0.8521,
          "median": 0.8521,
          "values": [
            0.8521
          ]
        }
      }
    },
    "ecqa_positive_explanation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.22993602607248176,
          "std": 0.0,
          "min": 0.22993602607248176,
          "max": 0.22993602607248176,
          "median": 0.22993602607248176,
          "values": [
            0.22993602607248176
          ]
        },
        "recall": {
          "mean": 0.3856810967414349,
          "std": 0.0,
          "min": 0.3856810967414349,
          "max": 0.3856810967414349,
          "median": 0.3856810967414349,
          "values": [
            0.3856810967414349
          ]
        },
        "f1_score": {
          "mean": 0.2702972475885435,
          "std": 0.0,
          "min": 0.2702972475885435,
          "max": 0.2702972475885435,
          "median": 0.2702972475885435,
          "values": [
            0.2702972475885435
          ]
        },
        "jaccard": {
          "mean": 0.16312343186193573,
          "std": 0.0,
          "min": 0.16312343186193573,
          "max": 0.16312343186193573,
          "median": 0.16312343186193573,
          "values": [
            0.16312343186193573
          ]
        },
        "semantic_similarity": {
          "mean": 0.6743487492203712,
          "std": 0.0,
          "min": 0.6743487492203712,
          "max": 0.6743487492203712,
          "median": 0.6743487492203712,
          "values": [
            0.6743487492203712
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.4222416666666667,
          "std": 0.0,
          "min": 0.4222416666666667,
          "max": 0.4222416666666667,
          "median": 0.4222416666666667,
          "values": [
            0.4222416666666667
          ]
        },
        "correct_answer_focus": {
          "mean": 0.922,
          "std": 0.0,
          "min": 0.922,
          "max": 0.922,
          "median": 0.922,
          "values": [
            0.922
          ]
        },
        "concise_justification": {
          "mean": 0.8477,
          "std": 0.0,
          "min": 0.8477,
          "max": 0.8477,
          "median": 0.8477,
          "values": [
            0.8477
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.18224464488739073,
          "std": 0.0,
          "min": 0.18224464488739073,
          "max": 0.18224464488739073,
          "median": 0.18224464488739073,
          "values": [
            0.18224464488739073
          ]
        },
        "recall": {
          "mean": 0.40707478148907894,
          "std": 0.0,
          "min": 0.40707478148907894,
          "max": 0.40707478148907894,
          "median": 0.40707478148907894,
          "values": [
            0.40707478148907894
          ]
        },
        "f1_score": {
          "mean": 0.23807440498595006,
          "std": 0.0,
          "min": 0.23807440498595006,
          "max": 0.23807440498595006,
          "median": 0.23807440498595006,
          "values": [
            0.23807440498595006
          ]
        },
        "jaccard": {
          "mean": 0.13944884498651247,
          "std": 0.0,
          "min": 0.13944884498651247,
          "max": 0.13944884498651247,
          "median": 0.13944884498651247,
          "values": [
            0.13944884498651247
          ]
        },
        "semantic_similarity": {
          "mean": 0.6837376776337624,
          "std": 0.0,
          "min": 0.6837376776337624,
          "max": 0.6837376776337624,
          "median": 0.6837376776337624,
          "values": [
            0.6837376776337624
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.6207083333333334,
          "std": 0.0,
          "min": 0.6207083333333334,
          "max": 0.6207083333333334,
          "median": 0.6207083333333334,
          "values": [
            0.6207083333333334
          ]
        },
        "correct_answer_focus": {
          "mean": 0.6459999999999999,
          "std": 0.0,
          "min": 0.6459999999999999,
          "max": 0.6459999999999999,
          "median": 0.6459999999999999,
          "values": [
            0.6459999999999999
          ]
        },
        "concise_justification": {
          "mean": 0.8412,
          "std": 0.0,
          "min": 0.8412,
          "max": 0.8412,
          "median": 0.8412,
          "values": [
            0.8412
          ]
        }
      }
    },
    "ecqa_positive_explanation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.28863006709410444,
          "std": 0.0,
          "min": 0.28863006709410444,
          "max": 0.28863006709410444,
          "median": 0.28863006709410444,
          "values": [
            0.28863006709410444
          ]
        },
        "recall": {
          "mean": 0.37233378418672053,
          "std": 0.0,
          "min": 0.37233378418672053,
          "max": 0.37233378418672053,
          "median": 0.37233378418672053,
          "values": [
            0.37233378418672053
          ]
        },
        "f1_score": {
          "mean": 0.2804271626322052,
          "std": 0.0,
          "min": 0.2804271626322052,
          "max": 0.2804271626322052,
          "median": 0.2804271626322052,
          "values": [
            0.2804271626322052
          ]
        },
        "jaccard": {
          "mean": 0.17314617116284642,
          "std": 0.0,
          "min": 0.17314617116284642,
          "max": 0.17314617116284642,
          "median": 0.17314617116284642,
          "values": [
            0.17314617116284642
          ]
        },
        "semantic_similarity": {
          "mean": 0.6046197773516178,
          "std": 0.0,
          "min": 0.6046197773516178,
          "max": 0.6046197773516178,
          "median": 0.6046197773516178,
          "values": [
            0.6046197773516178
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.7534486028677206,
          "std": 0.0,
          "min": 0.7534486028677206,
          "max": 0.7534486028677206,
          "median": 0.7534486028677206,
          "values": [
            0.7534486028677206
          ]
        },
        "correct_answer_focus": {
          "mean": 0.6884999999999999,
          "std": 0.0,
          "min": 0.6884999999999999,
          "max": 0.6884999999999999,
          "median": 0.6884999999999999,
          "values": [
            0.6884999999999999
          ]
        },
        "concise_justification": {
          "mean": 0.7844999999999998,
          "std": 0.0,
          "min": 0.7844999999999998,
          "max": 0.7844999999999998,
          "median": 0.7844999999999998,
          "values": [
            0.7844999999999998
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-llama3.3-70b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.21777957840974974,
          "std": 0.0,
          "min": 0.21777957840974974,
          "max": 0.21777957840974974,
          "median": 0.21777957840974974,
          "values": [
            0.21777957840974974
          ]
        },
        "recall": {
          "mean": 0.40589122920410575,
          "std": 0.0,
          "min": 0.40589122920410575,
          "max": 0.40589122920410575,
          "median": 0.40589122920410575,
          "values": [
            0.40589122920410575
          ]
        },
        "f1_score": {
          "mean": 0.2735406292840013,
          "std": 0.0,
          "min": 0.2735406292840013,
          "max": 0.2735406292840013,
          "median": 0.2735406292840013,
          "values": [
            0.2735406292840013
          ]
        },
        "jaccard": {
          "mean": 0.16312967489327948,
          "std": 0.0,
          "min": 0.16312967489327948,
          "max": 0.16312967489327948,
          "median": 0.16312967489327948,
          "values": [
            0.16312967489327948
          ]
        },
        "semantic_similarity": {
          "mean": 0.6960352997481823,
          "std": 0.0,
          "min": 0.6960352997481823,
          "max": 0.6960352997481823,
          "median": 0.6960352997481823,
          "values": [
            0.6960352997481823
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.9127463578088579,
          "std": 0.0,
          "min": 0.9127463578088579,
          "max": 0.9127463578088579,
          "median": 0.9127463578088579,
          "values": [
            0.9127463578088579
          ]
        },
        "correct_answer_focus": {
          "mean": 0.6465000000000001,
          "std": 0.0,
          "min": 0.6465000000000001,
          "max": 0.6465000000000001,
          "median": 0.6465000000000001,
          "values": [
            0.6465000000000001
          ]
        },
        "concise_justification": {
          "mean": 0.894,
          "std": 0.0,
          "min": 0.894,
          "max": 0.894,
          "median": 0.894,
          "values": [
            0.894
          ]
        }
      }
    },
    "ecqa_positive_explanation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.247546451733437,
          "std": 0.0,
          "min": 0.247546451733437,
          "max": 0.247546451733437,
          "median": 0.247546451733437,
          "values": [
            0.247546451733437
          ]
        },
        "recall": {
          "mean": 0.35424753913137935,
          "std": 0.0,
          "min": 0.35424753913137935,
          "max": 0.35424753913137935,
          "median": 0.35424753913137935,
          "values": [
            0.35424753913137935
          ]
        },
        "f1_score": {
          "mean": 0.28001469433145515,
          "std": 0.0,
          "min": 0.28001469433145515,
          "max": 0.28001469433145515,
          "median": 0.28001469433145515,
          "values": [
            0.28001469433145515
          ]
        },
        "jaccard": {
          "mean": 0.17102613728874708,
          "std": 0.0,
          "min": 0.17102613728874708,
          "max": 0.17102613728874708,
          "median": 0.17102613728874708,
          "values": [
            0.17102613728874708
          ]
        },
        "semantic_similarity": {
          "mean": 0.685083657503128,
          "std": 0.0,
          "min": 0.685083657503128,
          "max": 0.685083657503128,
          "median": 0.685083657503128,
          "values": [
            0.685083657503128
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.39141666666666663,
          "std": 0.0,
          "min": 0.39141666666666663,
          "max": 0.39141666666666663,
          "median": 0.39141666666666663,
          "values": [
            0.39141666666666663
          ]
        },
        "correct_answer_focus": {
          "mean": 0.714,
          "std": 0.0,
          "min": 0.714,
          "max": 0.714,
          "median": 0.714,
          "values": [
            0.714
          ]
        },
        "concise_justification": {
          "mean": 0.8468999999999999,
          "std": 0.0,
          "min": 0.8468999999999999,
          "max": 0.8468999999999999,
          "median": 0.8468999999999999,
          "values": [
            0.8468999999999999
          ]
        }
      }
    },
    "ecqa_positive_explanation_formatted_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-mistral-7b__zero-shot__ecqa_positive_explanation_formatted__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.15928865884265464,
          "std": 0.0,
          "min": 0.15928865884265464,
          "max": 0.15928865884265464,
          "median": 0.15928865884265464,
          "values": [
            0.15928865884265464
          ]
        },
        "recall": {
          "mean": 0.43978218040815426,
          "std": 0.0,
          "min": 0.43978218040815426,
          "max": 0.43978218040815426,
          "median": 0.43978218040815426,
          "values": [
            0.43978218040815426
          ]
        },
        "f1_score": {
          "mean": 0.22667320491504128,
          "std": 0.0,
          "min": 0.22667320491504128,
          "max": 0.22667320491504128,
          "median": 0.22667320491504128,
          "values": [
            0.22667320491504128
          ]
        },
        "jaccard": {
          "mean": 0.13051938039109187,
          "std": 0.0,
          "min": 0.13051938039109187,
          "max": 0.13051938039109187,
          "median": 0.13051938039109187,
          "values": [
            0.13051938039109187
          ]
        },
        "semantic_similarity": {
          "mean": 0.6958865889906883,
          "std": 0.0,
          "min": 0.6958865889906883,
          "max": 0.6958865889906883,
          "median": 0.6958865889906883,
          "values": [
            0.6958865889906883
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.8441799603174605,
          "std": 0.0,
          "min": 0.8441799603174605,
          "max": 0.8441799603174605,
          "median": 0.8441799603174605,
          "values": [
            0.8441799603174605
          ]
        },
        "correct_answer_focus": {
          "mean": 0.4335,
          "std": 0.0,
          "min": 0.4335,
          "max": 0.4335,
          "median": 0.4335,
          "values": [
            0.4335
          ]
        },
        "concise_justification": {
          "mean": 0.6778000000000003,
          "std": 0.0,
          "min": 0.6778000000000003,
          "max": 0.6778000000000003,
          "median": 0.6778000000000003,
          "values": [
            0.6778000000000003
          ]
        }
      }
    },
    "ecqa_positive_explanation_u4b-mistral-7b_ft_ecqa_positive_ft": {
      "num_experiments": 1,
      "experiments": [
        "ecqa_positive__u4b-mistral-7b_ft_ecqa_positive_ft__zero-shot__ecqa_positive_explanation__100__0p100"
      ],
      "setups": [
        "ecqa_positive"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.2890322051165036,
          "std": 0.0,
          "min": 0.2890322051165036,
          "max": 0.2890322051165036,
          "median": 0.2890322051165036,
          "values": [
            0.2890322051165036
          ]
        },
        "recall": {
          "mean": 0.5112916105060541,
          "std": 0.0,
          "min": 0.5112916105060541,
          "max": 0.5112916105060541,
          "median": 0.5112916105060541,
          "values": [
            0.5112916105060541
          ]
        },
        "f1_score": {
          "mean": 0.3469011914995081,
          "std": 0.0,
          "min": 0.3469011914995081,
          "max": 0.3469011914995081,
          "median": 0.3469011914995081,
          "values": [
            0.3469011914995081
          ]
        },
        "jaccard": {
          "mean": 0.21822543738909142,
          "std": 0.0,
          "min": 0.21822543738909142,
          "max": 0.21822543738909142,
          "median": 0.21822543738909142,
          "values": [
            0.21822543738909142
          ]
        },
        "semantic_similarity": {
          "mean": 0.6373477092385292,
          "std": 0.0,
          "min": 0.6373477092385292,
          "max": 0.6373477092385292,
          "median": 0.6373477092385292,
          "values": [
            0.6373477092385292
          ]
        },
        "simple_atomic_sentences": {
          "mean": 0.9223757869933645,
          "std": 0.0,
          "min": 0.9223757869933645,
          "max": 0.9223757869933645,
          "median": 0.9223757869933645,
          "values": [
            0.9223757869933645
          ]
        },
        "correct_answer_focus": {
          "mean": 0.7935,
          "std": 0.0,
          "min": 0.7935,
          "max": 0.7935,
          "median": 0.7935,
          "values": [
            0.7935
          ]
        },
        "concise_justification": {
          "mean": 0.5112000000000001,
          "std": 0.0,
          "min": 0.5112000000000001,
          "max": 0.5112000000000001,
          "median": 0.5112000000000001,
          "values": [
            0.5112000000000001
          ]
        }
      }
    },
    "gmeg_few_shot_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.12003115342841778,
          "std": 0.0,
          "min": 0.12003115342841778,
          "max": 0.12003115342841778,
          "median": 0.12003115342841778,
          "values": [
            0.12003115342841778
          ]
        },
        "recall": {
          "mean": 0.3778653268817748,
          "std": 0.0,
          "min": 0.3778653268817748,
          "max": 0.3778653268817748,
          "median": 0.3778653268817748,
          "values": [
            0.3778653268817748
          ]
        },
        "f1_score": {
          "mean": 0.17316757229496202,
          "std": 0.0,
          "min": 0.17316757229496202,
          "max": 0.17316757229496202,
          "median": 0.17316757229496202,
          "values": [
            0.17316757229496202
          ]
        },
        "jaccard": {
          "mean": 0.09716132290198534,
          "std": 0.0,
          "min": 0.09716132290198534,
          "max": 0.09716132290198534,
          "median": 0.09716132290198534,
          "values": [
            0.09716132290198534
          ]
        },
        "semantic_similarity": {
          "mean": 0.5275601665768772,
          "std": 0.0,
          "min": 0.5275601665768772,
          "max": 0.5275601665768772,
          "median": 0.5275601665768772,
          "values": [
            0.5275601665768772
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.4388333333333332,
          "std": 0.0,
          "min": 1.4388333333333332,
          "max": 1.4388333333333332,
          "median": 1.4388333333333332,
          "values": [
            1.4388333333333332
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7558333333333335,
          "std": 0.0,
          "min": 0.7558333333333335,
          "max": 0.7558333333333335,
          "median": 0.7558333333333335,
          "values": [
            0.7558333333333335
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__hf-mistral-nemo-12b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11652862486942757,
          "std": 0.0,
          "min": 0.11652862486942757,
          "max": 0.11652862486942757,
          "median": 0.11652862486942757,
          "values": [
            0.11652862486942757
          ]
        },
        "recall": {
          "mean": 0.380493739585753,
          "std": 0.0,
          "min": 0.380493739585753,
          "max": 0.380493739585753,
          "median": 0.380493739585753,
          "values": [
            0.380493739585753
          ]
        },
        "f1_score": {
          "mean": 0.16958565690582078,
          "std": 0.0,
          "min": 0.16958565690582078,
          "max": 0.16958565690582078,
          "median": 0.16958565690582078,
          "values": [
            0.16958565690582078
          ]
        },
        "jaccard": {
          "mean": 0.09533744963654023,
          "std": 0.0,
          "min": 0.09533744963654023,
          "max": 0.09533744963654023,
          "median": 0.09533744963654023,
          "values": [
            0.09533744963654023
          ]
        },
        "semantic_similarity": {
          "mean": 0.5151659706793725,
          "std": 0.0,
          "min": 0.5151659706793725,
          "max": 0.5151659706793725,
          "median": 0.5151659706793725,
          "values": [
            0.5151659706793725
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.4663333333333333,
          "std": 0.0,
          "min": 1.4663333333333333,
          "max": 1.4663333333333333,
          "median": 1.4663333333333333,
          "values": [
            1.4663333333333333
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8008333333333333,
          "std": 0.0,
          "min": 0.8008333333333333,
          "max": 0.8008333333333333,
          "median": 0.8008333333333333,
          "values": [
            0.8008333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.13734193549503154,
          "std": 0.0,
          "min": 0.13734193549503154,
          "max": 0.13734193549503154,
          "median": 0.13734193549503154,
          "values": [
            0.13734193549503154
          ]
        },
        "recall": {
          "mean": 0.3500143408670139,
          "std": 0.0,
          "min": 0.3500143408670139,
          "max": 0.3500143408670139,
          "median": 0.3500143408670139,
          "values": [
            0.3500143408670139
          ]
        },
        "f1_score": {
          "mean": 0.185488221032632,
          "std": 0.0,
          "min": 0.185488221032632,
          "max": 0.185488221032632,
          "median": 0.185488221032632,
          "values": [
            0.185488221032632
          ]
        },
        "jaccard": {
          "mean": 0.10520710932164107,
          "std": 0.0,
          "min": 0.10520710932164107,
          "max": 0.10520710932164107,
          "median": 0.10520710932164107,
          "values": [
            0.10520710932164107
          ]
        },
        "semantic_similarity": {
          "mean": 0.5092172273900359,
          "std": 0.0,
          "min": 0.5092172273900359,
          "max": 0.5092172273900359,
          "median": 0.5092172273900359,
          "values": [
            0.5092172273900359
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5721666666666665,
          "std": 0.0,
          "min": 1.5721666666666665,
          "max": 1.5721666666666665,
          "median": 1.5721666666666665,
          "values": [
            1.5721666666666665
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6699999999999998,
          "std": 0.0,
          "min": 0.6699999999999998,
          "max": 0.6699999999999998,
          "median": 0.6699999999999998,
          "values": [
            0.6699999999999998
          ]
        },
        "structural_format_match": {
          "mean": 0.9,
          "std": 0.0,
          "min": 0.9,
          "max": 0.9,
          "median": 0.9,
          "values": [
            0.9
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__hf-qwen2.5-3b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.12051848220273391,
          "std": 0.0,
          "min": 0.12051848220273391,
          "max": 0.12051848220273391,
          "median": 0.12051848220273391,
          "values": [
            0.12051848220273391
          ]
        },
        "recall": {
          "mean": 0.3467005139055157,
          "std": 0.0,
          "min": 0.3467005139055157,
          "max": 0.3467005139055157,
          "median": 0.3467005139055157,
          "values": [
            0.3467005139055157
          ]
        },
        "f1_score": {
          "mean": 0.17079180141365133,
          "std": 0.0,
          "min": 0.17079180141365133,
          "max": 0.17079180141365133,
          "median": 0.17079180141365133,
          "values": [
            0.17079180141365133
          ]
        },
        "jaccard": {
          "mean": 0.09600669797746281,
          "std": 0.0,
          "min": 0.09600669797746281,
          "max": 0.09600669797746281,
          "median": 0.09600669797746281,
          "values": [
            0.09600669797746281
          ]
        },
        "semantic_similarity": {
          "mean": 0.5254467269033194,
          "std": 0.0,
          "min": 0.5254467269033194,
          "max": 0.5254467269033194,
          "median": 0.5254467269033194,
          "values": [
            0.5254467269033194
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.7113333333333336,
          "std": 0.0,
          "min": 1.7113333333333336,
          "max": 1.7113333333333336,
          "median": 1.7113333333333336,
          "values": [
            1.7113333333333336
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_hf-qwen3-30b-thinking": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__hf-qwen3-30b-thinking__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.06619874367276073,
          "std": 0.0,
          "min": 0.06619874367276073,
          "max": 0.06619874367276073,
          "median": 0.06619874367276073,
          "values": [
            0.06619874367276073
          ]
        },
        "recall": {
          "mean": 0.43873223702430225,
          "std": 0.0,
          "min": 0.43873223702430225,
          "max": 0.43873223702430225,
          "median": 0.43873223702430225,
          "values": [
            0.43873223702430225
          ]
        },
        "f1_score": {
          "mean": 0.11161980964318186,
          "std": 0.0,
          "min": 0.11161980964318186,
          "max": 0.11161980964318186,
          "median": 0.11161980964318186,
          "values": [
            0.11161980964318186
          ]
        },
        "jaccard": {
          "mean": 0.06007680550816749,
          "std": 0.0,
          "min": 0.06007680550816749,
          "max": 0.06007680550816749,
          "median": 0.06007680550816749,
          "values": [
            0.06007680550816749
          ]
        },
        "semantic_similarity": {
          "mean": 0.4586498977150768,
          "std": 0.0,
          "min": 0.4586498977150768,
          "max": 0.4586498977150768,
          "median": 0.4586498977150768,
          "values": [
            0.4586498977150768
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.8035,
          "std": 0.0,
          "min": 0.8035,
          "max": 0.8035,
          "median": 0.8035,
          "values": [
            0.8035
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8666666666666667,
          "std": 0.0,
          "min": 0.8666666666666667,
          "max": 0.8666666666666667,
          "median": 0.8666666666666667,
          "values": [
            0.8666666666666667
          ]
        },
        "structural_format_match": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-llama3-8b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10537446616922469,
          "std": 0.0,
          "min": 0.10537446616922469,
          "max": 0.10537446616922469,
          "median": 0.10537446616922469,
          "values": [
            0.10537446616922469
          ]
        },
        "recall": {
          "mean": 0.3321391484752698,
          "std": 0.0,
          "min": 0.3321391484752698,
          "max": 0.3321391484752698,
          "median": 0.3321391484752698,
          "values": [
            0.3321391484752698
          ]
        },
        "f1_score": {
          "mean": 0.1531497419523496,
          "std": 0.0,
          "min": 0.1531497419523496,
          "max": 0.1531497419523496,
          "median": 0.1531497419523496,
          "values": [
            0.1531497419523496
          ]
        },
        "jaccard": {
          "mean": 0.0847984098093961,
          "std": 0.0,
          "min": 0.0847984098093961,
          "max": 0.0847984098093961,
          "median": 0.0847984098093961,
          "values": [
            0.0847984098093961
          ]
        },
        "semantic_similarity": {
          "mean": 0.5342562747793272,
          "std": 0.0,
          "min": 0.5342562747793272,
          "max": 0.5342562747793272,
          "median": 0.5342562747793272,
          "values": [
            0.5342562747793272
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.0806666666666667,
          "std": 0.0,
          "min": 1.0806666666666667,
          "max": 1.0806666666666667,
          "median": 1.0806666666666667,
          "values": [
            1.0806666666666667
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7300000000000001,
          "std": 0.0,
          "min": 0.7300000000000001,
          "max": 0.7300000000000001,
          "median": 0.7300000000000001,
          "values": [
            0.7300000000000001
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-llama3-8b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10478612299373015,
          "std": 0.0,
          "min": 0.10478612299373015,
          "max": 0.10478612299373015,
          "median": 0.10478612299373015,
          "values": [
            0.10478612299373015
          ]
        },
        "recall": {
          "mean": 0.33908495498513547,
          "std": 0.0,
          "min": 0.33908495498513547,
          "max": 0.33908495498513547,
          "median": 0.33908495498513547,
          "values": [
            0.33908495498513547
          ]
        },
        "f1_score": {
          "mean": 0.15450160147144312,
          "std": 0.0,
          "min": 0.15450160147144312,
          "max": 0.15450160147144312,
          "median": 0.15450160147144312,
          "values": [
            0.15450160147144312
          ]
        },
        "jaccard": {
          "mean": 0.0857823976586241,
          "std": 0.0,
          "min": 0.0857823976586241,
          "max": 0.0857823976586241,
          "median": 0.0857823976586241,
          "values": [
            0.0857823976586241
          ]
        },
        "semantic_similarity": {
          "mean": 0.5267234312975779,
          "std": 0.0,
          "min": 0.5267234312975779,
          "max": 0.5267234312975779,
          "median": 0.5267234312975779,
          "values": [
            0.5267234312975779
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.2633333333333332,
          "std": 0.0,
          "min": 1.2633333333333332,
          "max": 1.2633333333333332,
          "median": 1.2633333333333332,
          "values": [
            1.2633333333333332
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7216666666666667,
          "std": 0.0,
          "min": 0.7216666666666667,
          "max": 0.7216666666666667,
          "median": 0.7216666666666667,
          "values": [
            0.7216666666666667
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.08999826368648688,
          "std": 0.0,
          "min": 0.08999826368648688,
          "max": 0.08999826368648688,
          "median": 0.08999826368648688,
          "values": [
            0.08999826368648688
          ]
        },
        "recall": {
          "mean": 0.27067536678929693,
          "std": 0.0,
          "min": 0.27067536678929693,
          "max": 0.27067536678929693,
          "median": 0.27067536678929693,
          "values": [
            0.27067536678929693
          ]
        },
        "f1_score": {
          "mean": 0.12721664086706144,
          "std": 0.0,
          "min": 0.12721664086706144,
          "max": 0.12721664086706144,
          "median": 0.12721664086706144,
          "values": [
            0.12721664086706144
          ]
        },
        "jaccard": {
          "mean": 0.0695525443532205,
          "std": 0.0,
          "min": 0.0695525443532205,
          "max": 0.0695525443532205,
          "median": 0.0695525443532205,
          "values": [
            0.0695525443532205
          ]
        },
        "semantic_similarity": {
          "mean": 0.42756740694865586,
          "std": 0.0,
          "min": 0.42756740694865586,
          "max": 0.42756740694865586,
          "median": 0.42756740694865586,
          "values": [
            0.42756740694865586
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.745333333333333,
          "std": 0.0,
          "min": 1.745333333333333,
          "max": 1.745333333333333,
          "median": 1.745333333333333,
          "values": [
            1.745333333333333
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6316666666666666,
          "std": 0.0,
          "min": 0.6316666666666666,
          "max": 0.6316666666666666,
          "median": 0.6316666666666666,
          "values": [
            0.6316666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_u4b-llama3.2-1b": {
      "num_experiments": 2,
      "experiments": [
        "gmeg__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100",
        "gmeg_paper__u4b-llama3.2-1b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg_paper",
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0,
            0.0
          ]
        },
        "precision": {
          "mean": 0.08616170758962777,
          "std": 0.001975595118279029,
          "min": 0.08418611247134875,
          "max": 0.0881373027079068,
          "median": 0.08616170758962777,
          "values": [
            0.08418611247134875,
            0.0881373027079068
          ]
        },
        "recall": {
          "mean": 0.26626004257629965,
          "std": 0.007591614816211539,
          "min": 0.2586684277600881,
          "max": 0.27385165739251116,
          "median": 0.26626004257629965,
          "values": [
            0.2586684277600881,
            0.27385165739251116
          ]
        },
        "f1_score": {
          "mean": 0.12304966210213669,
          "std": 0.0027568668663380744,
          "min": 0.12029279523579862,
          "max": 0.12580652896847477,
          "median": 0.12304966210213669,
          "values": [
            0.12029279523579862,
            0.12580652896847477
          ]
        },
        "jaccard": {
          "mean": 0.06691641493022646,
          "std": 0.001621398521121649,
          "min": 0.0652950164091048,
          "max": 0.0685378134513481,
          "median": 0.06691641493022646,
          "values": [
            0.0652950164091048,
            0.0685378134513481
          ]
        },
        "semantic_similarity": {
          "mean": 0.46024371324805546,
          "std": 0.0008359648729674707,
          "min": 0.45940774837508797,
          "max": 0.4610796781210229,
          "median": 0.46024371324805546,
          "values": [
            0.4610796781210229,
            0.45940774837508797
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.6381666666666665,
          "std": 0.0,
          "min": 1.6381666666666665,
          "max": 1.6381666666666665,
          "median": 1.6381666666666665,
          "values": [
            1.6381666666666665
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.5225,
          "std": 0.0,
          "min": 0.5225,
          "max": 0.5225,
          "median": 0.5225,
          "values": [
            0.5225
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1285366835995709,
          "std": 0.0,
          "min": 0.1285366835995709,
          "max": 0.1285366835995709,
          "median": 0.1285366835995709,
          "values": [
            0.1285366835995709
          ]
        },
        "recall": {
          "mean": 0.3712485230943114,
          "std": 0.0,
          "min": 0.3712485230943114,
          "max": 0.3712485230943114,
          "median": 0.3712485230943114,
          "values": [
            0.3712485230943114
          ]
        },
        "f1_score": {
          "mean": 0.1826537246445808,
          "std": 0.0,
          "min": 0.1826537246445808,
          "max": 0.1826537246445808,
          "median": 0.1826537246445808,
          "values": [
            0.1826537246445808
          ]
        },
        "jaccard": {
          "mean": 0.10369909564428108,
          "std": 0.0,
          "min": 0.10369909564428108,
          "max": 0.10369909564428108,
          "median": 0.10369909564428108,
          "values": [
            0.10369909564428108
          ]
        },
        "semantic_similarity": {
          "mean": 0.5686166930478066,
          "std": 0.0,
          "min": 0.5686166930478066,
          "max": 0.5686166930478066,
          "median": 0.5686166930478066,
          "values": [
            0.5686166930478066
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.48,
          "std": 0.0,
          "min": 1.48,
          "max": 1.48,
          "median": 1.48,
          "values": [
            1.48
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7425,
          "std": 0.0,
          "min": 0.7425,
          "max": 0.7425,
          "median": 0.7425,
          "values": [
            0.7425
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-llama3.3-70b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.10859725201986996,
          "std": 0.0,
          "min": 0.10859725201986996,
          "max": 0.10859725201986996,
          "median": 0.10859725201986996,
          "values": [
            0.10859725201986996
          ]
        },
        "recall": {
          "mean": 0.34937610571185396,
          "std": 0.0,
          "min": 0.34937610571185396,
          "max": 0.34937610571185396,
          "median": 0.34937610571185396,
          "values": [
            0.34937610571185396
          ]
        },
        "f1_score": {
          "mean": 0.15933894110014646,
          "std": 0.0,
          "min": 0.15933894110014646,
          "max": 0.15933894110014646,
          "median": 0.15933894110014646,
          "values": [
            0.15933894110014646
          ]
        },
        "jaccard": {
          "mean": 0.08878555714359121,
          "std": 0.0,
          "min": 0.08878555714359121,
          "max": 0.08878555714359121,
          "median": 0.08878555714359121,
          "values": [
            0.08878555714359121
          ]
        },
        "semantic_similarity": {
          "mean": 0.5469943140354008,
          "std": 0.0,
          "min": 0.5469943140354008,
          "max": 0.5469943140354008,
          "median": 0.5469943140354008,
          "values": [
            0.5469943140354008
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.4945,
          "std": 0.0,
          "min": 1.4945,
          "max": 1.4945,
          "median": 1.4945,
          "values": [
            1.4945
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7116666666666666,
          "std": 0.0,
          "min": 0.7116666666666666,
          "max": 0.7116666666666666,
          "median": 0.7116666666666666,
          "values": [
            0.7116666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-mistral-7b__few-shot-456__gmeg_few_shot__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11578537894240647,
          "std": 0.0,
          "min": 0.11578537894240647,
          "max": 0.11578537894240647,
          "median": 0.11578537894240647,
          "values": [
            0.11578537894240647
          ]
        },
        "recall": {
          "mean": 0.39691623353839034,
          "std": 0.0,
          "min": 0.39691623353839034,
          "max": 0.39691623353839034,
          "median": 0.39691623353839034,
          "values": [
            0.39691623353839034
          ]
        },
        "f1_score": {
          "mean": 0.1707729036132271,
          "std": 0.0,
          "min": 0.1707729036132271,
          "max": 0.1707729036132271,
          "median": 0.1707729036132271,
          "values": [
            0.1707729036132271
          ]
        },
        "jaccard": {
          "mean": 0.09621973620240214,
          "std": 0.0,
          "min": 0.09621973620240214,
          "max": 0.09621973620240214,
          "median": 0.09621973620240214,
          "values": [
            0.09621973620240214
          ]
        },
        "semantic_similarity": {
          "mean": 0.4863689078018069,
          "std": 0.0,
          "min": 0.4863689078018069,
          "max": 0.4863689078018069,
          "median": 0.4863689078018069,
          "values": [
            0.4863689078018069
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.7301666666666664,
          "std": 0.0,
          "min": 1.7301666666666664,
          "max": 1.7301666666666664,
          "median": 1.7301666666666664,
          "values": [
            1.7301666666666664
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8341666666666667,
          "std": 0.0,
          "min": 0.8341666666666667,
          "max": 0.8341666666666667,
          "median": 0.8341666666666667,
          "values": [
            0.8341666666666667
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-mistral-7b__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09588343999711746,
          "std": 0.0,
          "min": 0.09588343999711746,
          "max": 0.09588343999711746,
          "median": 0.09588343999711746,
          "values": [
            0.09588343999711746
          ]
        },
        "recall": {
          "mean": 0.38020979066697336,
          "std": 0.0,
          "min": 0.38020979066697336,
          "max": 0.38020979066697336,
          "median": 0.38020979066697336,
          "values": [
            0.38020979066697336
          ]
        },
        "f1_score": {
          "mean": 0.1461540075876452,
          "std": 0.0,
          "min": 0.1461540075876452,
          "max": 0.1461540075876452,
          "median": 0.1461540075876452,
          "values": [
            0.1461540075876452
          ]
        },
        "jaccard": {
          "mean": 0.08080910737350303,
          "std": 0.0,
          "min": 0.08080910737350303,
          "max": 0.08080910737350303,
          "median": 0.08080910737350303,
          "values": [
            0.08080910737350303
          ]
        },
        "semantic_similarity": {
          "mean": 0.49559973377734423,
          "std": 0.0,
          "min": 0.49559973377734423,
          "max": 0.49559973377734423,
          "median": 0.49559973377734423,
          "values": [
            0.49559973377734423
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.2266666666666666,
          "std": 0.0,
          "min": 1.2266666666666666,
          "max": 1.2266666666666666,
          "median": 1.2266666666666666,
          "values": [
            1.2266666666666666
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7066666666666666,
          "std": 0.0,
          "min": 0.7066666666666666,
          "max": 0.7066666666666666,
          "median": 0.7066666666666666,
          "values": [
            0.7066666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_explaination_u4b-mistral-7b_ft_gmeg_explanation_ft": {
      "num_experiments": 1,
      "experiments": [
        "gmeg__u4b-mistral-7b_ft_gmeg_explanation_ft__zero-shot__gmeg_explaination__100__0p100"
      ],
      "setups": [
        "gmeg"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.3252198118008294,
          "std": 0.0,
          "min": 0.3252198118008294,
          "max": 0.3252198118008294,
          "median": 0.3252198118008294,
          "values": [
            0.3252198118008294
          ]
        },
        "recall": {
          "mean": 0.3299169805016207,
          "std": 0.0,
          "min": 0.3299169805016207,
          "max": 0.3299169805016207,
          "median": 0.3299169805016207,
          "values": [
            0.3299169805016207
          ]
        },
        "f1_score": {
          "mean": 0.31348241920638736,
          "std": 0.0,
          "min": 0.31348241920638736,
          "max": 0.31348241920638736,
          "median": 0.31348241920638736,
          "values": [
            0.31348241920638736
          ]
        },
        "jaccard": {
          "mean": 0.19661618599278566,
          "std": 0.0,
          "min": 0.19661618599278566,
          "max": 0.19661618599278566,
          "median": 0.19661618599278566,
          "values": [
            0.19661618599278566
          ]
        },
        "semantic_similarity": {
          "mean": 0.6538133403286338,
          "std": 0.0,
          "min": 0.6538133403286338,
          "max": 0.6538133403286338,
          "median": 0.6538133403286338,
          "values": [
            0.6538133403286338
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.1121666666666667,
          "std": 0.0,
          "min": 1.1121666666666667,
          "max": 1.1121666666666667,
          "median": 1.1121666666666667,
          "values": [
            1.1121666666666667
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7775,
          "std": 0.0,
          "min": 0.7775,
          "max": 0.7775,
          "median": 0.7775,
          "values": [
            0.7775
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "gmeg_few_shot_original_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11534583487662842,
          "std": 0.0,
          "min": 0.11534583487662842,
          "max": 0.11534583487662842,
          "median": 0.11534583487662842,
          "values": [
            0.11534583487662842
          ]
        },
        "recall": {
          "mean": 0.3602410515207706,
          "std": 0.0,
          "min": 0.3602410515207706,
          "max": 0.3602410515207706,
          "median": 0.3602410515207706,
          "values": [
            0.3602410515207706
          ]
        },
        "f1_score": {
          "mean": 0.1685442323280916,
          "std": 0.0,
          "min": 0.1685442323280916,
          "max": 0.1685442323280916,
          "median": 0.1685442323280916,
          "values": [
            0.1685442323280916
          ]
        },
        "jaccard": {
          "mean": 0.09463171337853234,
          "std": 0.0,
          "min": 0.09463171337853234,
          "max": 0.09463171337853234,
          "median": 0.09463171337853234,
          "values": [
            0.09463171337853234
          ]
        },
        "semantic_similarity": {
          "mean": 0.49668813809752466,
          "std": 0.0,
          "min": 0.49668813809752466,
          "max": 0.49668813809752466,
          "median": 0.49668813809752466,
          "values": [
            0.49668813809752466
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.4028333333333336,
          "std": 0.0,
          "min": 1.4028333333333336,
          "max": 1.4028333333333336,
          "median": 1.4028333333333336,
          "values": [
            1.4028333333333336
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7058333333333333,
          "std": 0.0,
          "min": 0.7058333333333333,
          "max": 0.7058333333333333,
          "median": 0.7058333333333333,
          "values": [
            0.7058333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.9939732620320855,
          "std": 0.0,
          "min": 0.9939732620320855,
          "max": 0.9939732620320855,
          "median": 0.9939732620320855,
          "values": [
            0.9939732620320855
          ]
        }
      }
    },
    "gmeg_basic_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__hf-mistral-nemo-12b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.16653098620827958,
          "std": 0.0,
          "min": 0.16653098620827958,
          "max": 0.16653098620827958,
          "median": 0.16653098620827958,
          "values": [
            0.16653098620827958
          ]
        },
        "recall": {
          "mean": 0.2932858193706418,
          "std": 0.0,
          "min": 0.2932858193706418,
          "max": 0.2932858193706418,
          "median": 0.2932858193706418,
          "values": [
            0.2932858193706418
          ]
        },
        "f1_score": {
          "mean": 0.1945179387881015,
          "std": 0.0,
          "min": 0.1945179387881015,
          "max": 0.1945179387881015,
          "median": 0.1945179387881015,
          "values": [
            0.1945179387881015
          ]
        },
        "jaccard": {
          "mean": 0.11120084179688913,
          "std": 0.0,
          "min": 0.11120084179688913,
          "max": 0.11120084179688913,
          "median": 0.11120084179688913,
          "values": [
            0.11120084179688913
          ]
        },
        "semantic_similarity": {
          "mean": 0.5141218559071422,
          "std": 0.0,
          "min": 0.5141218559071422,
          "max": 0.5141218559071422,
          "median": 0.5141218559071422,
          "values": [
            0.5141218559071422
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.3773333333333335,
          "std": 0.0,
          "min": 1.3773333333333335,
          "max": 1.3773333333333335,
          "median": 1.3773333333333335,
          "values": [
            1.3773333333333335
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8275,
          "std": 0.0,
          "min": 0.8275,
          "max": 0.8275,
          "median": 0.8275,
          "values": [
            0.8275
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.4102483278436294,
          "std": 0.0,
          "min": 0.4102483278436294,
          "max": 0.4102483278436294,
          "median": 0.4102483278436294,
          "values": [
            0.4102483278436294
          ]
        }
      }
    },
    "gmeg_few_shot_original_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__hf-qwen2.5-3b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0982685163310635,
          "std": 0.0,
          "min": 0.0982685163310635,
          "max": 0.0982685163310635,
          "median": 0.0982685163310635,
          "values": [
            0.0982685163310635
          ]
        },
        "recall": {
          "mean": 0.3532869886921498,
          "std": 0.0,
          "min": 0.3532869886921498,
          "max": 0.3532869886921498,
          "median": 0.3532869886921498,
          "values": [
            0.3532869886921498
          ]
        },
        "f1_score": {
          "mean": 0.14787739182203263,
          "std": 0.0,
          "min": 0.14787739182203263,
          "max": 0.14787739182203263,
          "median": 0.14787739182203263,
          "values": [
            0.14787739182203263
          ]
        },
        "jaccard": {
          "mean": 0.08129240193485901,
          "std": 0.0,
          "min": 0.08129240193485901,
          "max": 0.08129240193485901,
          "median": 0.08129240193485901,
          "values": [
            0.08129240193485901
          ]
        },
        "semantic_similarity": {
          "mean": 0.48781229396816345,
          "std": 0.0,
          "min": 0.48781229396816345,
          "max": 0.48781229396816345,
          "median": 0.48781229396816345,
          "values": [
            0.48781229396816345
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.1455,
          "std": 0.0,
          "min": 1.1455,
          "max": 1.1455,
          "median": 1.1455,
          "values": [
            1.1455
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6216666666666666,
          "std": 0.0,
          "min": 0.6216666666666666,
          "max": 0.6216666666666666,
          "median": 0.6216666666666666,
          "values": [
            0.6216666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.9859064736433157,
          "std": 0.0,
          "min": 0.9859064736433157,
          "max": 0.9859064736433157,
          "median": 0.9859064736433157,
          "values": [
            0.9859064736433157
          ]
        }
      }
    },
    "gmeg_basic_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__hf-qwen2.5-3b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.18316898454422095,
          "std": 0.0,
          "min": 0.18316898454422095,
          "max": 0.18316898454422095,
          "median": 0.18316898454422095,
          "values": [
            0.18316898454422095
          ]
        },
        "recall": {
          "mean": 0.24429637255947415,
          "std": 0.0,
          "min": 0.24429637255947415,
          "max": 0.24429637255947415,
          "median": 0.24429637255947415,
          "values": [
            0.24429637255947415
          ]
        },
        "f1_score": {
          "mean": 0.19483243416739132,
          "std": 0.0,
          "min": 0.19483243416739132,
          "max": 0.19483243416739132,
          "median": 0.19483243416739132,
          "values": [
            0.19483243416739132
          ]
        },
        "jaccard": {
          "mean": 0.11225558852135702,
          "std": 0.0,
          "min": 0.11225558852135702,
          "max": 0.11225558852135702,
          "median": 0.11225558852135702,
          "values": [
            0.11225558852135702
          ]
        },
        "semantic_similarity": {
          "mean": 0.4819990609586239,
          "std": 0.0,
          "min": 0.4819990609586239,
          "max": 0.4819990609586239,
          "median": 0.4819990609586239,
          "values": [
            0.4819990609586239
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5055,
          "std": 0.0,
          "min": 1.5055,
          "max": 1.5055,
          "median": 1.5055,
          "values": [
            1.5055
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7441666666666668,
          "std": 0.0,
          "min": 0.7441666666666668,
          "max": 0.7441666666666668,
          "median": 0.7441666666666668,
          "values": [
            0.7441666666666668
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.42361581295930323,
          "std": 0.0,
          "min": 0.42361581295930323,
          "max": 0.42361581295930323,
          "median": 0.42361581295930323,
          "values": [
            0.42361581295930323
          ]
        }
      }
    },
    "gmeg_few_shot_original_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3-8b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11629653123470433,
          "std": 0.0,
          "min": 0.11629653123470433,
          "max": 0.11629653123470433,
          "median": 0.11629653123470433,
          "values": [
            0.11629653123470433
          ]
        },
        "recall": {
          "mean": 0.3286627266774538,
          "std": 0.0,
          "min": 0.3286627266774538,
          "max": 0.3286627266774538,
          "median": 0.3286627266774538,
          "values": [
            0.3286627266774538
          ]
        },
        "f1_score": {
          "mean": 0.1646939794235943,
          "std": 0.0,
          "min": 0.1646939794235943,
          "max": 0.1646939794235943,
          "median": 0.1646939794235943,
          "values": [
            0.1646939794235943
          ]
        },
        "jaccard": {
          "mean": 0.09180649463146945,
          "std": 0.0,
          "min": 0.09180649463146945,
          "max": 0.09180649463146945,
          "median": 0.09180649463146945,
          "values": [
            0.09180649463146945
          ]
        },
        "semantic_similarity": {
          "mean": 0.5034227759391069,
          "std": 0.0,
          "min": 0.5034227759391069,
          "max": 0.5034227759391069,
          "median": 0.5034227759391069,
          "values": [
            0.5034227759391069
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.8246666666666667,
          "std": 0.0,
          "min": 0.8246666666666667,
          "max": 0.8246666666666667,
          "median": 0.8246666666666667,
          "values": [
            0.8246666666666667
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6458333333333335,
          "std": 0.0,
          "min": 0.6458333333333335,
          "max": 0.6458333333333335,
          "median": 0.6458333333333335,
          "values": [
            0.6458333333333335
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.9840740740740741,
          "std": 0.0,
          "min": 0.9840740740740741,
          "max": 0.9840740740740741,
          "median": 0.9840740740740741,
          "values": [
            0.9840740740740741
          ]
        }
      }
    },
    "gmeg_basic_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3-8b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.13412336132142902,
          "std": 0.0,
          "min": 0.13412336132142902,
          "max": 0.13412336132142902,
          "median": 0.13412336132142902,
          "values": [
            0.13412336132142902
          ]
        },
        "recall": {
          "mean": 0.26802137138088716,
          "std": 0.0,
          "min": 0.26802137138088716,
          "max": 0.26802137138088716,
          "median": 0.26802137138088716,
          "values": [
            0.26802137138088716
          ]
        },
        "f1_score": {
          "mean": 0.16935422342754966,
          "std": 0.0,
          "min": 0.16935422342754966,
          "max": 0.16935422342754966,
          "median": 0.16935422342754966,
          "values": [
            0.16935422342754966
          ]
        },
        "jaccard": {
          "mean": 0.09514077750810275,
          "std": 0.0,
          "min": 0.09514077750810275,
          "max": 0.09514077750810275,
          "median": 0.09514077750810275,
          "values": [
            0.09514077750810275
          ]
        },
        "semantic_similarity": {
          "mean": 0.5068688761256636,
          "std": 0.0,
          "min": 0.5068688761256636,
          "max": 0.5068688761256636,
          "median": 0.5068688761256636,
          "values": [
            0.5068688761256636
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.7861666666666668,
          "std": 0.0,
          "min": 0.7861666666666668,
          "max": 0.7861666666666668,
          "median": 0.7861666666666668,
          "values": [
            0.7861666666666668
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7316666666666666,
          "std": 0.0,
          "min": 0.7316666666666666,
          "max": 0.7316666666666666,
          "median": 0.7316666666666666,
          "values": [
            0.7316666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.2660764189742175,
          "std": 0.0,
          "min": 0.2660764189742175,
          "max": 0.2660764189742175,
          "median": 0.2660764189742175,
          "values": [
            0.2660764189742175
          ]
        }
      }
    },
    "gmeg_few_shot_original_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09777369094408789,
          "std": 0.0,
          "min": 0.09777369094408789,
          "max": 0.09777369094408789,
          "median": 0.09777369094408789,
          "values": [
            0.09777369094408789
          ]
        },
        "recall": {
          "mean": 0.2784053754524449,
          "std": 0.0,
          "min": 0.2784053754524449,
          "max": 0.2784053754524449,
          "median": 0.2784053754524449,
          "values": [
            0.2784053754524449
          ]
        },
        "f1_score": {
          "mean": 0.1382949847372993,
          "std": 0.0,
          "min": 0.1382949847372993,
          "max": 0.1382949847372993,
          "median": 0.1382949847372993,
          "values": [
            0.1382949847372993
          ]
        },
        "jaccard": {
          "mean": 0.07593057669509175,
          "std": 0.0,
          "min": 0.07593057669509175,
          "max": 0.07593057669509175,
          "median": 0.07593057669509175,
          "values": [
            0.07593057669509175
          ]
        },
        "semantic_similarity": {
          "mean": 0.3811633627023548,
          "std": 0.0,
          "min": 0.3811633627023548,
          "max": 0.3811633627023548,
          "median": 0.3811633627023548,
          "values": [
            0.3811633627023548
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.3329999999999997,
          "std": 0.0,
          "min": 1.3329999999999997,
          "max": 1.3329999999999997,
          "median": 1.3329999999999997,
          "values": [
            1.3329999999999997
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.565,
          "std": 0.0,
          "min": 0.565,
          "max": 0.565,
          "median": 0.565,
          "values": [
            0.565
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.949774572320238,
          "std": 0.0,
          "min": 0.949774572320238,
          "max": 0.949774572320238,
          "median": 0.949774572320238,
          "values": [
            0.949774572320238
          ]
        }
      }
    },
    "gmeg_basic_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3.2-1b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.09094990802899909,
          "std": 0.0,
          "min": 0.09094990802899909,
          "max": 0.09094990802899909,
          "median": 0.09094990802899909,
          "values": [
            0.09094990802899909
          ]
        },
        "recall": {
          "mean": 0.19459645234727632,
          "std": 0.0,
          "min": 0.19459645234727632,
          "max": 0.19459645234727632,
          "median": 0.19459645234727632,
          "values": [
            0.19459645234727632
          ]
        },
        "f1_score": {
          "mean": 0.11468630378624185,
          "std": 0.0,
          "min": 0.11468630378624185,
          "max": 0.11468630378624185,
          "median": 0.11468630378624185,
          "values": [
            0.11468630378624185
          ]
        },
        "jaccard": {
          "mean": 0.06226144043722152,
          "std": 0.0,
          "min": 0.06226144043722152,
          "max": 0.06226144043722152,
          "median": 0.06226144043722152,
          "values": [
            0.06226144043722152
          ]
        },
        "semantic_similarity": {
          "mean": 0.4205879895738326,
          "std": 0.0,
          "min": 0.4205879895738326,
          "max": 0.4205879895738326,
          "median": 0.4205879895738326,
          "values": [
            0.4205879895738326
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.6651666666666665,
          "std": 0.0,
          "min": 1.6651666666666665,
          "max": 1.6651666666666665,
          "median": 1.6651666666666665,
          "values": [
            1.6651666666666665
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7875,
          "std": 0.0,
          "min": 0.7875,
          "max": 0.7875,
          "median": 0.7875,
          "values": [
            0.7875
          ]
        },
        "structural_format_match": {
          "mean": 0.91,
          "std": 0.0,
          "min": 0.91,
          "max": 0.91,
          "median": 0.91,
          "values": [
            0.91
          ]
        },
        "original_text_mention": {
          "mean": 0.7313372442761521,
          "std": 0.0,
          "min": 0.7313372442761521,
          "max": 0.7313372442761521,
          "median": 0.7313372442761521,
          "values": [
            0.7313372442761521
          ]
        }
      }
    },
    "gmeg_few_shot_original_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3.3-70b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1155949405089411,
          "std": 0.0,
          "min": 0.1155949405089411,
          "max": 0.1155949405089411,
          "median": 0.1155949405089411,
          "values": [
            0.1155949405089411
          ]
        },
        "recall": {
          "mean": 0.41224016301355076,
          "std": 0.0,
          "min": 0.41224016301355076,
          "max": 0.41224016301355076,
          "median": 0.41224016301355076,
          "values": [
            0.41224016301355076
          ]
        },
        "f1_score": {
          "mean": 0.16670877671799855,
          "std": 0.0,
          "min": 0.16670877671799855,
          "max": 0.16670877671799855,
          "median": 0.16670877671799855,
          "values": [
            0.16670877671799855
          ]
        },
        "jaccard": {
          "mean": 0.09391348107290702,
          "std": 0.0,
          "min": 0.09391348107290702,
          "max": 0.09391348107290702,
          "median": 0.09391348107290702,
          "values": [
            0.09391348107290702
          ]
        },
        "semantic_similarity": {
          "mean": 0.5408527564443648,
          "std": 0.0,
          "min": 0.5408527564443648,
          "max": 0.5408527564443648,
          "median": 0.5408527564443648,
          "values": [
            0.5408527564443648
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.8734999999999999,
          "std": 0.0,
          "min": 0.8734999999999999,
          "max": 0.8734999999999999,
          "median": 0.8734999999999999,
          "values": [
            0.8734999999999999
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7616666666666666,
          "std": 0.0,
          "min": 0.7616666666666666,
          "max": 0.7616666666666666,
          "median": 0.7616666666666666,
          "values": [
            0.7616666666666666
          ]
        },
        "structural_format_match": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "original_text_mention": {
          "mean": 0.7313370641270258,
          "std": 0.0,
          "min": 0.7313370641270258,
          "max": 0.7313370641270258,
          "median": 0.7313370641270258,
          "values": [
            0.7313370641270258
          ]
        }
      }
    },
    "gmeg_basic_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-llama3.3-70b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1241289540300264,
          "std": 0.0,
          "min": 0.1241289540300264,
          "max": 0.1241289540300264,
          "median": 0.1241289540300264,
          "values": [
            0.1241289540300264
          ]
        },
        "recall": {
          "mean": 0.3172650134685309,
          "std": 0.0,
          "min": 0.3172650134685309,
          "max": 0.3172650134685309,
          "median": 0.3172650134685309,
          "values": [
            0.3172650134685309
          ]
        },
        "f1_score": {
          "mean": 0.15963878152546848,
          "std": 0.0,
          "min": 0.15963878152546848,
          "max": 0.15963878152546848,
          "median": 0.15963878152546848,
          "values": [
            0.15963878152546848
          ]
        },
        "jaccard": {
          "mean": 0.09029920966369975,
          "std": 0.0,
          "min": 0.09029920966369975,
          "max": 0.09029920966369975,
          "median": 0.09029920966369975,
          "values": [
            0.09029920966369975
          ]
        },
        "semantic_similarity": {
          "mean": 0.4718954822793603,
          "std": 0.0,
          "min": 0.4718954822793603,
          "max": 0.4718954822793603,
          "median": 0.4718954822793603,
          "values": [
            0.4718954822793603
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5045,
          "std": 0.0,
          "min": 1.5045,
          "max": 1.5045,
          "median": 1.5045,
          "values": [
            1.5045
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.8383333333333333,
          "std": 0.0,
          "min": 0.8383333333333333,
          "max": 0.8383333333333333,
          "median": 0.8383333333333333,
          "values": [
            0.8383333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.89,
          "std": 0.0,
          "min": 0.89,
          "max": 0.89,
          "median": 0.89,
          "values": [
            0.89
          ]
        },
        "original_text_mention": {
          "mean": 0.4469594710879274,
          "std": 0.0,
          "min": 0.4469594710879274,
          "max": 0.4469594710879274,
          "median": 0.4469594710879274,
          "values": [
            0.4469594710879274
          ]
        }
      }
    },
    "gmeg_few_shot_original_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-mistral-7b__few-shot-456__gmeg_few_shot_original__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.1102378207457716,
          "std": 0.0,
          "min": 0.1102378207457716,
          "max": 0.1102378207457716,
          "median": 0.1102378207457716,
          "values": [
            0.1102378207457716
          ]
        },
        "recall": {
          "mean": 0.34835267746018006,
          "std": 0.0,
          "min": 0.34835267746018006,
          "max": 0.34835267746018006,
          "median": 0.34835267746018006,
          "values": [
            0.34835267746018006
          ]
        },
        "f1_score": {
          "mean": 0.15953902044230595,
          "std": 0.0,
          "min": 0.15953902044230595,
          "max": 0.15953902044230595,
          "median": 0.15953902044230595,
          "values": [
            0.15953902044230595
          ]
        },
        "jaccard": {
          "mean": 0.08900780796416687,
          "std": 0.0,
          "min": 0.08900780796416687,
          "max": 0.08900780796416687,
          "median": 0.08900780796416687,
          "values": [
            0.08900780796416687
          ]
        },
        "semantic_similarity": {
          "mean": 0.45694554714486,
          "std": 0.0,
          "min": 0.45694554714486,
          "max": 0.45694554714486,
          "median": 0.45694554714486,
          "values": [
            0.45694554714486
          ]
        },
        "bullet_point_ratio": {
          "mean": 1.5923333333333334,
          "std": 0.0,
          "min": 1.5923333333333334,
          "max": 1.5923333333333334,
          "median": 1.5923333333333334,
          "values": [
            1.5923333333333334
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.6808333333333334,
          "std": 0.0,
          "min": 0.6808333333333334,
          "max": 0.6808333333333334,
          "median": 0.6808333333333334,
          "values": [
            0.6808333333333334
          ]
        },
        "structural_format_match": {
          "mean": 0.9,
          "std": 0.0,
          "min": 0.9,
          "max": 0.9,
          "median": 0.9,
          "values": [
            0.9
          ]
        },
        "original_text_mention": {
          "mean": 0.6453567466202336,
          "std": 0.0,
          "min": 0.6453567466202336,
          "max": 0.6453567466202336,
          "median": 0.6453567466202336,
          "values": [
            0.6453567466202336
          ]
        }
      }
    },
    "gmeg_basic_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "gmeg_ours__u4b-mistral-7b__zero-shot__gmeg_basic__100__0p100"
      ],
      "setups": [
        "gmeg_ours"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.11292639877345302,
          "std": 0.0,
          "min": 0.11292639877345302,
          "max": 0.11292639877345302,
          "median": 0.11292639877345302,
          "values": [
            0.11292639877345302
          ]
        },
        "recall": {
          "mean": 0.2977708339931752,
          "std": 0.0,
          "min": 0.2977708339931752,
          "max": 0.2977708339931752,
          "median": 0.2977708339931752,
          "values": [
            0.2977708339931752
          ]
        },
        "f1_score": {
          "mean": 0.1548593250861159,
          "std": 0.0,
          "min": 0.1548593250861159,
          "max": 0.1548593250861159,
          "median": 0.1548593250861159,
          "values": [
            0.1548593250861159
          ]
        },
        "jaccard": {
          "mean": 0.0861344346935833,
          "std": 0.0,
          "min": 0.0861344346935833,
          "max": 0.0861344346935833,
          "median": 0.0861344346935833,
          "values": [
            0.0861344346935833
          ]
        },
        "semantic_similarity": {
          "mean": 0.4819056508410722,
          "std": 0.0,
          "min": 0.4819056508410722,
          "max": 0.4819056508410722,
          "median": 0.4819056508410722,
          "values": [
            0.4819056508410722
          ]
        },
        "bullet_point_ratio": {
          "mean": 0.505,
          "std": 0.0,
          "min": 0.505,
          "max": 0.505,
          "median": 0.505,
          "values": [
            0.505
          ]
        },
        "correction_terminology_recall": {
          "mean": 0.7858333333333333,
          "std": 0.0,
          "min": 0.7858333333333333,
          "max": 0.7858333333333333,
          "median": 0.7858333333333333,
          "values": [
            0.7858333333333333
          ]
        },
        "structural_format_match": {
          "mean": 0.3,
          "std": 0.0,
          "min": 0.3,
          "max": 0.3,
          "median": 0.3,
          "values": [
            0.3
          ]
        },
        "original_text_mention": {
          "mean": 0.491661741270094,
          "std": 0.0,
          "min": 0.491661741270094,
          "max": 0.491661741270094,
          "median": 0.491661741270094,
          "values": [
            0.491661741270094
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "recall": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "f1_score": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "jaccard": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "semantic_similarity": {
          "mean": 0.47510350450873373,
          "std": 0.0,
          "min": 0.47510350450873373,
          "max": 0.47510350450873373,
          "median": 0.47510350450873373,
          "values": [
            0.47510350450873373
          ]
        },
        "answer_correctness": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "strict_format_compliance": {
          "mean": 0.3719999999999999,
          "std": 0.0,
          "min": 0.3719999999999999,
          "max": 0.3719999999999999,
          "median": 0.3719999999999999,
          "values": [
            0.3719999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.88,
          "std": 0.0,
          "min": 0.88,
          "max": 0.88,
          "median": 0.88,
          "values": [
            0.88
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "precision": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "recall": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "f1_score": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "jaccard": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "semantic_similarity": {
          "mean": 0.5360703114420176,
          "std": 0.0,
          "min": 0.5360703114420176,
          "max": 0.5360703114420176,
          "median": 0.5360703114420176,
          "values": [
            0.5360703114420176
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 0.5479999999999999,
          "std": 0.0,
          "min": 0.5479999999999999,
          "max": 0.5479999999999999,
          "median": 0.5479999999999999,
          "values": [
            0.5479999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "precision": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "jaccard": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "semantic_similarity": {
          "mean": 0.566428534835577,
          "std": 0.0,
          "min": 0.566428534835577,
          "max": 0.566428534835577,
          "median": 0.566428534835577,
          "values": [
            0.566428534835577
          ]
        },
        "answer_correctness": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7799999999999998,
          "std": 0.0,
          "min": 0.7799999999999998,
          "max": 0.7799999999999998,
          "median": 0.7799999999999998,
          "values": [
            0.7799999999999998
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.98,
          "std": 0.0,
          "min": 0.98,
          "max": 0.98,
          "median": 0.98,
          "values": [
            0.98
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "precision": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "recall": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "f1_score": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "jaccard": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "semantic_similarity": {
          "mean": 0.8938113880157471,
          "std": 0.0,
          "min": 0.8938113880157471,
          "max": 0.8938113880157471,
          "median": 0.8938113880157471,
          "values": [
            0.8938113880157471
          ]
        },
        "answer_correctness": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7999999999999998,
          "std": 0.0,
          "min": 0.7999999999999998,
          "max": 0.7999999999999998,
          "median": 0.7999999999999998,
          "values": [
            0.7999999999999998
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0008295723152653063,
          "std": 0.0,
          "min": 0.0008295723152653063,
          "max": 0.0008295723152653063,
          "median": 0.0008295723152653063,
          "values": [
            0.0008295723152653063
          ]
        },
        "recall": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "f1_score": {
          "mean": 0.001634911296259006,
          "std": 0.0,
          "min": 0.001634911296259006,
          "max": 0.001634911296259006,
          "median": 0.001634911296259006,
          "values": [
            0.001634911296259006
          ]
        },
        "jaccard": {
          "mean": 0.0008295723152653063,
          "std": 0.0,
          "min": 0.0008295723152653063,
          "max": 0.0008295723152653063,
          "median": 0.0008295723152653063,
          "values": [
            0.0008295723152653063
          ]
        },
        "semantic_similarity": {
          "mean": 0.0827535550808534,
          "std": 0.0,
          "min": 0.0827535550808534,
          "max": 0.0827535550808534,
          "median": 0.0827535550808534,
          "values": [
            0.0827535550808534
          ]
        },
        "answer_correctness": {
          "mean": 0.19,
          "std": 0.0,
          "min": 0.19,
          "max": 0.19,
          "median": 0.19,
          "values": [
            0.19
          ]
        },
        "strict_format_compliance": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5712407356500626,
          "std": 0.0,
          "min": 0.5712407356500626,
          "max": 0.5712407356500626,
          "median": 0.5712407356500626,
          "values": [
            0.5712407356500626
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 0.8820000000000001,
          "std": 0.0,
          "min": 0.8820000000000001,
          "max": 0.8820000000000001,
          "median": 0.8820000000000001,
          "values": [
            0.8820000000000001
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "precision": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "recall": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "f1_score": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "jaccard": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "semantic_similarity": {
          "mean": 0.6201044678688049,
          "std": 0.0,
          "min": 0.6201044678688049,
          "max": 0.6201044678688049,
          "median": 0.6201044678688049,
          "values": [
            0.6201044678688049
          ]
        },
        "answer_correctness": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0004881729019660054,
          "std": 0.0,
          "min": 0.0004881729019660054,
          "max": 0.0004881729019660054,
          "median": 0.0004881729019660054,
          "values": [
            0.0004881729019660054
          ]
        },
        "recall": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "f1_score": {
          "mean": 0.0009590556730091613,
          "std": 0.0,
          "min": 0.0009590556730091613,
          "max": 0.0009590556730091613,
          "median": 0.0009590556730091613,
          "values": [
            0.0009590556730091613
          ]
        },
        "jaccard": {
          "mean": 0.0004881729019660054,
          "std": 0.0,
          "min": 0.0004881729019660054,
          "max": 0.0004881729019660054,
          "median": 0.0004881729019660054,
          "values": [
            0.0004881729019660054
          ]
        },
        "semantic_similarity": {
          "mean": 0.11679093430982902,
          "std": 0.0,
          "min": 0.11679093430982902,
          "max": 0.11679093430982902,
          "median": 0.11679093430982902,
          "values": [
            0.11679093430982902
          ]
        },
        "answer_correctness": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        },
        "strict_format_compliance": {
          "mean": 0.032,
          "std": 0.0,
          "min": 0.032,
          "max": 0.032,
          "median": 0.032,
          "values": [
            0.032
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.08,
          "std": 0.0,
          "min": 0.08,
          "max": 0.08,
          "median": 0.08,
          "values": [
            0.08
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "precision": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "recall": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "f1_score": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "jaccard": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "semantic_similarity": {
          "mean": 0.7084457975625992,
          "std": 0.0,
          "min": 0.7084457975625992,
          "max": 0.7084457975625992,
          "median": 0.7084457975625992,
          "values": [
            0.7084457975625992
          ]
        },
        "answer_correctness": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "strict_format_compliance": {
          "mean": 0.8220000000000001,
          "std": 0.0,
          "min": 0.8220000000000001,
          "max": 0.8220000000000001,
          "median": 0.8220000000000001,
          "values": [
            0.8220000000000001
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "precision": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "recall": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "f1_score": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "jaccard": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "semantic_similarity": {
          "mean": 0.7666562902927399,
          "std": 0.0,
          "min": 0.7666562902927399,
          "max": 0.7666562902927399,
          "median": 0.7666562902927399,
          "values": [
            0.7666562902927399
          ]
        },
        "answer_correctness": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "strict_format_compliance": {
          "mean": 0.8019999999999999,
          "std": 0.0,
          "min": 0.8019999999999999,
          "max": 0.8019999999999999,
          "median": 0.8019999999999999,
          "values": [
            0.8019999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "precision": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "recall": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "f1_score": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "jaccard": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "semantic_similarity": {
          "mean": 0.8741695193341001,
          "std": 0.0,
          "min": 0.8741695193341001,
          "max": 0.8741695193341001,
          "median": 0.8741695193341001,
          "values": [
            0.8741695193341001
          ]
        },
        "answer_correctness": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7639999999999999,
          "std": 0.0,
          "min": 0.7639999999999999,
          "max": 0.7639999999999999,
          "median": 0.7639999999999999,
          "values": [
            0.7639999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "precision": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "recall": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "f1_score": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "jaccard": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "semantic_similarity": {
          "mean": 0.8586994200944901,
          "std": 0.0,
          "min": 0.8586994200944901,
          "max": 0.8586994200944901,
          "median": 0.8586994200944901,
          "values": [
            0.8586994200944901
          ]
        },
        "answer_correctness": {
          "mean": 0.63,
          "std": 0.0,
          "min": 0.63,
          "max": 0.63,
          "median": 0.63,
          "values": [
            0.63
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7999999999999998,
          "std": 0.0,
          "min": 0.7999999999999998,
          "max": 0.7999999999999998,
          "median": 0.7999999999999998,
          "values": [
            0.7999999999999998
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "precision": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "recall": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "f1_score": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "jaccard": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "semantic_similarity": {
          "mean": 0.933139499425888,
          "std": 0.0,
          "min": 0.933139499425888,
          "max": 0.933139499425888,
          "median": 0.933139499425888,
          "values": [
            0.933139499425888
          ]
        },
        "answer_correctness": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "strict_format_compliance": {
          "mean": 0.8039999999999998,
          "std": 0.0,
          "min": 0.8039999999999998,
          "max": 0.8039999999999998,
          "median": 0.8039999999999998,
          "values": [
            0.8039999999999998
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "precision": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "recall": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "f1_score": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "jaccard": {
          "mean": 0.13,
          "std": 0.0,
          "min": 0.13,
          "max": 0.13,
          "median": 0.13,
          "values": [
            0.13
          ]
        },
        "semantic_similarity": {
          "mean": 0.868618765771389,
          "std": 0.0,
          "min": 0.868618765771389,
          "max": 0.868618765771389,
          "median": 0.868618765771389,
          "values": [
            0.868618765771389
          ]
        },
        "answer_correctness": {
          "mean": 0.85,
          "std": 0.0,
          "min": 0.85,
          "max": 0.85,
          "median": 0.85,
          "values": [
            0.85
          ]
        },
        "strict_format_compliance": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.9840000000000001,
          "std": 0.0,
          "min": 0.9840000000000001,
          "max": 0.9840000000000001,
          "median": 0.9840000000000001,
          "values": [
            0.9840000000000001
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00016877937615724257,
          "std": 0.0,
          "min": 0.00016877937615724257,
          "max": 0.00016877937615724257,
          "median": 0.00016877937615724257,
          "values": [
            0.00016877937615724257
          ]
        },
        "recall": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "f1_score": {
          "mean": 0.00033473389355742297,
          "std": 0.0,
          "min": 0.00033473389355742297,
          "max": 0.00033473389355742297,
          "median": 0.00033473389355742297,
          "values": [
            0.00033473389355742297
          ]
        },
        "jaccard": {
          "mean": 0.00016877937615724257,
          "std": 0.0,
          "min": 0.00016877937615724257,
          "max": 0.00016877937615724257,
          "median": 0.00016877937615724257,
          "values": [
            0.00016877937615724257
          ]
        },
        "semantic_similarity": {
          "mean": 0.25781503386795523,
          "std": 0.0,
          "min": 0.25781503386795523,
          "max": 0.25781503386795523,
          "median": 0.25781503386795523,
          "values": [
            0.25781503386795523
          ]
        },
        "answer_correctness": {
          "mean": 0.07,
          "std": 0.0,
          "min": 0.07,
          "max": 0.07,
          "median": 0.07,
          "values": [
            0.07
          ]
        },
        "strict_format_compliance": {
          "mean": 0.16,
          "std": 0.0,
          "min": 0.16,
          "max": 0.16,
          "median": 0.16,
          "values": [
            0.16
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.414,
          "std": 0.0,
          "min": 0.414,
          "max": 0.414,
          "median": 0.414,
          "values": [
            0.414
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "strict_format_compliance": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.08,
          "std": 0.0,
          "min": 0.08,
          "max": 0.08,
          "median": 0.08,
          "values": [
            0.08
          ]
        },
        "precision": {
          "mean": 0.08125,
          "std": 0.0,
          "min": 0.08125,
          "max": 0.08125,
          "median": 0.08125,
          "values": [
            0.08125
          ]
        },
        "recall": {
          "mean": 0.09,
          "std": 0.0,
          "min": 0.09,
          "max": 0.09,
          "median": 0.09,
          "values": [
            0.09
          ]
        },
        "f1_score": {
          "mean": 0.08222222222222221,
          "std": 0.0,
          "min": 0.08222222222222221,
          "max": 0.08222222222222221,
          "median": 0.08222222222222221,
          "values": [
            0.08222222222222221
          ]
        },
        "jaccard": {
          "mean": 0.08125,
          "std": 0.0,
          "min": 0.08125,
          "max": 0.08125,
          "median": 0.08125,
          "values": [
            0.08125
          ]
        },
        "semantic_similarity": {
          "mean": 0.5798214510083198,
          "std": 0.0,
          "min": 0.5798214510083198,
          "max": 0.5798214510083198,
          "median": 0.5798214510083198,
          "values": [
            0.5798214510083198
          ]
        },
        "answer_correctness": {
          "mean": 0.09,
          "std": 0.0,
          "min": 0.09,
          "max": 0.09,
          "median": 0.09,
          "values": [
            0.09
          ]
        },
        "strict_format_compliance": {
          "mean": 0.9359999999999999,
          "std": 0.0,
          "min": 0.9359999999999999,
          "max": 0.9359999999999999,
          "median": 0.9359999999999999,
          "values": [
            0.9359999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.96,
          "std": 0.0,
          "min": 0.96,
          "max": 0.96,
          "median": 0.96,
          "values": [
            0.96
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "precision": {
          "mean": 0.645,
          "std": 0.0,
          "min": 0.645,
          "max": 0.645,
          "median": 0.645,
          "values": [
            0.645
          ]
        },
        "recall": {
          "mean": 0.67,
          "std": 0.0,
          "min": 0.67,
          "max": 0.67,
          "median": 0.67,
          "values": [
            0.67
          ]
        },
        "f1_score": {
          "mean": 0.6533333333333333,
          "std": 0.0,
          "min": 0.6533333333333333,
          "max": 0.6533333333333333,
          "median": 0.6533333333333333,
          "values": [
            0.6533333333333333
          ]
        },
        "jaccard": {
          "mean": 0.645,
          "std": 0.0,
          "min": 0.645,
          "max": 0.645,
          "median": 0.645,
          "values": [
            0.645
          ]
        },
        "semantic_similarity": {
          "mean": 0.8582887753285467,
          "std": 0.0,
          "min": 0.8582887753285467,
          "max": 0.8582887753285467,
          "median": 0.8582887753285467,
          "values": [
            0.8582887753285467
          ]
        },
        "answer_correctness": {
          "mean": 0.73,
          "std": 0.0,
          "min": 0.73,
          "max": 0.73,
          "median": 0.73,
          "values": [
            0.73
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7079999999999999,
          "std": 0.0,
          "min": 0.7079999999999999,
          "max": 0.7079999999999999,
          "median": 0.7079999999999999,
          "values": [
            0.7079999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.9319999999999999,
          "std": 0.0,
          "min": 0.9319999999999999,
          "max": 0.9319999999999999,
          "median": 0.9319999999999999,
          "values": [
            0.9319999999999999
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00010752688172043012,
          "std": 0.0,
          "min": 0.00010752688172043012,
          "max": 0.00010752688172043012,
          "median": 0.00010752688172043012,
          "values": [
            0.00010752688172043012
          ]
        },
        "recall": {
          "mean": 0.01,
          "std": 0.0,
          "min": 0.01,
          "max": 0.01,
          "median": 0.01,
          "values": [
            0.01
          ]
        },
        "f1_score": {
          "mean": 0.00021276595744680854,
          "std": 0.0,
          "min": 0.00021276595744680854,
          "max": 0.00021276595744680854,
          "median": 0.00021276595744680854,
          "values": [
            0.00021276595744680854
          ]
        },
        "jaccard": {
          "mean": 0.00010752688172043012,
          "std": 0.0,
          "min": 0.00010752688172043012,
          "max": 0.00010752688172043012,
          "median": 0.00010752688172043012,
          "values": [
            0.00010752688172043012
          ]
        },
        "semantic_similarity": {
          "mean": 0.3501268028933555,
          "std": 0.0,
          "min": 0.3501268028933555,
          "max": 0.3501268028933555,
          "median": 0.3501268028933555,
          "values": [
            0.3501268028933555
          ]
        },
        "answer_correctness": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        },
        "strict_format_compliance": {
          "mean": 0.030999999999999996,
          "std": 0.0,
          "min": 0.030999999999999996,
          "max": 0.030999999999999996,
          "median": 0.030999999999999996,
          "values": [
            0.030999999999999996
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.30199999999999994,
          "std": 0.0,
          "min": 0.30199999999999994,
          "max": 0.30199999999999994,
          "median": 0.30199999999999994,
          "values": [
            0.30199999999999994
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.003968253968253968,
          "std": 0.0,
          "min": 0.003968253968253968,
          "max": 0.003968253968253968,
          "median": 0.003968253968253968,
          "values": [
            0.003968253968253968
          ]
        },
        "recall": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "f1_score": {
          "mean": 0.006999999999999999,
          "std": 0.0,
          "min": 0.006999999999999999,
          "max": 0.006999999999999999,
          "median": 0.006999999999999999,
          "values": [
            0.006999999999999999
          ]
        },
        "jaccard": {
          "mean": 0.003968253968253968,
          "std": 0.0,
          "min": 0.003968253968253968,
          "max": 0.003968253968253968,
          "median": 0.003968253968253968,
          "values": [
            0.003968253968253968
          ]
        },
        "semantic_similarity": {
          "mean": 0.4005956416297704,
          "std": 0.0,
          "min": 0.4005956416297704,
          "max": 0.4005956416297704,
          "median": 0.4005956416297704,
          "values": [
            0.4005956416297704
          ]
        },
        "answer_correctness": {
          "mean": 0.64,
          "std": 0.0,
          "min": 0.64,
          "max": 0.64,
          "median": 0.64,
          "values": [
            0.64
          ]
        },
        "strict_format_compliance": {
          "mean": 0.102,
          "std": 0.0,
          "min": 0.102,
          "max": 0.102,
          "median": 0.102,
          "values": [
            0.102
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.38199999999999995,
          "std": 0.0,
          "min": 0.38199999999999995,
          "max": 0.38199999999999995,
          "median": 0.38199999999999995,
          "values": [
            0.38199999999999995
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "recall": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "f1_score": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "jaccard": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "semantic_similarity": {
          "mean": 0.44549427420832216,
          "std": 0.0,
          "min": 0.44549427420832216,
          "max": 0.44549427420832216,
          "median": 0.44549427420832216,
          "values": [
            0.44549427420832216
          ]
        },
        "answer_correctness": {
          "mean": 0.79,
          "std": 0.0,
          "min": 0.79,
          "max": 0.79,
          "median": 0.79,
          "values": [
            0.79
          ]
        },
        "strict_format_compliance": {
          "mean": 0.165,
          "std": 0.0,
          "min": 0.165,
          "max": 0.165,
          "median": 0.165,
          "values": [
            0.165
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.40399999999999997,
          "std": 0.0,
          "min": 0.40399999999999997,
          "max": 0.40399999999999997,
          "median": 0.40399999999999997,
          "values": [
            0.40399999999999997
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00588113777212776,
          "std": 0.0,
          "min": 0.00588113777212776,
          "max": 0.00588113777212776,
          "median": 0.00588113777212776,
          "values": [
            0.00588113777212776
          ]
        },
        "recall": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "f1_score": {
          "mean": 0.01033094017094017,
          "std": 0.0,
          "min": 0.01033094017094017,
          "max": 0.01033094017094017,
          "median": 0.01033094017094017,
          "values": [
            0.01033094017094017
          ]
        },
        "jaccard": {
          "mean": 0.00588113777212776,
          "std": 0.0,
          "min": 0.00588113777212776,
          "max": 0.00588113777212776,
          "median": 0.00588113777212776,
          "values": [
            0.00588113777212776
          ]
        },
        "semantic_similarity": {
          "mean": 0.41776324043050406,
          "std": 0.0,
          "min": 0.41776324043050406,
          "max": 0.41776324043050406,
          "median": 0.41776324043050406,
          "values": [
            0.41776324043050406
          ]
        },
        "answer_correctness": {
          "mean": 0.6,
          "std": 0.0,
          "min": 0.6,
          "max": 0.6,
          "median": 0.6,
          "values": [
            0.6
          ]
        },
        "strict_format_compliance": {
          "mean": 0.16899999999999998,
          "std": 0.0,
          "min": 0.16899999999999998,
          "max": 0.16899999999999998,
          "median": 0.16899999999999998,
          "values": [
            0.16899999999999998
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.462,
          "std": 0.0,
          "min": 0.462,
          "max": 0.462,
          "median": 0.462,
          "values": [
            0.462
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.009107142857142855,
          "std": 0.0,
          "min": 0.009107142857142855,
          "max": 0.009107142857142855,
          "median": 0.009107142857142855,
          "values": [
            0.009107142857142855
          ]
        },
        "recall": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "f1_score": {
          "mean": 0.01579365079365079,
          "std": 0.0,
          "min": 0.01579365079365079,
          "max": 0.01579365079365079,
          "median": 0.01579365079365079,
          "values": [
            0.01579365079365079
          ]
        },
        "jaccard": {
          "mean": 0.009107142857142855,
          "std": 0.0,
          "min": 0.009107142857142855,
          "max": 0.009107142857142855,
          "median": 0.009107142857142855,
          "values": [
            0.009107142857142855
          ]
        },
        "semantic_similarity": {
          "mean": 0.7479457410424948,
          "std": 0.0,
          "min": 0.7479457410424948,
          "max": 0.7479457410424948,
          "median": 0.7479457410424948,
          "values": [
            0.7479457410424948
          ]
        },
        "answer_correctness": {
          "mean": 0.9,
          "std": 0.0,
          "min": 0.9,
          "max": 0.9,
          "median": 0.9,
          "values": [
            0.9
          ]
        },
        "strict_format_compliance": {
          "mean": 0.34600000000000003,
          "std": 0.0,
          "min": 0.34600000000000003,
          "max": 0.34600000000000003,
          "median": 0.34600000000000003,
          "values": [
            0.34600000000000003
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.8180000000000001,
          "std": 0.0,
          "min": 0.8180000000000001,
          "max": 0.8180000000000001,
          "median": 0.8180000000000001,
          "values": [
            0.8180000000000001
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.044075493950493955,
          "std": 0.0,
          "min": 0.044075493950493955,
          "max": 0.044075493950493955,
          "median": 0.044075493950493955,
          "values": [
            0.044075493950493955
          ]
        },
        "recall": {
          "mean": 0.32,
          "std": 0.0,
          "min": 0.32,
          "max": 0.32,
          "median": 0.32,
          "values": [
            0.32
          ]
        },
        "f1_score": {
          "mean": 0.07575177749270576,
          "std": 0.0,
          "min": 0.07575177749270576,
          "max": 0.07575177749270576,
          "median": 0.07575177749270576,
          "values": [
            0.07575177749270576
          ]
        },
        "jaccard": {
          "mean": 0.044075493950493955,
          "std": 0.0,
          "min": 0.044075493950493955,
          "max": 0.044075493950493955,
          "median": 0.044075493950493955,
          "values": [
            0.044075493950493955
          ]
        },
        "semantic_similarity": {
          "mean": 0.5152227373234928,
          "std": 0.0,
          "min": 0.5152227373234928,
          "max": 0.5152227373234928,
          "median": 0.5152227373234928,
          "values": [
            0.5152227373234928
          ]
        },
        "answer_correctness": {
          "mean": 0.87,
          "std": 0.0,
          "min": 0.87,
          "max": 0.87,
          "median": 0.87,
          "values": [
            0.87
          ]
        },
        "strict_format_compliance": {
          "mean": 0.114,
          "std": 0.0,
          "min": 0.114,
          "max": 0.114,
          "median": 0.114,
          "values": [
            0.114
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.452,
          "std": 0.0,
          "min": 0.452,
          "max": 0.452,
          "median": 0.452,
          "values": [
            0.452
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        },
        "precision": {
          "mean": 0.12042107027770758,
          "std": 0.0,
          "min": 0.12042107027770758,
          "max": 0.12042107027770758,
          "median": 0.12042107027770758,
          "values": [
            0.12042107027770758
          ]
        },
        "recall": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "f1_score": {
          "mean": 0.1208350931653932,
          "std": 0.0,
          "min": 0.1208350931653932,
          "max": 0.1208350931653932,
          "median": 0.1208350931653932,
          "values": [
            0.1208350931653932
          ]
        },
        "jaccard": {
          "mean": 0.12042107027770758,
          "std": 0.0,
          "min": 0.12042107027770758,
          "max": 0.12042107027770758,
          "median": 0.12042107027770758,
          "values": [
            0.12042107027770758
          ]
        },
        "semantic_similarity": {
          "mean": 0.1547814832930453,
          "std": 0.0,
          "min": 0.1547814832930453,
          "max": 0.1547814832930453,
          "median": 0.1547814832930453,
          "values": [
            0.1547814832930453
          ]
        },
        "answer_correctness": {
          "mean": 0.2,
          "std": 0.0,
          "min": 0.2,
          "max": 0.2,
          "median": 0.2,
          "values": [
            0.2
          ]
        },
        "strict_format_compliance": {
          "mean": 0.13200000000000003,
          "std": 0.0,
          "min": 0.13200000000000003,
          "max": 0.13200000000000003,
          "median": 0.13200000000000003,
          "values": [
            0.13200000000000003
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.18,
          "std": 0.0,
          "min": 0.18,
          "max": 0.18,
          "median": 0.18,
          "values": [
            0.18
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04054884172345928,
          "std": 0.0,
          "min": 0.04054884172345928,
          "max": 0.04054884172345928,
          "median": 0.04054884172345928,
          "values": [
            0.04054884172345928
          ]
        },
        "recall": {
          "mean": 0.09,
          "std": 0.0,
          "min": 0.09,
          "max": 0.09,
          "median": 0.09,
          "values": [
            0.09
          ]
        },
        "f1_score": {
          "mean": 0.041084690612792414,
          "std": 0.0,
          "min": 0.041084690612792414,
          "max": 0.041084690612792414,
          "median": 0.041084690612792414,
          "values": [
            0.041084690612792414
          ]
        },
        "jaccard": {
          "mean": 0.04054884172345928,
          "std": 0.0,
          "min": 0.04054884172345928,
          "max": 0.04054884172345928,
          "median": 0.04054884172345928,
          "values": [
            0.04054884172345928
          ]
        },
        "semantic_similarity": {
          "mean": 0.21572590524796398,
          "std": 0.0,
          "min": 0.21572590524796398,
          "max": 0.21572590524796398,
          "median": 0.21572590524796398,
          "values": [
            0.21572590524796398
          ]
        },
        "answer_correctness": {
          "mean": 0.26,
          "std": 0.0,
          "min": 0.26,
          "max": 0.26,
          "median": 0.26,
          "values": [
            0.26
          ]
        },
        "strict_format_compliance": {
          "mean": 0.11200000000000002,
          "std": 0.0,
          "min": 0.11200000000000002,
          "max": 0.11200000000000002,
          "median": 0.11200000000000002,
          "values": [
            0.11200000000000002
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.22,
          "std": 0.0,
          "min": 0.22,
          "max": 0.22,
          "median": 0.22,
          "values": [
            0.22
          ]
        }
      }
    },
    "pubmedqa_expert_impersonation_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_impersonation__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.07,
          "std": 0.0,
          "min": 0.07,
          "max": 0.07,
          "median": 0.07,
          "values": [
            0.07
          ]
        },
        "precision": {
          "mean": 0.07012048192771085,
          "std": 0.0,
          "min": 0.07012048192771085,
          "max": 0.07012048192771085,
          "median": 0.07012048192771085,
          "values": [
            0.07012048192771085
          ]
        },
        "recall": {
          "mean": 0.08,
          "std": 0.0,
          "min": 0.08,
          "max": 0.08,
          "median": 0.08,
          "values": [
            0.08
          ]
        },
        "f1_score": {
          "mean": 0.07023809523809524,
          "std": 0.0,
          "min": 0.07023809523809524,
          "max": 0.07023809523809524,
          "median": 0.07023809523809524,
          "values": [
            0.07023809523809524
          ]
        },
        "jaccard": {
          "mean": 0.07012048192771085,
          "std": 0.0,
          "min": 0.07012048192771085,
          "max": 0.07012048192771085,
          "median": 0.07012048192771085,
          "values": [
            0.07012048192771085
          ]
        },
        "semantic_similarity": {
          "mean": 0.23036548116244376,
          "std": 0.0,
          "min": 0.23036548116244376,
          "max": 0.23036548116244376,
          "median": 0.23036548116244376,
          "values": [
            0.23036548116244376
          ]
        },
        "answer_correctness": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        },
        "strict_format_compliance": {
          "mean": 0.10000000000000002,
          "std": 0.0,
          "min": 0.10000000000000002,
          "max": 0.10000000000000002,
          "median": 0.10000000000000002,
          "values": [
            0.10000000000000002
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.08,
          "std": 0.0,
          "min": 0.08,
          "max": 0.08,
          "median": 0.08,
          "values": [
            0.08
          ]
        },
        "precision": {
          "mean": 0.08040330023435047,
          "std": 0.0,
          "min": 0.08040330023435047,
          "max": 0.08040330023435047,
          "median": 0.08040330023435047,
          "values": [
            0.08040330023435047
          ]
        },
        "recall": {
          "mean": 0.11,
          "std": 0.0,
          "min": 0.11,
          "max": 0.11,
          "median": 0.11,
          "values": [
            0.11
          ]
        },
        "f1_score": {
          "mean": 0.08079586285013975,
          "std": 0.0,
          "min": 0.08079586285013975,
          "max": 0.08079586285013975,
          "median": 0.08079586285013975,
          "values": [
            0.08079586285013975
          ]
        },
        "jaccard": {
          "mean": 0.08040330023435047,
          "std": 0.0,
          "min": 0.08040330023435047,
          "max": 0.08040330023435047,
          "median": 0.08040330023435047,
          "values": [
            0.08040330023435047
          ]
        },
        "semantic_similarity": {
          "mean": 0.34886065615341066,
          "std": 0.0,
          "min": 0.34886065615341066,
          "max": 0.34886065615341066,
          "median": 0.34886065615341066,
          "values": [
            0.34886065615341066
          ]
        },
        "answer_correctness": {
          "mean": 0.35,
          "std": 0.0,
          "min": 0.35,
          "max": 0.35,
          "median": 0.35,
          "values": [
            0.35
          ]
        },
        "strict_format_compliance": {
          "mean": 0.21599999999999997,
          "std": 0.0,
          "min": 0.21599999999999997,
          "max": 0.21599999999999997,
          "median": 0.21599999999999997,
          "values": [
            0.21599999999999997
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.38,
          "std": 0.0,
          "min": 0.38,
          "max": 0.38,
          "median": 0.38,
          "values": [
            0.38
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "precision": {
          "mean": 0.6802380952380952,
          "std": 0.0,
          "min": 0.6802380952380952,
          "max": 0.6802380952380952,
          "median": 0.6802380952380952,
          "values": [
            0.6802380952380952
          ]
        },
        "recall": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        },
        "f1_score": {
          "mean": 0.6804651162790698,
          "std": 0.0,
          "min": 0.6804651162790698,
          "max": 0.6804651162790698,
          "median": 0.6804651162790698,
          "values": [
            0.6804651162790698
          ]
        },
        "jaccard": {
          "mean": 0.6802380952380952,
          "std": 0.0,
          "min": 0.6802380952380952,
          "max": 0.6802380952380952,
          "median": 0.6802380952380952,
          "values": [
            0.6802380952380952
          ]
        },
        "semantic_similarity": {
          "mean": 0.8674150868877768,
          "std": 0.0,
          "min": 0.8674150868877768,
          "max": 0.8674150868877768,
          "median": 0.8674150868877768,
          "values": [
            0.8674150868877768
          ]
        },
        "answer_correctness": {
          "mean": 0.72,
          "std": 0.0,
          "min": 0.72,
          "max": 0.72,
          "median": 0.72,
          "values": [
            0.72
          ]
        },
        "strict_format_compliance": {
          "mean": 0.7399999999999999,
          "std": 0.0,
          "min": 0.7399999999999999,
          "max": 0.7399999999999999,
          "median": 0.7399999999999999,
          "values": [
            0.7399999999999999
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.9519999999999998,
          "std": 0.0,
          "min": 0.9519999999999998,
          "max": 0.9519999999999998,
          "median": 0.9519999999999998,
          "values": [
            0.9519999999999998
          ]
        }
      }
    },
    "pubmedqa_reasoning_free_min_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_free__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_free_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_free"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0014495515649701357,
          "std": 0.0,
          "min": 0.0014495515649701357,
          "max": 0.0014495515649701357,
          "median": 0.0014495515649701357,
          "values": [
            0.0014495515649701357
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.0027728824847468914,
          "std": 0.0,
          "min": 0.0027728824847468914,
          "max": 0.0027728824847468914,
          "median": 0.0027728824847468914,
          "values": [
            0.0027728824847468914
          ]
        },
        "jaccard": {
          "mean": 0.0014495515649701357,
          "std": 0.0,
          "min": 0.0014495515649701357,
          "max": 0.0014495515649701357,
          "median": 0.0014495515649701357,
          "values": [
            0.0014495515649701357
          ]
        },
        "semantic_similarity": {
          "mean": 0.045420683547854425,
          "std": 0.0,
          "min": 0.045420683547854425,
          "max": 0.045420683547854425,
          "median": 0.045420683547854425,
          "values": [
            0.045420683547854425
          ]
        },
        "answer_correctness": {
          "mean": 0.67,
          "std": 0.0,
          "min": 0.67,
          "max": 0.67,
          "median": 0.67,
          "values": [
            0.67
          ]
        },
        "strict_format_compliance": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "no_explanation_penalty": {
          "mean": 0.002,
          "std": 0.0,
          "min": 0.002,
          "max": 0.002,
          "median": 0.002,
          "values": [
            0.002
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "precision": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "recall": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "f1_score": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "jaccard": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "semantic_similarity": {
          "mean": 0.5519237720966339,
          "std": 0.0,
          "min": 0.5519237720966339,
          "max": 0.5519237720966339,
          "median": 0.5519237720966339,
          "values": [
            0.5519237720966339
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "precision": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "recall": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "f1_score": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "jaccard": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "semantic_similarity": {
          "mean": 0.5633874040842056,
          "std": 0.0,
          "min": 0.5633874040842056,
          "max": 0.5633874040842056,
          "median": 0.5633874040842056,
          "values": [
            0.5633874040842056
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "precision": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "recall": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "f1_score": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "jaccard": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "semantic_similarity": {
          "mean": 0.5800582206249237,
          "std": 0.0,
          "min": 0.5800582206249237,
          "max": 0.5800582206249237,
          "median": 0.5800582206249237,
          "values": [
            0.5800582206249237
          ]
        },
        "answer_correctness": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.07,
          "std": 0.0,
          "min": 0.07,
          "max": 0.07,
          "median": 0.07,
          "values": [
            0.07
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.505,
          "std": 0.0,
          "min": 0.505,
          "max": 0.505,
          "median": 0.505,
          "values": [
            0.505
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "precision": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "recall": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "f1_score": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "jaccard": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "semantic_similarity": {
          "mean": 0.7236178785562515,
          "std": 0.0,
          "min": 0.7236178785562515,
          "max": 0.7236178785562515,
          "median": 0.7236178785562515,
          "values": [
            0.7236178785562515
          ]
        },
        "answer_correctness": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.44,
          "std": 0.0,
          "min": 0.44,
          "max": 0.44,
          "median": 0.44,
          "values": [
            0.44
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.625,
          "std": 0.0,
          "min": 0.625,
          "max": 0.625,
          "median": 0.625,
          "values": [
            0.625
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_hf-mistral-nemo-12b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0013026776619167902,
          "std": 0.0,
          "min": 0.0013026776619167902,
          "max": 0.0013026776619167902,
          "median": 0.0013026776619167902,
          "values": [
            0.0013026776619167902
          ]
        },
        "recall": {
          "mean": 0.1,
          "std": 0.0,
          "min": 0.1,
          "max": 0.1,
          "median": 0.1,
          "values": [
            0.1
          ]
        },
        "f1_score": {
          "mean": 0.0025698495375862185,
          "std": 0.0,
          "min": 0.0025698495375862185,
          "max": 0.0025698495375862185,
          "median": 0.0025698495375862185,
          "values": [
            0.0025698495375862185
          ]
        },
        "jaccard": {
          "mean": 0.0013026776619167902,
          "std": 0.0,
          "min": 0.0013026776619167902,
          "max": 0.0013026776619167902,
          "median": 0.0013026776619167902,
          "values": [
            0.0013026776619167902
          ]
        },
        "semantic_similarity": {
          "mean": 0.07627987329848111,
          "std": 0.0,
          "min": 0.07627987329848111,
          "max": 0.07627987329848111,
          "median": 0.07627987329848111,
          "values": [
            0.07627987329848111
          ]
        },
        "answer_correctness": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.3,
          "std": 0.0,
          "min": 0.3,
          "max": 0.3,
          "median": 0.3,
          "values": [
            0.3
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.51,
          "std": 0.0,
          "min": 0.51,
          "max": 0.51,
          "median": 0.51,
          "values": [
            0.51
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_hf-qwen2.5-3b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__hf-qwen2.5-3b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.000930205780280948,
          "std": 0.0,
          "min": 0.000930205780280948,
          "max": 0.000930205780280948,
          "median": 0.000930205780280948,
          "values": [
            0.000930205780280948
          ]
        },
        "recall": {
          "mean": 0.07,
          "std": 0.0,
          "min": 0.07,
          "max": 0.07,
          "median": 0.07,
          "values": [
            0.07
          ]
        },
        "f1_score": {
          "mean": 0.0018351929223775132,
          "std": 0.0,
          "min": 0.0018351929223775132,
          "max": 0.0018351929223775132,
          "median": 0.0018351929223775132,
          "values": [
            0.0018351929223775132
          ]
        },
        "jaccard": {
          "mean": 0.000930205780280948,
          "std": 0.0,
          "min": 0.000930205780280948,
          "max": 0.000930205780280948,
          "median": 0.000930205780280948,
          "values": [
            0.000930205780280948
          ]
        },
        "semantic_similarity": {
          "mean": 0.04247692535398528,
          "std": 0.0,
          "min": 0.04247692535398528,
          "max": 0.04247692535398528,
          "median": 0.04247692535398528,
          "values": [
            0.04247692535398528
          ]
        },
        "answer_correctness": {
          "mean": 0.37,
          "std": 0.0,
          "min": 0.37,
          "max": 0.37,
          "median": 0.37,
          "values": [
            0.37
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.48,
          "std": 0.0,
          "min": 0.48,
          "max": 0.48,
          "median": 0.48,
          "values": [
            0.48
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.61,
          "std": 0.0,
          "min": 0.61,
          "max": 0.61,
          "median": 0.61,
          "values": [
            0.61
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "precision": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "recall": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "f1_score": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "jaccard": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "semantic_similarity": {
          "mean": 0.6345584142208099,
          "std": 0.0,
          "min": 0.6345584142208099,
          "max": 0.6345584142208099,
          "median": 0.6345584142208099,
          "values": [
            0.6345584142208099
          ]
        },
        "answer_correctness": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.22,
          "std": 0.0,
          "min": 0.22,
          "max": 0.22,
          "median": 0.22,
          "values": [
            0.22
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.545,
          "std": 0.0,
          "min": 0.545,
          "max": 0.545,
          "median": 0.545,
          "values": [
            0.545
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "precision": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "recall": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "f1_score": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "jaccard": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "semantic_similarity": {
          "mean": 0.6790171325206756,
          "std": 0.0,
          "min": 0.6790171325206756,
          "max": 0.6790171325206756,
          "median": 0.6790171325206756,
          "values": [
            0.6790171325206756
          ]
        },
        "answer_correctness": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "precision": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "recall": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "f1_score": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "jaccard": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "semantic_similarity": {
          "mean": 0.7599697810411453,
          "std": 0.0,
          "min": 0.7599697810411453,
          "max": 0.7599697810411453,
          "median": 0.7599697810411453,
          "values": [
            0.7599697810411453
          ]
        },
        "answer_correctness": {
          "mean": 0.45,
          "std": 0.0,
          "min": 0.45,
          "max": 0.45,
          "median": 0.45,
          "values": [
            0.45
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.49,
          "std": 0.0,
          "min": 0.49,
          "max": 0.49,
          "median": 0.49,
          "values": [
            0.49
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.69,
          "std": 0.0,
          "min": 0.69,
          "max": 0.69,
          "median": 0.69,
          "values": [
            0.69
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "precision": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "recall": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "f1_score": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "jaccard": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "semantic_similarity": {
          "mean": 0.8574998915195465,
          "std": 0.0,
          "min": 0.8574998915195465,
          "max": 0.8574998915195465,
          "median": 0.8574998915195465,
          "values": [
            0.8574998915195465
          ]
        },
        "answer_correctness": {
          "mean": 0.65,
          "std": 0.0,
          "min": 0.65,
          "max": 0.65,
          "median": 0.65,
          "values": [
            0.65
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.75,
          "std": 0.0,
          "min": 0.75,
          "max": 0.75,
          "median": 0.75,
          "values": [
            0.75
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.765,
          "std": 0.0,
          "min": 0.765,
          "max": 0.765,
          "median": 0.765,
          "values": [
            0.765
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_u4b-llama3-8b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.02,
          "std": 0.0,
          "min": 0.02,
          "max": 0.02,
          "median": 0.02,
          "values": [
            0.02
          ]
        },
        "precision": {
          "mean": 0.020163934426229504,
          "std": 0.0,
          "min": 0.020163934426229504,
          "max": 0.020163934426229504,
          "median": 0.020163934426229504,
          "values": [
            0.020163934426229504
          ]
        },
        "recall": {
          "mean": 0.03,
          "std": 0.0,
          "min": 0.03,
          "max": 0.03,
          "median": 0.03,
          "values": [
            0.03
          ]
        },
        "f1_score": {
          "mean": 0.02032258064516129,
          "std": 0.0,
          "min": 0.02032258064516129,
          "max": 0.02032258064516129,
          "median": 0.02032258064516129,
          "values": [
            0.02032258064516129
          ]
        },
        "jaccard": {
          "mean": 0.020163934426229504,
          "std": 0.0,
          "min": 0.020163934426229504,
          "max": 0.020163934426229504,
          "median": 0.020163934426229504,
          "values": [
            0.020163934426229504
          ]
        },
        "semantic_similarity": {
          "mean": 0.8324580839648843,
          "std": 0.0,
          "min": 0.8324580839648843,
          "max": 0.8324580839648843,
          "median": 0.8324580839648843,
          "values": [
            0.8324580839648843
          ]
        },
        "answer_correctness": {
          "mean": 0.84,
          "std": 0.0,
          "min": 0.84,
          "max": 0.84,
          "median": 0.84,
          "values": [
            0.84
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.845,
          "std": 0.0,
          "min": 0.845,
          "max": 0.845,
          "median": 0.845,
          "values": [
            0.845
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "precision": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "jaccard": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "semantic_similarity": {
          "mean": 0.5696539521217346,
          "std": 0.0,
          "min": 0.5696539521217346,
          "max": 0.5696539521217346,
          "median": 0.5696539521217346,
          "values": [
            0.5696539521217346
          ]
        },
        "answer_correctness": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "precision": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "jaccard": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "semantic_similarity": {
          "mean": 0.568500058054924,
          "std": 0.0,
          "min": 0.568500058054924,
          "max": 0.568500058054924,
          "median": 0.568500058054924,
          "values": [
            0.568500058054924
          ]
        },
        "answer_correctness": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.06,
          "std": 0.0,
          "min": 0.06,
          "max": 0.06,
          "median": 0.06,
          "values": [
            0.06
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.5,
          "std": 0.0,
          "min": 0.5,
          "max": 0.5,
          "median": 0.5,
          "values": [
            0.5
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_u4b-llama3.2-1b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.59,
          "std": 0.0,
          "min": 0.59,
          "max": 0.59,
          "median": 0.59,
          "values": [
            0.59
          ]
        },
        "precision": {
          "mean": 0.601,
          "std": 0.0,
          "min": 0.601,
          "max": 0.601,
          "median": 0.601,
          "values": [
            0.601
          ]
        },
        "recall": {
          "mean": 0.62,
          "std": 0.0,
          "min": 0.62,
          "max": 0.62,
          "median": 0.62,
          "values": [
            0.62
          ]
        },
        "f1_score": {
          "mean": 0.6051515151515151,
          "std": 0.0,
          "min": 0.6051515151515151,
          "max": 0.6051515151515151,
          "median": 0.6051515151515151,
          "values": [
            0.6051515151515151
          ]
        },
        "jaccard": {
          "mean": 0.601,
          "std": 0.0,
          "min": 0.601,
          "max": 0.601,
          "median": 0.601,
          "values": [
            0.601
          ]
        },
        "semantic_similarity": {
          "mean": 0.8452368076331913,
          "std": 0.0,
          "min": 0.8452368076331913,
          "max": 0.8452368076331913,
          "median": 0.8452368076331913,
          "values": [
            0.8452368076331913
          ]
        },
        "answer_correctness": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 1.0,
          "std": 0.0,
          "min": 1.0,
          "max": 1.0,
          "median": 1.0,
          "values": [
            1.0
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0051536861138175545,
          "std": 0.0,
          "min": 0.0051536861138175545,
          "max": 0.0051536861138175545,
          "median": 0.0051536861138175545,
          "values": [
            0.0051536861138175545
          ]
        },
        "recall": {
          "mean": 0.18,
          "std": 0.0,
          "min": 0.18,
          "max": 0.18,
          "median": 0.18,
          "values": [
            0.18
          ]
        },
        "f1_score": {
          "mean": 0.009632945747052552,
          "std": 0.0,
          "min": 0.009632945747052552,
          "max": 0.009632945747052552,
          "median": 0.009632945747052552,
          "values": [
            0.009632945747052552
          ]
        },
        "jaccard": {
          "mean": 0.0051536861138175545,
          "std": 0.0,
          "min": 0.0051536861138175545,
          "max": 0.0051536861138175545,
          "median": 0.0051536861138175545,
          "values": [
            0.0051536861138175545
          ]
        },
        "semantic_similarity": {
          "mean": 0.3890067627164535,
          "std": 0.0,
          "min": 0.3890067627164535,
          "max": 0.3890067627164535,
          "median": 0.3890067627164535,
          "values": [
            0.3890067627164535
          ]
        },
        "answer_correctness": {
          "mean": 0.68,
          "std": 0.0,
          "min": 0.68,
          "max": 0.68,
          "median": 0.68,
          "values": [
            0.68
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.83,
          "std": 0.0,
          "min": 0.83,
          "max": 0.83,
          "median": 0.83,
          "values": [
            0.83
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.775,
          "std": 0.0,
          "min": 0.775,
          "max": 0.775,
          "median": 0.775,
          "values": [
            0.775
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00870145948314553,
          "std": 0.0,
          "min": 0.00870145948314553,
          "max": 0.00870145948314553,
          "median": 0.00870145948314553,
          "values": [
            0.00870145948314553
          ]
        },
        "recall": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        },
        "f1_score": {
          "mean": 0.01572257991018602,
          "std": 0.0,
          "min": 0.01572257991018602,
          "max": 0.01572257991018602,
          "median": 0.01572257991018602,
          "values": [
            0.01572257991018602
          ]
        },
        "jaccard": {
          "mean": 0.00870145948314553,
          "std": 0.0,
          "min": 0.00870145948314553,
          "max": 0.00870145948314553,
          "median": 0.00870145948314553,
          "values": [
            0.00870145948314553
          ]
        },
        "semantic_similarity": {
          "mean": 0.38472949964227154,
          "std": 0.0,
          "min": 0.38472949964227154,
          "max": 0.38472949964227154,
          "median": 0.38472949964227154,
          "values": [
            0.38472949964227154
          ]
        },
        "answer_correctness": {
          "mean": 0.75,
          "std": 0.0,
          "min": 0.75,
          "max": 0.75,
          "median": 0.75,
          "values": [
            0.75
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.89,
          "std": 0.0,
          "min": 0.89,
          "max": 0.89,
          "median": 0.89,
          "values": [
            0.89
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.815,
          "std": 0.0,
          "min": 0.815,
          "max": 0.815,
          "median": 0.815,
          "values": [
            0.815
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.01151375664086748,
          "std": 0.0,
          "min": 0.01151375664086748,
          "max": 0.01151375664086748,
          "median": 0.01151375664086748,
          "values": [
            0.01151375664086748
          ]
        },
        "recall": {
          "mean": 0.18,
          "std": 0.0,
          "min": 0.18,
          "max": 0.18,
          "median": 0.18,
          "values": [
            0.18
          ]
        },
        "f1_score": {
          "mean": 0.020767816951360228,
          "std": 0.0,
          "min": 0.020767816951360228,
          "max": 0.020767816951360228,
          "median": 0.020767816951360228,
          "values": [
            0.020767816951360228
          ]
        },
        "jaccard": {
          "mean": 0.01151375664086748,
          "std": 0.0,
          "min": 0.01151375664086748,
          "max": 0.01151375664086748,
          "median": 0.01151375664086748,
          "values": [
            0.01151375664086748
          ]
        },
        "semantic_similarity": {
          "mean": 0.4576311853691004,
          "std": 0.0,
          "min": 0.4576311853691004,
          "max": 0.4576311853691004,
          "median": 0.4576311853691004,
          "values": [
            0.4576311853691004
          ]
        },
        "answer_correctness": {
          "mean": 0.83,
          "std": 0.0,
          "min": 0.83,
          "max": 0.83,
          "median": 0.83,
          "values": [
            0.83
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.97,
          "std": 0.0,
          "min": 0.97,
          "max": 0.97,
          "median": 0.97,
          "values": [
            0.97
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.845,
          "std": 0.0,
          "min": 0.845,
          "max": 0.845,
          "median": 0.845,
          "values": [
            0.845
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.00753968253968254,
          "std": 0.0,
          "min": 0.00753968253968254,
          "max": 0.00753968253968254,
          "median": 0.00753968253968254,
          "values": [
            0.00753968253968254
          ]
        },
        "recall": {
          "mean": 0.05,
          "std": 0.0,
          "min": 0.05,
          "max": 0.05,
          "median": 0.05,
          "values": [
            0.05
          ]
        },
        "f1_score": {
          "mean": 0.013071428571428569,
          "std": 0.0,
          "min": 0.013071428571428569,
          "max": 0.013071428571428569,
          "median": 0.013071428571428569,
          "values": [
            0.013071428571428569
          ]
        },
        "jaccard": {
          "mean": 0.00753968253968254,
          "std": 0.0,
          "min": 0.00753968253968254,
          "max": 0.00753968253968254,
          "median": 0.00753968253968254,
          "values": [
            0.00753968253968254
          ]
        },
        "semantic_similarity": {
          "mean": 0.7455847392976284,
          "std": 0.0,
          "min": 0.7455847392976284,
          "max": 0.7455847392976284,
          "median": 0.7455847392976284,
          "values": [
            0.7455847392976284
          ]
        },
        "answer_correctness": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.98,
          "std": 0.0,
          "min": 0.98,
          "max": 0.98,
          "median": 0.98,
          "values": [
            0.98
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.81,
          "std": 0.0,
          "min": 0.81,
          "max": 0.81,
          "median": 0.81,
          "values": [
            0.81
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_u4b-llama3.3-70b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-llama3.3-70b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.011117706746905244,
          "std": 0.0,
          "min": 0.011117706746905244,
          "max": 0.011117706746905244,
          "median": 0.011117706746905244,
          "values": [
            0.011117706746905244
          ]
        },
        "recall": {
          "mean": 0.28,
          "std": 0.0,
          "min": 0.28,
          "max": 0.28,
          "median": 0.28,
          "values": [
            0.28
          ]
        },
        "f1_score": {
          "mean": 0.019786220772375525,
          "std": 0.0,
          "min": 0.019786220772375525,
          "max": 0.019786220772375525,
          "median": 0.019786220772375525,
          "values": [
            0.019786220772375525
          ]
        },
        "jaccard": {
          "mean": 0.011117706746905244,
          "std": 0.0,
          "min": 0.011117706746905244,
          "max": 0.011117706746905244,
          "median": 0.011117706746905244,
          "values": [
            0.011117706746905244
          ]
        },
        "semantic_similarity": {
          "mean": 0.2086011575302109,
          "std": 0.0,
          "min": 0.2086011575302109,
          "max": 0.2086011575302109,
          "median": 0.2086011575302109,
          "values": [
            0.2086011575302109
          ]
        },
        "answer_correctness": {
          "mean": 0.8,
          "std": 0.0,
          "min": 0.8,
          "max": 0.8,
          "median": 0.8,
          "values": [
            0.8
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.95,
          "std": 0.0,
          "min": 0.95,
          "max": 0.95,
          "median": 0.95,
          "values": [
            0.95
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.825,
          "std": 0.0,
          "min": 0.825,
          "max": 0.825,
          "median": 0.825,
          "values": [
            0.825
          ]
        }
      }
    },
    "pubmedqa_expert_biostatistician_reasoning_required_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_biostatistician_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.14,
          "std": 0.0,
          "min": 0.14,
          "max": 0.14,
          "median": 0.14,
          "values": [
            0.14
          ]
        },
        "precision": {
          "mean": 0.14033545008912657,
          "std": 0.0,
          "min": 0.14033545008912657,
          "max": 0.14033545008912657,
          "median": 0.14033545008912657,
          "values": [
            0.14033545008912657
          ]
        },
        "recall": {
          "mean": 0.17,
          "std": 0.0,
          "min": 0.17,
          "max": 0.17,
          "median": 0.17,
          "values": [
            0.17
          ]
        },
        "f1_score": {
          "mean": 0.14066346280766878,
          "std": 0.0,
          "min": 0.14066346280766878,
          "max": 0.14066346280766878,
          "median": 0.14066346280766878,
          "values": [
            0.14066346280766878
          ]
        },
        "jaccard": {
          "mean": 0.14033545008912657,
          "std": 0.0,
          "min": 0.14033545008912657,
          "max": 0.14033545008912657,
          "median": 0.14033545008912657,
          "values": [
            0.14033545008912657
          ]
        },
        "semantic_similarity": {
          "mean": 0.21163585524074732,
          "std": 0.0,
          "min": 0.21163585524074732,
          "max": 0.21163585524074732,
          "median": 0.21163585524074732,
          "values": [
            0.21163585524074732
          ]
        },
        "answer_correctness": {
          "mean": 0.21,
          "std": 0.0,
          "min": 0.21,
          "max": 0.21,
          "median": 0.21,
          "values": [
            0.21
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.27,
          "std": 0.0,
          "min": 0.27,
          "max": 0.27,
          "median": 0.27,
          "values": [
            0.27
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.56,
          "std": 0.0,
          "min": 0.56,
          "max": 0.56,
          "median": 0.56,
          "values": [
            0.56
          ]
        }
      }
    },
    "pubmedqa_expert_epidemiologist_reasoning_required_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_epidemiologist_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        },
        "precision": {
          "mean": 0.12039618406285073,
          "std": 0.0,
          "min": 0.12039618406285073,
          "max": 0.12039618406285073,
          "median": 0.12039618406285073,
          "values": [
            0.12039618406285073
          ]
        },
        "recall": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "f1_score": {
          "mean": 0.12078200283627971,
          "std": 0.0,
          "min": 0.12078200283627971,
          "max": 0.12078200283627971,
          "median": 0.12078200283627971,
          "values": [
            0.12078200283627971
          ]
        },
        "jaccard": {
          "mean": 0.12039618406285073,
          "std": 0.0,
          "min": 0.12039618406285073,
          "max": 0.12039618406285073,
          "median": 0.12039618406285073,
          "values": [
            0.12039618406285073
          ]
        },
        "semantic_similarity": {
          "mean": 0.2613967066071928,
          "std": 0.0,
          "min": 0.2613967066071928,
          "max": 0.2613967066071928,
          "median": 0.2613967066071928,
          "values": [
            0.2613967066071928
          ]
        },
        "answer_correctness": {
          "mean": 0.24,
          "std": 0.0,
          "min": 0.24,
          "max": 0.24,
          "median": 0.24,
          "values": [
            0.24
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.29,
          "std": 0.0,
          "min": 0.29,
          "max": 0.29,
          "median": 0.29,
          "values": [
            0.29
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.575,
          "std": 0.0,
          "min": 0.575,
          "max": 0.575,
          "median": 0.575,
          "values": [
            0.575
          ]
        }
      }
    },
    "pubmedqa_expert_primary_care_reasoning_required_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_expert_primary_care_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.12,
          "std": 0.0,
          "min": 0.12,
          "max": 0.12,
          "median": 0.12,
          "values": [
            0.12
          ]
        },
        "precision": {
          "mean": 0.12043249967496779,
          "std": 0.0,
          "min": 0.12043249967496779,
          "max": 0.12043249967496779,
          "median": 0.12043249967496779,
          "values": [
            0.12043249967496779
          ]
        },
        "recall": {
          "mean": 0.15,
          "std": 0.0,
          "min": 0.15,
          "max": 0.15,
          "median": 0.15,
          "values": [
            0.15
          ]
        },
        "f1_score": {
          "mean": 0.12085259117517183,
          "std": 0.0,
          "min": 0.12085259117517183,
          "max": 0.12085259117517183,
          "median": 0.12085259117517183,
          "values": [
            0.12085259117517183
          ]
        },
        "jaccard": {
          "mean": 0.12043249967496779,
          "std": 0.0,
          "min": 0.12043249967496779,
          "max": 0.12043249967496779,
          "median": 0.12043249967496779,
          "values": [
            0.12043249967496779
          ]
        },
        "semantic_similarity": {
          "mean": 0.3236174924112856,
          "std": 0.0,
          "min": 0.3236174924112856,
          "max": 0.3236174924112856,
          "median": 0.3236174924112856,
          "values": [
            0.3236174924112856
          ]
        },
        "answer_correctness": {
          "mean": 0.22,
          "std": 0.0,
          "min": 0.22,
          "max": 0.22,
          "median": 0.22,
          "values": [
            0.22
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.24,
          "std": 0.0,
          "min": 0.24,
          "max": 0.24,
          "median": 0.24,
          "values": [
            0.24
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.58,
          "std": 0.0,
          "min": 0.58,
          "max": 0.58,
          "median": 0.58,
          "values": [
            0.58
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.36,
          "std": 0.0,
          "min": 0.36,
          "max": 0.36,
          "median": 0.36,
          "values": [
            0.36
          ]
        },
        "precision": {
          "mean": 0.3605707482993197,
          "std": 0.0,
          "min": 0.3605707482993197,
          "max": 0.3605707482993197,
          "median": 0.3605707482993197,
          "values": [
            0.3605707482993197
          ]
        },
        "recall": {
          "mean": 0.39,
          "std": 0.0,
          "min": 0.39,
          "max": 0.39,
          "median": 0.39,
          "values": [
            0.39
          ]
        },
        "f1_score": {
          "mean": 0.3611200257152041,
          "std": 0.0,
          "min": 0.3611200257152041,
          "max": 0.3611200257152041,
          "median": 0.3611200257152041,
          "values": [
            0.3611200257152041
          ]
        },
        "jaccard": {
          "mean": 0.3605707482993197,
          "std": 0.0,
          "min": 0.3605707482993197,
          "max": 0.3605707482993197,
          "median": 0.3605707482993197,
          "values": [
            0.3605707482993197
          ]
        },
        "semantic_similarity": {
          "mean": 0.5370984437223524,
          "std": 0.0,
          "min": 0.5370984437223524,
          "max": 0.5370984437223524,
          "median": 0.5370984437223524,
          "values": [
            0.5370984437223524
          ]
        },
        "answer_correctness": {
          "mean": 0.43,
          "std": 0.0,
          "min": 0.43,
          "max": 0.43,
          "median": 0.43,
          "values": [
            0.43
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.52,
          "std": 0.0,
          "min": 0.52,
          "max": 0.52,
          "median": 0.52,
          "values": [
            0.52
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.655,
          "std": 0.0,
          "min": 0.655,
          "max": 0.655,
          "median": 0.655,
          "values": [
            0.655
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_min_u4b-mistral-7b": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b__zero-shot__pubmedqa_reasoning_required_min__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.0,
          "std": 0.0,
          "min": 0.0,
          "max": 0.0,
          "median": 0.0,
          "values": [
            0.0
          ]
        },
        "precision": {
          "mean": 0.0008062354312354311,
          "std": 0.0,
          "min": 0.0008062354312354311,
          "max": 0.0008062354312354311,
          "median": 0.0008062354312354311,
          "values": [
            0.0008062354312354311
          ]
        },
        "recall": {
          "mean": 0.04,
          "std": 0.0,
          "min": 0.04,
          "max": 0.04,
          "median": 0.04,
          "values": [
            0.04
          ]
        },
        "f1_score": {
          "mean": 0.0015774319688448113,
          "std": 0.0,
          "min": 0.0015774319688448113,
          "max": 0.0015774319688448113,
          "median": 0.0015774319688448113,
          "values": [
            0.0015774319688448113
          ]
        },
        "jaccard": {
          "mean": 0.0008062354312354311,
          "std": 0.0,
          "min": 0.0008062354312354311,
          "max": 0.0008062354312354311,
          "median": 0.0008062354312354311,
          "values": [
            0.0008062354312354311
          ]
        },
        "semantic_similarity": {
          "mean": 0.042592798094265166,
          "std": 0.0,
          "min": 0.042592798094265166,
          "max": 0.042592798094265166,
          "median": 0.042592798094265166,
          "values": [
            0.042592798094265166
          ]
        },
        "answer_correctness": {
          "mean": 0.6,
          "std": 0.0,
          "min": 0.6,
          "max": 0.6,
          "median": 0.6,
          "values": [
            0.6
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.74,
          "std": 0.0,
          "min": 0.74,
          "max": 0.74,
          "median": 0.74,
          "values": [
            0.74
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.725,
          "std": 0.0,
          "min": 0.725,
          "max": 0.725,
          "median": 0.725,
          "values": [
            0.725
          ]
        }
      }
    },
    "pubmedqa_reasoning_required_u4b-mistral-7b_ft_pubmedqa_reasoning_ft": {
      "num_experiments": 1,
      "experiments": [
        "pubmedqa_reasoning_required__u4b-mistral-7b_ft_pubmedqa_reasoning_ft__zero-shot__pubmedqa_reasoning_required__100__0p100"
      ],
      "setups": [
        "pubmedqa_reasoning_required"
      ],
      "metrics": {
        "exact_match": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "precision": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "recall": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "f1_score": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "jaccard": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "semantic_similarity": {
          "mean": 0.9230746108293534,
          "std": 0.0,
          "min": 0.9230746108293534,
          "max": 0.9230746108293534,
          "median": 0.9230746108293534,
          "values": [
            0.9230746108293534
          ]
        },
        "answer_correctness": {
          "mean": 0.77,
          "std": 0.0,
          "min": 0.77,
          "max": 0.77,
          "median": 0.77,
          "values": [
            0.77
          ]
        },
        "maybe_overuse_penalty": {
          "mean": 0.99,
          "std": 0.0,
          "min": 0.99,
          "max": 0.99,
          "median": 0.99,
          "values": [
            0.99
          ]
        },
        "correct_definitive_answers": {
          "mean": 0.775,
          "std": 0.0,
          "min": 0.775,
          "max": 0.775,
          "median": 0.775,
          "values": [
            0.775
          ]
        }
      }
    }
  },
  "overall_best_performers": {
    "by_answer_correctness": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.9
    },
    "by_precision": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.8861134676564157
    },
    "by_response_cleanliness": {
      "experiment_name": "RHAI_cose_explanation__hf-mistral-nemo-12b__zero-shot__cose_explanation__100__0p100",
      "prompt": "cose_explanation",
      "model": "hf-mistral-nemo-12b",
      "setup": "RHAI_cose_explanation",
      "score": 1.0
    },
    "by_strict_format_compliance": {
      "experiment_name": "pubmedqa_reasoning_free__hf-qwen2.5-3b__zero-shot__pubmedqa_expert_biostatistician__100__0p100",
      "prompt": "pubmedqa_expert_biostatistician",
      "model": "hf-qwen2.5-3b",
      "setup": "pubmedqa_reasoning_free",
      "score": 1.0
    },
    "by_atomic_sentence_format": {
      "experiment_name": "ecqa_negative__u4b-llama3.3-70b__zero-shot__ecqa_negative_explanation__100__0p100",
      "prompt": "ecqa_negative_explanation",
      "model": "u4b-llama3.3-70b",
      "setup": "ecqa_negative",
      "score": 0.9057197691884995
    },
    "by_correct_answer_focus": {
      "experiment_name": "ecqa_positive__u4b-llama3.2-1b__zero-shot__ecqa_positive_explanation__100__0p100",
      "prompt": "ecqa_positive_explanation",
      "model": "u4b-llama3.2-1b",
      "setup": "ecqa_positive",
      "score": 0.922
    },
    "by_follows_format_instruction": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.9710000000000001
    },
    "by_all_choices_addressed": {
      "experiment_name": "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
      "prompt": "ecqa_freeflow_explanation",
      "model": "u4b-llama3.3-70b",
      "setup": "ecqa_freeflow",
      "score": 0.98
    },
    "by_answer_extractability": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.995
    },
    "by_original_text_mention": {
      "experiment_name": "gmeg_ours__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot_original__100__0p100",
      "prompt": "gmeg_few_shot_original",
      "model": "hf-mistral-nemo-12b",
      "setup": "gmeg_ours",
      "score": 0.9939732620320855
    },
    "by_no_explanation_penalty": {
      "experiment_name": "pubmedqa_reasoning_free__hf-mistral-nemo-12b__zero-shot__pubmedqa_reasoning_free__100__0p100",
      "prompt": "pubmedqa_reasoning_free",
      "model": "hf-mistral-nemo-12b",
      "setup": "pubmedqa_reasoning_free",
      "score": 1.0
    },
    "by_explanation_quality": {
      "experiment_name": "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
      "prompt": "cose_explanation",
      "model": "u4b-llama3.3-70b",
      "setup": "RHAI_cose_explanation",
      "score": 0.828
    },
    "by_correct_definitive_answers": {
      "experiment_name": "pubmedqa_reasoning_required__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
      "prompt": "pubmedqa_reasoning_required_min",
      "model": "u4b-llama3-8b",
      "setup": "pubmedqa_reasoning_required",
      "score": 0.845
    },
    "by_single_sentence_format": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
      "prompt": "ecare_explanation_generation",
      "model": "u4b-llama3.2-1b",
      "setup": "ecare_explanation_generation",
      "score": 0.602
    },
    "by_simple_atomic_sentences": {
      "experiment_name": "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation_formatted__100__0p100",
      "prompt": "ecqa_positive_explanation_formatted",
      "model": "hf-mistral-nemo-12b",
      "setup": "ecqa_positive",
      "score": 0.9596666666666667
    },
    "by_instruction_following_penalty": {
      "experiment_name": "RHAI_cose_choice_selection__hf-mistral-nemo-12b__zero-shot__cose_choice_selection__100__0p100",
      "prompt": "cose_choice_selection",
      "model": "hf-mistral-nemo-12b",
      "setup": "RHAI_cose_choice_selection",
      "score": 1.0
    },
    "by_incorrect_choices_coverage": {
      "experiment_name": "ecqa_negative__u4b-mistral-7b__zero-shot__ecqa_negative_explanation__100__0p100",
      "prompt": "ecqa_negative_explanation",
      "model": "u4b-mistral-7b",
      "setup": "ecqa_negative",
      "score": 0.9631666666666667
    },
    "by_explanation_correctness": {
      "experiment_name": "RHAI_cose_explanation_with_answer__hf-qwen3-30b-thinking__zero-shot__cose_explanation_with_answer__100__0p100",
      "prompt": "cose_explanation_with_answer",
      "model": "hf-qwen3-30b-thinking",
      "setup": "RHAI_cose_explanation_with_answer",
      "score": 1.0
    },
    "by_format_cleanliness": {
      "experiment_name": "RHAI_cose_explanation_with_answer__hf-mistral-nemo-12b__zero-shot__cose_explanation_with_answer__100__0p100",
      "prompt": "cose_explanation_with_answer",
      "model": "hf-mistral-nemo-12b",
      "setup": "RHAI_cose_explanation_with_answer",
      "score": 1.0
    },
    "by_response_brevity": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation__100__0p100",
      "prompt": "ecare_explanation_generation",
      "model": "u4b-llama3.2-1b",
      "setup": "ecare_explanation_generation",
      "score": 0.6809999999999999
    },
    "by_semantic_similarity": {
      "experiment_name": "pubmedqa_reasoning_free__u4b-llama3-8b__zero-shot__pubmedqa_reasoning_free__100__0p100",
      "prompt": "pubmedqa_reasoning_free",
      "model": "u4b-llama3-8b",
      "setup": "pubmedqa_reasoning_free",
      "score": 0.933139499425888
    },
    "by_exact_match": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.88
    },
    "by_external_knowledge_usage": {
      "experiment_name": "ecqa_negative__hf-mistral-nemo-12b__zero-shot__ecqa_negative_explanation__100__0p100",
      "prompt": "ecqa_negative_explanation",
      "model": "hf-mistral-nemo-12b",
      "setup": "ecqa_negative",
      "score": 1.0
    },
    "by_f1_score": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.8871428571428572
    },
    "by_jaccard": {
      "experiment_name": "ecqa_choice_selection_3choices__hf-qwen2.5-3b__zero-shot__ecqa_choice_selection_3choices__100__0p100",
      "prompt": "ecqa_choice_selection_3choices",
      "model": "hf-qwen2.5-3b",
      "setup": "ecqa_choice_selection_3choices",
      "score": 0.8844468009897491
    },
    "by_maybe_overuse_penalty": {
      "experiment_name": "pubmedqa_reasoning_required__u4b-llama3.2-1b__zero-shot__pubmedqa_reasoning_required_min__100__0p100",
      "prompt": "pubmedqa_reasoning_required_min",
      "model": "u4b-llama3.2-1b",
      "setup": "pubmedqa_reasoning_required",
      "score": 1.0
    },
    "by_answer_identification_correctness": {
      "experiment_name": "RHAI_cose_explanation__u4b-llama3.3-70b__zero-shot__cose_explanation__100__0p100",
      "prompt": "cose_explanation",
      "model": "u4b-llama3.3-70b",
      "setup": "RHAI_cose_explanation",
      "score": 0.705
    },
    "by_concise_justification": {
      "experiment_name": "ecqa_positive__hf-mistral-nemo-12b__zero-shot__ecqa_positive_explanation__100__0p100",
      "prompt": "ecqa_positive_explanation",
      "model": "hf-mistral-nemo-12b",
      "setup": "ecqa_positive",
      "score": 0.9618999999999999
    },
    "by_single_paragraph_format": {
      "experiment_name": "ecqa_freeflow__hf-mistral-nemo-12b__zero-shot__ecqa_expert_behavioral_economist__100__0p100",
      "prompt": "ecqa_expert_behavioral_economist",
      "model": "hf-mistral-nemo-12b",
      "setup": "ecqa_freeflow",
      "score": 1.0
    },
    "by_bullet_point_ratio": {
      "experiment_name": "gmeg__u4b-llama3.2-1b__few-shot-456__gmeg_few_shot__100__0p100",
      "prompt": "gmeg_few_shot",
      "model": "u4b-llama3.2-1b",
      "setup": "gmeg",
      "score": 1.745333333333333
    },
    "by_response_conciseness": {
      "experiment_name": "RHAI_cose_choice_selection__hf-qwen2.5-3b__zero-shot__cose_choice_selection__100__0p100",
      "prompt": "cose_choice_selection",
      "model": "hf-qwen2.5-3b",
      "setup": "RHAI_cose_choice_selection",
      "score": 0.992
    },
    "by_explanation_depth": {
      "experiment_name": "ecqa_freeflow__u4b-llama3.3-70b__zero-shot__ecqa_freeflow_explanation__100__0p100",
      "prompt": "ecqa_freeflow_explanation",
      "model": "u4b-llama3.3-70b",
      "setup": "ecqa_freeflow",
      "score": 0.5681
    },
    "by_correction_terminology_recall": {
      "experiment_name": "gmeg__hf-qwen3-30b-thinking__zero-shot__gmeg_explaination__100__0p100",
      "prompt": "gmeg_explaination",
      "model": "hf-qwen3-30b-thinking",
      "setup": "gmeg",
      "score": 0.8666666666666667
    },
    "by_structural_format_match": {
      "experiment_name": "gmeg__hf-mistral-nemo-12b__few-shot-456__gmeg_few_shot__100__0p100",
      "prompt": "gmeg_few_shot",
      "model": "hf-mistral-nemo-12b",
      "setup": "gmeg",
      "score": 0.91
    },
    "by_brevity_compliance": {
      "experiment_name": "RHAI_cose_explanation_with_answer__u4b-mistral-7b__zero-shot__cose_explanation_with_answer__100__0p100",
      "prompt": "cose_explanation_with_answer",
      "model": "u4b-mistral-7b",
      "setup": "RHAI_cose_explanation_with_answer",
      "score": 0.972
    },
    "by_conceptual_abstraction_level": {
      "experiment_name": "ecare_explanation_generation__u4b-llama3.2-1b__zero-shot__ecare_explanation_generation_demographic_young_american__100__0p100",
      "prompt": "ecare_explanation_generation_demographic_young_american",
      "model": "u4b-llama3.2-1b",
      "setup": "ecare_explanation_generation",
      "score": 0.9294999999999999
    },
    "by_recall": {
      "experiment_name": "ecare_choice_selection__u4b-llama3.3-70b__zero-shot__ecare_choice_selection__100__0p100",
      "prompt": "ecare_choice_selection",
      "model": "u4b-llama3.3-70b",
      "setup": "ecare_choice_selection",
      "score": 0.92
    }
  }
}