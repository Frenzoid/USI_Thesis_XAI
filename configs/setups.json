{
  "gmeg": {
    "description": "LLM explanation generation benchmark based on Grammarly's My Edit Graph (GMEG) dataset, which includes human-annotated explanations for text revisions.",
    "dataset": {
      "download_link": "https://github.com/grammarly/gmeg-exp/archive/refs/heads/main.zip",
      "download_path": "DS_GMEG_EXP",
      "csv_file": "gmeg-exp-main/data/3_annotated_data/full-scale_data/full_scale_annotated_full.csv"
    },
    "prompt_fields": {
      "question_fields": ["aligned"],
      "answer_field": "please_explain_the_revisions_write_na_if_not_annotatable"
    },
    "prune_row": {
      "please_explain_the_revisions_write_na_if_not_annotatable": [""]
    },
    "custom_metrics": {
      "module_path": "custom_metrics.gmeg_cm",
      "metrics_registry": "GMEG_METRICS"
    }
  },
  "gmeg_ours": {
    "description": "A Dataset of Human and LLM-Generated Explanations of Grammatical and Fluency Edits",
    "dataset": {
      "download_link": "https://github.com/grammarly/gmeg-exp/archive/refs/heads/main.zip",
      "download_path": "DS_GMEG_EXP",
      "csv_file": "gmeg-exp-main/data/3_annotated_data/full-scale_data/full_scale_annotated_full.csv"
    },
    "prompt_fields": {
      "question_fields": ["original", "revised"],
      "answer_field": "please_explain_the_revisions_write_na_if_not_annotatable"
    },
    "prune_row": {
      "please_explain_the_revisions_write_na_if_not_annotatable": [""]
    },
    "custom_metrics": {
      "module_path": "custom_metrics.gmeg_cm",
      "metrics_registry": "GMEG_METRICS"
    }
  },
  
  "RHAI_cose_explanation_with_answer": {
    "description": "Evaluates the ability of models to generate human-like explanations for various tasks, focusing on the reframing of explanations to enhance clarity and understanding.",
    "dataset": {
      "download_link": "https://github.com/allenai/few_shot_explanations/archive/refs/heads/main.zip",
      "download_path": "DS_ReframingHumanAI",
      "csv_file": "few_shot_explanations-main/data/handwritten_cose_v1.11_examples.csv"
    },
    "prompt_fields": {
      "question_fields": ["question", "answer"],
      "answer_field": "our_explanation"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.cose_explaination_w_answer_cm",
      "metrics_registry": "COMMONSENSE_REASONING_JUSTIFICATION_METRICS"
    }
  },
  "RHAI_cose_explanation_with_answer_and_questions": {
    "description": "Evaluates the ability of models to generate human-like explanations for various tasks, focusing on the reframing of explanations to enhance clarity and understanding.",
    "dataset": {
      "download_link": "https://github.com/allenai/few_shot_explanations/archive/refs/heads/main.zip",
      "download_path": "DS_ReframingHumanAI",
      "csv_file": "few_shot_explanations-main/data/handwritten_cose_v1.11_examples.csv"
    },
    "prompt_fields": {
      "question_fields": ["question", "choices", "answer"],
      "answer_field": "our_explanation"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.cose_explaination_w_answer_cm",
      "metrics_registry": "COMMONSENSE_REASONING_JUSTIFICATION_METRICS"
    }
  },
  "RHAI_cose_explanation": {
    "description": "Evaluates the ability of models to generate human-like explanations for various tasks, focusing on the reframing of explanations to enhance clarity and understanding.",
    "dataset": {
      "download_link": "https://github.com/allenai/few_shot_explanations/archive/refs/heads/main.zip",
      "download_path": "DS_ReframingHumanAI",
      "csv_file": "few_shot_explanations-main/data/handwritten_cose_v1.11_examples.csv"
    },
    "prompt_fields": {
      "question_fields": ["question", "choices"],
      "answer_field": "our_explanation"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.cose_explaination_cm",
      "metrics_registry": "COMMONSENSE_REASONING_EXPLANATION_METRICS"
    }
  },
  "RHAI_cose_choice_selection": {
    "description": "Evaluates the ability of models to select the correct answer from multiple choices for various tasks, focusing on understanding and reasoning.",
    "dataset": {
      "download_link": "https://github.com/allenai/few_shot_explanations/archive/refs/heads/main.zip",
      "download_path": "DS_ReframingHumanAI", 
      "csv_file": "few_shot_explanations-main/data/handwritten_cose_v1.11_examples.csv"
    },
    "prompt_fields": {
      "question_fields": ["question", "choices"],
      "answer_field": "answer"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.cose_choice_cm",
      "metrics_registry": "COMMONSENSE_REASONING_METRICS"
    }
  },

  
  "ecqa_choice": {
    "description": "Choice selection for ECQA commonsense questions",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_ECQA", 
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["q_concept", "q_text", "q_op1", "q_op2", "q_op3", "q_op4", "q_op5"],
      "answer_field": "q_ans"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_choice_cm",
      "metrics_registry": "COMMONSENSE_REASONING_METRICS"
    }
  },
  "ecqa_choice_selection_3choices": {
    "description": "Choice selection for ECQA commonsense questions with only 3 choices (correct + 2 random distractors)",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_ECQA", 
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["q_concept", "q_text", "q_op1", "q_ans", "q_op3"],
      "answer_field": "q_ans"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_choice_cm",
      "metrics_registry": "COMMONSENSE_REASONING_METRICS"
    }
  },
  "ecqa_choice_selection_no_concept":{
    "description": "Remove topic context (q_concept field) to test if reasoning works without explicit domain framing",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_ECQA", 
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["q_text", "q_op1", "q_op2", "q_op3", "q_op4", "q_op5"],
      "answer_field": "q_ans"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_choice_cm",
      "metrics_registry": "COMMONSENSE_REASONING_METRICS"
    }
  },
  "ecqa_positive": {
    "description": "Generate positive properties explaining correct answer",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true", 
      "download_path": "DS_ECQA",
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["q_text", "q_op1", "q_op2", "q_op3", "q_op4", "q_op5", "q_ans"],
      "answer_field": "taskA_pos"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_positive_cm",
      "metrics_registry": "POSITIVE_JUSTIFICATION_METRICS"
    }
  },
  "ecqa_negative": {
    "description": "Generate negative properties explaining why other choices are wrong",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_ECQA", 
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["q_text", "q_op1", "q_op2", "q_op3", "q_op4", "q_op5", "q_ans"],
      "answer_field": "taskA_neg"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_negative_cm",
      "metrics_registry": "NEGATIVE_PROPERTIES_METRICS"
    }
  },
  "ecqa_freeflow": {
    "description": "Generate complete free-form explanation", 
    "dataset": {
      "download_link": "https://huggingface.co/datasets/yangdong/ecqa/resolve/main/data/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_ECQA",
      "parquet_file": "train-00000-of-00001.parquet" 
    },
    "prompt_fields": {
      "question_fields": ["q_text", "q_op1", "q_op2", "q_op3", "q_op4", "q_op5", "q_ans"],
      "answer_field": "taskB"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecqa_freeflow_cm",
      "metrics_registry": "EXPLANATION_GENERATION_METRICS"
    }
  },


  "ecare_choice_selection": {
    "description": "Choice selection for e-CARE causal reasoning questions - select correct hypothesis for given premise",
    "dataset": {
      "download_link": "https://github.com/Waste-Wood/e-CARE/archive/refs/heads/main.zip",
      "download_path": "DS_ECARE",
      "jsonl_file": "e-CARE-main/dataset/Causal_Reasoning/train.jsonl"
    },
    "prompt_fields": {
      "question_fields": ["premise", "ask-for", "hypothesis1", "hypothesis2"],
      "answer_field": "label"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecare_choice_cm",
      "metrics_registry": "CAUSAL_REASONING_METRICS"
    }
  },

  "ecare_explanation_generation": {
    "description": "Generate conceptual explanations for e-CARE causal reasoning questions", 
    "dataset": {
      "download_link": "https://github.com/Waste-Wood/e-CARE/archive/refs/heads/main.zip",
      "download_path": "DS_ECARE",
      "jsonl_file": "e-CARE-main/dataset/Explanation_Generation/train.jsonl"
    },
    "prompt_fields": {
      "question_fields": ["cause", "effect"],
      "answer_field": "conceptual_explanation"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.ecare_explaination_cm",
      "metrics_registry": "CONCEPTUAL_EXPLANATION_METRICS"
    }
  },


  "pubmedqa_reasoning_free": {
    "comments": "If we get ≥90% accuracy on reasoning-free → matches human performance",
    "description": "Replicates Annotator 1: reasoning-free with conclusion available",
    "dataset": {
      "download_link": "https://huggingface.co/datasets/qiaojin/PubMedQA/resolve/main/pqa_labeled/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_PubMedQA", 
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["question", "json:context.contexts[*]", "long_answer"],
      "answer_field": "final_decision"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.pubmedqa_free_cm",
      "metrics_registry": "MEDICAL_QA_METRICS"
    }
  },
  "pubmedqa_reasoning_required": {
    "comments": "if we get ≥78% accuracy on reasoning-required → matches human performance",
    "description": "Replicates Annotator 2: reasoning-required without conclusion", 
    "dataset": {
      "download_link": "https://huggingface.co/datasets/qiaojin/PubMedQA/resolve/main/pqa_labeled/train-00000-of-00001.parquet?download=true",
      "download_path": "DS_PubMedQA",
      "parquet_file": "train-00000-of-00001.parquet"
    },
    "prompt_fields": {
      "question_fields": ["question", "json:context.contexts[*]"],
      "answer_field": "final_decision"
    },
    "custom_metrics": {
      "module_path": "custom_metrics.pubmedqa_required_cm",
      "metrics_registry": "EVIDENCE_BASED_MEDICAL_QA_METRICS"
    }
  }
}