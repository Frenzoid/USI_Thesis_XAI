{
  "u4b-llama3.2-1b": {
    "type": "local",
    "model_path": "unsloth/llama-3.2-1b-instruct-bnb-4bit",
    "description": "Llama 3.2 1B parameter model (4-bit quantized with Unsloth) - Small",
    "parameters": "1B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "u4b-llama3-8b": {
    "type": "local",
    "model_path": "unsloth/llama-3-8b-instruct-bnb-4bit",
    "description": "Llama 3 8B parameter model (4-bit quantized with Unsloth) - Medium",
    "parameters": "8B",
    "size_category": "medium",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "u4b-llama3.3-70b": {
    "type": "local",
    "model_path": "unsloth/llama-3.3-70b-instruct-bnb-4bit",
    "description": "Llama 3.3 70B parameter model (4-bit quantized with Unsloth) - Large",
    "parameters": "70B",
    "size_category": "large",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": false
  },
  "u4b-mistral-7b": {
    "type": "local",
    "model_path": "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "description": "Mistral 7B parameter model (4-bit quantized with Unsloth) - Small",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "hf-mistral-nemo-12b": {
    "type": "local",
    "model_path": "mistralai/Mistral-Nemo-Instruct-2407",
    "description": "Mistral Nemo 12B parameter model with 128k context - Medium",
    "parameters": "12B",
    "size_category": "medium",
    "max_tokens": 512,
    "context_length": "128k",
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "generation_config": {
      "temperature": 0.3,
      "top_p": 0.8,
      "max_new_tokens": 512,
      "do_sample": true
    }
  },
  "hf-mistral-small-24b": {
    "type": "local",
    "model_path": "mistralai/Mistral-Small-24B-Instruct-2501",
    "description": "Mistral Small 24B parameter instruction-tuned model - Large",
    "parameters": "24B",
    "size_category": "large",
    "max_tokens": 512,
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "generation_config": {
      "temperature": 0.7,
      "top_p": 0.8,
      "max_new_tokens": 512,
      "do_sample": true
    }
  },
  "hf-qwen2.5-3b": {
    "type": "local",
    "model_path": "Qwen/Qwen2.5-3B-Instruct",
    "description": "Qwen2.5 3B parameter instruction-tuned model - Small",
    "parameters": "3B",
    "size_category": "small",
    "max_tokens": 512,
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "context_length": "128k",
    "generation_config": {
      "temperature": 0.7,
      "max_new_tokens": 512,
      "do_sample": true
    }
  },
  "hf-qwen2.5-32b": {
    "type": "local",
    "model_path": "Qwen/Qwen2.5-32B-Instruct",
    "description": "Qwen2.5 32B parameter instruction-tuned model - Medium",
    "parameters": "32B",
    "size_category": "medium",
    "max_tokens": 1024,
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "context_length": "128k",
    "generation_config": {
      "temperature": 0.7,
      "max_new_tokens": 1024,
      "do_sample": true
    }
  },
  "hf-qwen3-30b-thinking": {
    "type": "local",
    "model_path": "Qwen/Qwen3-30B-A3B-Thinking-2507",
    "description": "Qwen3 30B parameter reasoning model with thinking capabilities - Reasoning",
    "parameters": "30B",
    "size_category": "reasoning",
    "is_reasoning_model": true,
    "reasoning_config": {
      "thinking_budget": 512,
      "tokenize_chat_template": true,
      "decode_full_output": true
    },
    "generation_config": {
      "temperature": 0.7,
      "max_new_tokens": 2048,
      "do_sample": true
    }
  },
  "gpt-3.5-turbo": {
    "type": "api",
    "provider": "openai",
    "model_name": "gpt-3.5-turbo",
    "description": "GPT-3.5 Turbo model",
    "max_tokens": 256
  },
  "gpt-4-turbo": {
    "type": "api",
    "provider": "openai",
    "model_name": "gpt-4-turbo",
    "description": "GPT-4 Turbo model",
    "max_tokens": 256
  },
  "gpt-4o": {
    "type": "api",
    "provider": "openai",
    "model_name": "gpt-4o",
    "description": "GPT-4 Omni model",
    "max_tokens": 256
  },
  "gpt-4o-mini": {
    "type": "api",
    "provider": "openai",
    "model_name": "gpt-4o-mini",
    "description": "GPT-4 Omni Mini model",
    "max_tokens": 256
  },
  "gemini-1.5-flash": {
    "type": "api",
    "provider": "google",
    "model_name": "gemini-1.5-flash",
    "description": "Google Gemini 1.5 Flash model",
    "max_tokens": 256
  },
  "gemini-1.5-pro": {
    "type": "api",
    "provider": "google",
    "model_name": "gemini-1.5-pro",
    "description": "Google Gemini 1.5 Pro model",
    "max_tokens": 256
  },
  "claude-3-opus": {
    "type": "api",
    "provider": "anthropic",
    "model_name": "claude-3-opus-20240229",
    "description": "Claude 3 Opus model",
    "max_tokens": 256
  },
  "claude-3.5-sonnet": {
    "type": "api",
    "provider": "anthropic",
    "model_name": "claude-3-5-sonnet-20241022",
    "description": "Claude 3.5 Sonnet model",
    "max_tokens": 256
  },
  "u4b-mistral-7b_ft_gmeg_explanation_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_gmeg_explanation_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for gmeg task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "gmeg_explanation_ft",
    "training_info": {
      "setup_name": "gmeg",
      "prompt_name": "gmeg_explaination",
      "train_size": 375,
      "val_size": 46,
      "timestamp": "2025-09-30T10:25:27.278811"
    },
    "local_files_only": true
  }
}