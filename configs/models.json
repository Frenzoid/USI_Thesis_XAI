{
  "u4b-llama3.2-1b": {
    "type": "local",
    "model_path": "unsloth/llama-3.2-1b-instruct-bnb-4bit",
    "description": "Llama 3.2 1B parameter model (4-bit quantized with Unsloth) - Small",
    "parameters": "1B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "u4b-llama3-8b": {
    "type": "local",
    "model_path": "unsloth/llama-3-8b-instruct-bnb-4bit",
    "description": "Llama 3 8B parameter model (4-bit quantized with Unsloth) - Medium",
    "parameters": "8B",
    "size_category": "medium",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "u4b-llama3.3-70b": {
    "type": "local",
    "model_path": "unsloth/llama-3.3-70b-instruct-bnb-4bit",
    "description": "Llama 3.3 70B parameter model (4-bit quantized with Unsloth) - Large",
    "parameters": "70B",
    "size_category": "large",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": false
  },
  "u4b-mistral-7b": {
    "type": "local",
    "model_path": "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "description": "Mistral 7B parameter model (4-bit quantized with Unsloth) - Small",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": false,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true
  },
  "hf-mistral-nemo-12b": {
    "type": "local",
    "model_path": "mistralai/Mistral-Nemo-Instruct-2407",
    "description": "Mistral Nemo 12B parameter model with 128k context - Medium",
    "parameters": "12B",
    "size_category": "medium",
    "max_tokens": 512,
    "context_length": "128k",
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "generation_config": {
      "temperature": 0.3,
      "top_p": 0.8,
      "max_new_tokens": 512,
      "do_sample": true
    }
  },
  "hf-qwen2.5-3b": {
    "type": "local",
    "model_path": "Qwen/Qwen2.5-3B-Instruct",
    "description": "Qwen2.5 3B parameter instruction-tuned model - Small",
    "parameters": "3B",
    "size_category": "small",
    "max_tokens": 512,
    "finetuned": false,
    "load_in_4bit": false,
    "use_unsloth": false,
    "use_chat_template": true,
    "trust_remote_code": true,
    "local_files_only": false,
    "torch_dtype": "auto",
    "device_map": "auto",
    "context_length": "128k",
    "generation_config": {
      "temperature": 0.7,
      "max_new_tokens": 512,
      "do_sample": true
    }
  },

  "u4b-mistral-7b_ft_gmeg_explanation_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_gmeg_explanation_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for gmeg task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "gmeg_explanation_ft",
    "training_info": {
      "setup_name": "gmeg",
      "prompt_name": "gmeg_explaination",
      "train_size": 376,
      "val_size": 47,
      "timestamp": "2025-10-04T10:31:07.475022"
    },
    "local_files_only": true
  },
  "u4b-mistral-7b_ft_cose_explanation_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_cose_explanation_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for RHAI_cose_explanation task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "cose_explanation_ft",
    "training_info": {
      "setup_name": "RHAI_cose_explanation",
      "prompt_name": "cose_explanation",
      "train_size": 92,
      "val_size": 11,
      "timestamp": "2025-10-04T10:31:55.896003"
    },
    "local_files_only": true
  },
  "u4b-mistral-7b_ft_ecqa_positive_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_ecqa_positive_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for ecqa_positive task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "ecqa_positive_ft",
    "training_info": {
      "setup_name": "ecqa_positive",
      "prompt_name": "ecqa_positive_explanation",
      "train_size": 425,
      "val_size": 50,
      "timestamp": "2025-10-04T10:36:51.612547"
    },
    "local_files_only": true
  },
  "u4b-mistral-7b_ft_ecqa_negative_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_ecqa_negative_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for ecqa_negative task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "ecqa_negative_ft",
    "training_info": {
      "setup_name": "ecqa_negative",
      "prompt_name": "ecqa_negative_explanation",
      "train_size": 425,
      "val_size": 50,
      "timestamp": "2025-10-04T10:41:51.282747"
    },
    "local_files_only": true
  },
  "u4b-mistral-7b_ft_ecqa_freeflow_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_ecqa_freeflow_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for ecqa_freeflow task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "ecqa_freeflow_ft",
    "training_info": {
      "setup_name": "ecqa_freeflow",
      "prompt_name": "ecqa_freeflow_explanation",
      "train_size": 425,
      "val_size": 50,
      "timestamp": "2025-10-04T10:45:16.237135"
    },
    "local_files_only": true
  },
  "u4b-mistral-7b_ft_pubmedqa_reasoning_ft": {
    "type": "local",
    "model_path": "/teamspace/studios/this_studio/USI_Thesis_XAI/finetuned_models/u4b-mistral-7b_ft_pubmedqa_reasoning_ft",
    "description": "Finetuned Mistral 7B parameter model (4-bit quantized with Unsloth) - Small for pubmedqa_reasoning_required task",
    "parameters": "7B",
    "size_category": "small",
    "max_tokens": 256,
    "finetuned": true,
    "load_in_4bit": true,
    "use_unsloth": true,
    "use_chat_template": true,
    "base_model": "u4b-mistral-7b",
    "tune_name": "pubmedqa_reasoning_ft",
    "training_info": {
      "setup_name": "pubmedqa_reasoning_required",
      "prompt_name": "pubmedqa_reasoning_required",
      "train_size": 400,
      "val_size": 75,
      "timestamp": "2025-10-04T10:49:08.786376"
    },
    "local_files_only": true
  }
}